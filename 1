###If its customwrote, its for because my enviornment doesnt supoort that  

import os
import requests
import pickle
import json
import time 
import gzip
import io
import re
import math
import random 
import logging
from packaging import version
from threading import Event, Thread, RLock, Lock, local, enumerate as threading_enumerate
from joblib import dump, load
from sha256 import SHA256, sha256, MD5, md5 #custom sha256 i wrote
from concurrent.futures import as_completed
from bs4 import BeautifulSoup
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GroupShuffleSplit
from sklearn.utils.fixes import loguniform
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, log_loss
from urllib.parse import urlparse, urljoin
from datetime import datetime
import queue 
from excludedpatterns import excluded_patterns
from warcio import ArchiveIterator
import concurrent.futures
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import gc 
from EmergencyData import emergency_data_raw
import Custom as psutil #custom psutil
from requests.adapters import HTTPAdapter
from sklearn.pipeline import Pipeline
import sklearn
from collections import defaultdict, Counter, deque
from contextlib import contextmanager, closing
import sqlite3
from sklearn.feature_extraction.text import TfidfVectorizer
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_exponential
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm
from sklearn.linear_model import SGDClassifier
from urllib.robotparser import RobotFileParser
from concurrent.futures import ThreadPoolExecutor
from queue import Queue, PriorityQueue





DATA_DIR = os.path.expanduser("~/Documents/GodlikeAI/")  #leftover

# Configure logging
log_file_path = os.path.join(DATA_DIR, "godlike_ai.log")

# Ensure the directory exists

os.makedirs(DATA_DIR, exist_ok=True)
# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler(log_file_path)
stream_handler = logging.StreamHandler()

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)
stream_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(stream_handler)



# Gutenberg Constants
GUTENBERG_BASE_URL = "https://www.gutenberg.org"
GUTENBERG_CATALOG_URL = "https://www.gutenberg.org/browse/scores/top"
GUTENBERG_ROBOTS_DELAY = .5 # seconds between requests (be respectful)

MAX_MEMORY_MB = 1024 * 3 #3GB
#ADD OTHER SUPOORT
WEB_CRAWLER_CONFIG = {
    'offline_mode': False,
    'offline_storage': {
    'max_cache_size': 1024,  # MB
    'min_retention_days': 7,
    'auto_purge': True
    },
    'commoncrawl': {
        'index_url': 'http://index.commoncrawl.org/collinfo.json',
        'warc_base': 'https://data.commoncrawl.org/', 
        'max_pages': 1000000,
        'parallel_requests': 4,
        'warc_processing': True
    },
    'default_delay': 1.0,
    'seed_urls': [
        "https://commoncrawl.org",
    ],
    'user_agent': 'Godlike Head Researcher',
    'max_depth': 3,
    'politeness': {
        'max_parallel_domains': 10,
        'max_retries': 7,
        'respect_nofollow': True
    },
    'content_requirements': {
        'min_text_length': 500,
        'required_tags': ['main', 'article']
    }
    
}

MAX_BOOKS_TO_COLLECT = 50000  # Upper limit for book collection

##SKIP THIS SECTION

#END OF SECTION
class ProgressStream:
    def __init__(self, response, update_callback):
        self.response = response
        self.update_callback = update_callback
        self.total_size = int(response.headers.get('Content-Length', 0))
        self.bytes_read = 0
        self.complete = False

    def read(self, size=-1):
        chunk = self.response.raw.read(size)
        if chunk:
            self.bytes_read += len(chunk)
            if self.total_size > 0:
                progress = self.bytes_read / self.total_size
                self.update_callback(progress)
        elif not self.complete:  # Final update on EOF
            self.update_callback(1.0)
            self.complete = True
        return chunk

    def __iter__(self):
        return self

    def __next__(self):
        chunk = self.read(8192)
        if not chunk:
            raise StopIteration
        return chunk 
        
class AdaptiveRateLimiter:
      def __init__(self, initial_rate=1.2, min_delay=1.0, backoff_factor=1.8, max_delay=45.0):
        # Add adaptive response analysis
        self.response_time_window = deque(maxlen=100)
        self.error_rate_window = deque(maxlen=50)
        self.current_delay = initial_rate
        self.min_delay = min_delay
        self.backoff_factor = backoff_factor
        self.success_streak = 0
        self.fail_streak = 0
        self.last_request_time = 0
        self.lock = Lock()
        self.max_delay = max_delay 
        self.model_load_lock = Lock() 
        
        MAX_MEMORY_MB = (1024 * 3) #3GB 
        self.domain_queues = None  # Initialize first
        self.healthy_threshold = 0.8  # 80% success rate target
        self.excluded_patterns = [re.compile(p) for p in excluded_patterns]
        
      def wait(self):
        with self.lock:
            now = time.monotonic()
            elapsed = now - self.last_request_time
            sleep_time = max(self.current_delay - elapsed, 0)
            
            # Precision sleep using event wait
            if sleep_time > 0:
                Event().wait(sleep_time)
            
            self.last_request_time = time.monotonic()
    
      def success(self):
        with self.lock:
        # Increment success streak and reset failure count
          self.fail_streak = 0
          self.success_streak += 1

        # Track recent response time
          self.response_time_window.append(time.monotonic() - self.last_request_time)

        # Evaluate performance
          avg_response = np.mean(self.response_time_window) if self.response_time_window else 0

        # If responses are fast, reduce delay—recover quickly
          if avg_response < 1.0 or self.success_streak > 3:
            self.current_delay = max(self.current_delay / self.backoff_factor, self.min_delay)

    
      def failure(self, retry_after=None):
        with self.lock:
        # Reset success streak and increment failure count
          self.success_streak = 0
          self.fail_streak += 1

          if retry_after is not None:
            self.current_delay = max(
                self.current_delay * self.backoff_factor,
                retry_after
            )
          else:
            # Apply backoff without exceeding max_delay
            self.current_delay = min(
                self.current_delay * self.backoff_factor,
                self.max_delay
            )




class GodlikeAI:
    USER_CONFIG_SCHEMA = {
    'batch_size': {
        'type': int, 
        'min': 16,
        'max': 4096,
        'default': 1024
    },
    'active_model': {
        'type': str,
        'options': ["naive_bayes", "random_forest", "gradient_boosting", "sgd"],
        'default': "naive_bayes"  # Added default value
    },
    # Rest of the schema remains unchanged
    'memory_limit_mb': {
        'type': int,
        'min': 512,
        'max': 4096,
        'default': 2048
    },
    'learning_rate': {
        'type': float,
        'min': 0.0001,
        'max': 1.0,
        'default': 0.01
    }
    }
    
    
    genre_descriptions = {
    "adventure": "Stories about journeys, quests, and exploration with elements of risk and discovery",
    "mystery": "Focuses on solving crimes or uncovering secrets, often featuring detectives or amateur sleuths",
    "romance": "Centers on relationships, love, and emotional connections between characters",
    "science": "explores concepts grounded in scientific principles, often emphasizing discovery, experimentation, and the pursuit of knowledge through empirical evidence and logical reasoning.",
    "science_fiction": "Explores futuristic concepts, advanced technology, space travel, and alternate realities",
    "philosophy": "Examines existential questions, ethics, metaphysics, and the nature of reality and consciousness",
    "fantasy": "Tales set in imaginary worlds with magic, mythical creatures, and epic battles",
    "horror": "Intended to frighten, disturb, or unsettle, often involving supernatural or psychological elements",
    "historical": "Set in the past, highlighting specific periods, events, or figures in history",
    "thriller": "Fast-paced, suspenseful stories with high stakes and unexpected twists",
    "comedy": "Aims to entertain and amuse with humor, satire, and witty dialogue",
    "drama": "Realistic, character-driven narratives focusing on emotional themes and interpersonal conflict",
    "crime": "Revolves around criminal activities, investigations, and the justice system",
    "action": "High-energy stories with physical feats, battles, and intense sequences",
    "dystopian": "Depicts societies suffering under oppressive systems, often as cautionary tales",
    "cyberpunk": "Features high technology amid social disorder, focusing on the fusion of humans and machines",
    "erotica": "Explores sensuality and sexuality with descriptive intimate encounters",
    "noir": "Dark, cynical stories with morally ambiguous characters and a gritty urban setting",
    "western": "Set in the American frontier, featuring cowboys, outlaws, and rugged landscapes",
    "slice_of_life": "Portrays ordinary experiences in everyday existence, often highlighting quiet moments",
    "superhero": "Chronicles extraordinary individuals with special abilities who battle evil and protect society",
    "military": "Centers on armed conflict, soldier experiences, strategy, and the realities of warfare",
    "magical_realism": "Blends realistic settings with magical elements that are accepted as normal within the story",
    "steampunk": "Combines Victorian aesthetics with steam-powered technology in alternate historical settings",
    "gothic": "Features dark, mysterious settings, supernatural occurrences, and psychological intensity",
    "post-apocalyptic": "Set after global catastrophe, exploring survival and rebuilding of human society",
    "paranormal": "Involves psychic abilities, ghosts, and supernatural phenomena beyond scientific explanation",
    "urban_fantasy": "Places magical or supernatural elements within contemporary city settings",
    "space_opera": "Epic science fiction adventures across vast interstellar settings with dramatic conflicts",
    "hard_science_fiction": "Emphasizes scientific accuracy and technical detail in futuristic scenarios",
    "psychological_thriller": "Explores characters' unstable mental states and perception-altering experiences",
    "courtroom_drama": "Centers on legal proceedings, ethical dilemmas, and the pursuit of justice",
    "coming_of_age": "Chronicles a character's journey from youth to maturity through formative experiences",
    "historical_romance": "Combines love stories with carefully researched historical settings and social contexts",
    "satire": "Uses irony, humor, and exaggeration to criticize human folly and societal issues",
    "alternate_history": "Explores how history might have developed differently given changed circumstances",
    "epistolary": "Told through documents like letters, diary entries, news articles, or other records",
    "bildungsroman": "Follows a character's psychological and moral growth from youth to adulthood",
    "fable": "Short tales featuring anthropomorphized animals or objects that convey moral lessons",
    "surrealism": "Combines dreamlike or illogical elements to access deeper psychological truths",
    "occult": "Deals with hidden knowledge, mystical practices, and communication with supernatural forces",
    "espionage": "Features spies, intelligence agencies, and covert operations in high-stakes scenarios",
    "cosmic_horror": "Emphasizes humanity's insignificance against vast, incomprehensible cosmic terrors",
    "body_horror": "Focuses on the violation, transformation, or destruction of the human body",
    "utopian": "Portrays idealized societies with perfect social, political, and economic systems",
    "tragedy": "Chronicles the downfall of a protagonist through fatal flaws or external circumstances",
    "new_weird": "Blends science fiction, fantasy, and horror with surrealist techniques and literary sensibilities",
    "epic": "Sweeping narratives of heroic figures undertaking quests of great significance",
    "pastoral": "Idealizes rural life and landscapes, often contrasting with urban corruption",
    "biopunk": "Explores biotechnology's effects on society, including genetic manipulation and enhancement",
    "medical_drama": "Centers on healthcare professionals, ethical dilemmas, and intense medical scenarios",
    "political_thriller": "Combines political intrigue with suspense and high-stakes power struggles",
    "maritime": "Set at sea, focusing on seafaring adventures, naval warfare, or oceanic exploration",
    "pulp": "Fast-paced, plot-driven stories with vivid action and larger-than-life characters",
    "hard_boiled": "Features tough, cynical protagonists in gritty urban environments investigating crimes",
    "travel": "Chronicles journeys through various locations with emphasis on culture and place",
    "domestic_fiction": "Focuses on family relationships, home life, and everyday social interactions",
    "absurdist": "Highlights illogical or meaningless situations, challenging rational thought",
    "metaphysical": "Explores fundamental questions about existence, reality, and consciousness",
    "mythic": "Incorporates or reimagines traditional myths, legends, and folklore",
    "cli-fi": "Climate fiction examining the impact of climate change on civilization",
    "solarpunk": "Envisions optimistic futures combining sustainable technology with social progress",
    "silkpunk": "Features advanced Asian-inspired technology based on organic materials",
    "portal_fantasy": "Involves characters traveling between our world and magical realms",
    "grimdark": "Features morally ambiguous characters in brutal, nihilistic settings",
    "wuxia": "Chinese martial arts fiction emphasizing honor, justice, and skilled combat",
    "bizarro": "Deliberately weird, absurd, and outlandish scenarios pushing literary boundaries",
    "gaslamp_fantasy": "Victorian-era settings with supernatural elements but without steampunk's technological focus",
    "allegory": "Uses symbolic figures and actions to convey deeper political or moral meanings",
    "magical_girl": "Features young female protagonists who transform to battle evil with magical powers",
    "lovecraftian": "Inspired by H.P. Lovecraft's cosmic horror, featuring ancient malevolent entities",
    "technothriller": "Combines suspense with detailed technological elements central to the plot",
    "contemporary": "Set in the present day with realistic characters facing modern challenges",
    "antinovel": "Deliberately subverts conventional narrative techniques and plot structures",
    "southern_gothic": "Set in the American South with grotesque characters and decaying settings",
    "new_adult": "Focuses on experiences of early adulthood including identity, relationships, and independence",
    "historical_fantasy": "Combines historical settings with fantasy elements like magic or mythical creatures",
    "fairytale_retelling": "Reimagines traditional fairy tales with new perspectives or modern settings",
    "planetary_romance": "Adventures on alien worlds with exotic environments and civilizations",
    "mannerpunk": "Emphasizes social rules, class structures, and intricate personal politics",
    "psychedelic": "Features consciousness-altering experiences and reality-bending perceptions",
    "quest": "Structured around a protagonist's mission to obtain an object or reach a destination",
    "bangsian": "Set in the afterlife, often featuring famous historical figures as characters",
    "eco-fiction": "Addresses environmental issues and humanity's relationship with nature",
    "military_science_fiction": "Combines military themes with futuristic technology and interplanetary conflict",
    "cosmic_fantasy": "Combines science fiction elements with mystical or spiritual dimensions",
    "flintlock_fantasy": "Fantasy set in an age of gunpowder with muskets alongside magical elements",
    "paleofiction": "Set in prehistoric times, often featuring early human societies",
    "superhuman": "Explores enhanced abilities through technology, evolution, or supernatural means"
    
    }
    
    
    
    warmup_texts = [
    ("The explorer hacked through dense jungle vines, map in hand, as ancient ruins emerged through the mist ahead. This expedition had faced countless perils, but the promise of discovery drove them forward.", "adventure"),
    
    ("The detective examined the hidden clue under ultraviolet light, revealing a secret message. The investigation had taken an unexpected twist, and now every suspect's alibi needed scrutiny.", "mystery"),
    
    ("Their hands touched across the table, an electric moment of chemistry neither could deny. She felt her heart race as their eyes met, years of longing finally finding expression in this quiet moment.", "romance"),
    
    ("The laboratory erupted in excitement as data confirmed the hypothesis. This genetic breakthrough would revolutionize our understanding of cellular repair mechanisms and potentially lead to unprecedented medical innovations.", "science"),
    
    ("Is consciousness merely an emergent property of neural activity, or something more fundamental to reality itself? The paradox of subjective experience challenges both rationalism and empiricism.", "philosophy"),
    
    ("The apprentice wizard traced glowing runes around the portal, whispering ancient enchantments as magical energy filled the chamber. Beyond the shimmering gateway lay a realm no human had entered for centuries.", "fantasy"),
    
    ("Shadows moved against natural laws in the abandoned mansion. A cold presence brushed past, whispering names of the dead, as the darkness seemed to breathe with supernatural malice.", "horror"),
    
    ("The empire's fall was inevitable after the dynasty's three-century reign. As peasants stormed the palace gates, historians documented the revolution that would transform a medieval society into something entirely new.", "historical"),
    
    ("The agent discovered the conspiracy reached the highest levels of government. With twenty-four hours before the plot would execute, the race began to uncover the mole while evading the assassins on her trail.", "thriller"),
    
    ("His elaborate proposal plan went spectacularly wrong when the ring ended up in the punch bowl, the flash mob arrived an hour late, and her ex showed up dressed identically to him. The resulting chaos had everyone in stitches.", "comedy"),
    
    ("Tears streamed down her face as decades of family secrets spilled into the open. The emotional confrontation brought painful truths to light, yet somehow opened a path toward healing their fractured relationship.", "drama"),
    
    ("The heist required perfect timing as security cameras rotated. When forensic evidence linked the gang to three previous robberies, the police interrogation focused on finding their inside man at the bank.", "crime"),
    
    ("Bullets ricocheted as she dived behind the overturned table. The explosion had triggered a firefight, and now she had only seconds to disarm the device before the entire building would collapse in flames.", "action"),
    
    ("Citizens averted their eyes from surveillance drones patrolling above. The resistance operated from underground tunnels, planning their uprising against the totalitarian regime that controlled every aspect of daily life.", "dystopian"),
    
    ("Neon advertisements reflected in puddles as she adjusted her neural implant. After hacking the corporation's mainframe, her augmented vision revealed glitches in the urban grid – evidence of something hidden in the network.", "cyberpunk"),
    
    ("His body tensed at her intimate touch, desire flooding his senses. Their passionate encounter intensified as clothes fell away, their naked forms moving together with increasing urgency and heat.", "erotica"),
    
    ("Rain pattered against the office window as the private eye studied crime scene photos. The femme fatale's story didn't add up, and somewhere in this corrupt city, a murderer thought they'd gotten away clean.", "noir"),
    
    ("Dust kicked up as the sheriff approached the saloon. His hand hovered near his revolver, ready for the inevitable confrontation with the outlaw who had terrorized the frontier town for too long.", "western"),
    
    ("She sipped coffee while watching morning light filter through kitchen curtains. The quiet routine of preparing breakfast for her family brought a sense of contentment, these simple moments revealing life's deeper meaning.", "slice_of_life"),
    
    ("The masked hero soared between skyscrapers, using superhuman strength to catch the falling bus. His nemesis watched from a distance, already plotting the next challenge to test the limits of those extraordinary powers.", "superhero"),
    
    ("The platoon moved silently through enemy territory, following strategic commands through their headsets. Military discipline kept them focused despite the airstrike that had just obliterated their extraction point.", "military"),


    ("The terraforming drones circled Mars' atmosphere, seeding clouds that would bring the first rain in billions of years. Dr. Chen watched from the colony dome, knowing her genetic adaptations would allow future generations to breathe the new air without suits.", "hard_science_fiction"),
    
    ("The Caretaker's reality shifted again, fractal patterns bleeding through normal perception as vast entities moved between dimensions. These glimpses beyond the veil confirmed humanity's cosmic insignificance—mere bacteria observed by incomprehensible beings.", "cosmic_horror"),
    
    ("The flowers erupted from his skin, roots visibly threading beneath the surface as petals bloomed from his cheeks and neck. He stared at his transformation in the mirror, both horrified and mesmerized by his body's botanical rebellion.", "body_horror"),
    
    ("Madame Lefèvre's letter trembled in Charlotte's hands, its yellowed pages revealing the scandal that would destroy her family's reputation. The correspondence chronicled events from thirty years prior, exposing secrets her mother had carefully buried.", "epistolary"),
    
    ("The rain fell upward as María walked through the village square. Nobody seemed to notice the backward rainfall or the fish swimming through air instead of water. Such impossibilities had become ordinary since her grandmother's passing.", "magical_realism"),
    
    ("Brass gears clicked as the analytical engine processed the complex calculations. Lady Augusta adjusted her corset, smearing coal dust across expensive silk as she fine-tuned the difference engine that would revolutionize the Empire's computational capabilities.", "steampunk"),
    
    ("The ambassador adjusted her neural translator as the alien delegation entered. Earth's survival depended on this negotiation succeeding, despite the vast philosophical differences between species evolved in entirely different biochemical environments.", "space_opera"),
    
    ("Cecilia hesitated at the castle gates, her heart racing beneath her modest gown. Lord Harrington awaited inside, a man of wealth and power who had inexplicably chosen her—a mere governess—as the object of his affections.", "historical_romance"),
    
    ("President Reynolds never gave the order to drop the bomb on Beijing in 1962. Historians now trace today's Chinese-American Alliance to that pivotal moment when nuclear annihilation was avoided by a single decision.", "alternate_history"),
    
    ("The teenage werewolf tried to control his transformation before prom night, practicing meditation techniques in his suburban bedroom while hiding his condition from his parents. Dating was complicated enough without growing fangs mid-conversation.", "urban_fantasy"),
    
    ("Judge Martinez examined the evidence again, troubled by inconsistencies in the prosecution's case. The young defendant's life hung in the balance, but the truth seemed deliberately obscured by powerful interests within the city government.", "courtroom_drama"),
    
    ("Nia studied her reflection, seeing both her sixteen-year-old self and glimpses of the woman she was becoming. Her mother's terminal diagnosis had thrust adult responsibilities upon her shoulders, accelerating her journey toward independence.", "coming_of_age"),
    
    ("The Minister of Productivity announced another increase in working hours, his plastic smile beaming from every wallscreen. Meanwhile, Citizen 7293 secretly recorded the disappearances that followed each protest, evidence for a resistance that might never materialize.", "dystopian"),
    
    ("The presidential assassination had been perfectly planned, but now CIA operative Walker found herself questioning the agency's motives. The encrypted message suggested a conspiracy reaching the highest levels of government, and only she had the decryption key.", "espionage"),
    
    ("The gene-spliced octopus tentacles replaced her damaged spine, giving her unprecedented mobility and strength. BlackSun Corporation owned the proprietary biotech—and by extension, owned her—but tonight's corporate espionage would change that power dynamic forever.", "biopunk"),
    
    ("Wind caressed the tall grasses as Samuel guided his herd across ancestral grazing lands. The rhythms of nature dictated life here, unchanged for generations despite the distant city's constant pressure to modernize and develop the untouched valley.", "pastoral"),
    
    ("Captain Trevelyan gripped the ship's wheel as the monstrous wave approached. The storm had claimed three men already, but abandoning their course meant certain starvation for the coastal village depending on their cargo of medicine and grain.", "maritime"),
    
    ("The Empress's assassination plunged the kingdom into chaos, as noble houses mobilized armies while publicly pledging peace. Lady Karishma recognized the opportunity hidden within crisis, carefully moving her agents into position for an unprecedented power grab.", "political_thriller"),
    
    ("The monastery perched on the cliffside, unchanged since the 15th century. Inside, Sister Agnes discovered the hidden manuscript suggesting the saint they venerated had actually been a witch, her miracles performed through forbidden arts.", "gothic"),
    
    ("Special Agent Cooper studied the victim's dental records, confirming his theory about the killer's background in orthodontics. The bite marks held a signature pattern that would finally allow them to identify the Midnight Butcher after seven years of investigation.", "crime"),
    
    ("Aiko summoned her magical transformation, pink light enveloping her as she changed from ordinary schoolgirl to Crystal Guardian Sparkle. The cosmic wand materialized in her hand just as the shadow monsters emerged from the dimensional portal near the shopping mall.", "magical_girl"),
    
    ("The journey through the wardrobe led them to a snow-covered forest unlike anything in England. Lucy's discovery of this parallel world would ultimately lead to their coronation as kings and queens of a realm where animals spoke and magic was real.", "portal_fantasy"),
    
    ("Glass skyscrapers stood alongside vertical farms, their surfaces collecting solar energy while drones pollinated rooftop gardens. The carbon-neutral city represented humanity's successful adaptation to environmental crisis through technology and social cooperation.", "solarpunk"),
    
    ("The Void Plague had reduced civilization to scattered enclaves fighting for dwindling resources. Maya loaded her shotgun as raiders approached the compound, knowing mercy was a luxury no survivor could afford in this brutal new world.", "post-apocalyptic"),
    
    ("The jade-crafted mechanical dragonfly delivered the emperor's message, its wings powered by refined lightning essence captured during the monsoon season. Master Zhan examined the intricate crystal circuitry, recognizing the urgent diplomatic code.", "silkpunk"),
    
    ("The gladiator wiped blood from his blade, surrounded by the corpses of opponents who had once been allies. Honor meant nothing in the fighting pits of Karash, where survival required embracing the darkest aspects of human nature.", "grimdark"),
    
    ("Master Zhao's sword technique moved beyond physical possibility, his chi manipulation allowing him to strike opponents from thirty feet away. The ancient scrolls had revealed spiritual cultivation methods thought lost for centuries.", "wuxia"),
    
    ("The elephant detective adjusted his monocle while the toaster witness recounted the murder confession of the sentient refrigerator. Meanwhile, the lamppost victim's ghost illuminated key evidence with its spectral glow.", "bizarro"),
    
    ("Dr. Eliza Thompson's modification to the particle accelerator would fundamentally alter our understanding of quantum states, potentially solving the energy crisis through zero-point field manipulation. Her colleagues called it impossible until the equations proved otherwise.", "technothriller")

    ]
    
    ##END OF SKIP
    def __init__(self): 
        
        self.lock = Lock()
        # Define default parameters first
        self.warmup_first = True
        self.batch_size = 32  # Start with smaller batches for initial data
        self.book_processing_batch = 64
        self.active_model = "naive_bayes"  # Default model
        self.autosave_interval = 300  # 5 minute
        self.foreground_event = Event()
        self.last_save_time = time.time()
        self.autosave_thread = None
        self.fitted = False
        self.last_request_time = 0
        self.request_lock = Lock()
        self.training_lock = Lock()
        self.db_lock = RLock()
        self.context_weights = []
        self.thread_local = local()
        self.model_load_lock = Lock()  
        self.max_memory_items = 1000
        self.memory_decay = 0.95  # 5% decay per time step
        self.memory_weights = defaultdict(float)
        self.max_context_length = 10  # Increased from 5
        self.context_decay = 0.85
        self.attention_network = None  # Could be enhanced with neural network later   
        self.db_pool = queue.Queue(maxsize=5)  # Initialize here
        self.vm = psutil.virtual_memory()
        self.domain_retries: dict[str, int] = {}
        self.fallback_trained = False
        self.last_confidence = 0.0  # New line added 
        self.context_window = deque(maxlen=self.max_context_length)
        self.domain_timeouts = defaultdict(float)  # Track domain block times 
        self.blacklisted_domains = set()
        self.processing_queue = {
        'books': deque(maxlen=500),
        'web_pages': deque(maxlen=1000),
        'priority': defaultdict(int)
        }
    
        
        self.web_crawl_stats = {
    'processed_pages': 0,
    'current_domain': None,
    'queued_domains': 0,
    'active_books': [],
    'active_websites': [],
    'book_progress': {},
    'website_progress': {},
    'warc_progress': {} # Add this line
        }
        self.context_attention = {
    'short_term': 0.6,
    'medium_term': 0.3,
    'long_term': 0.1
        }
        
        self.model_version = 1
        self.db_path = os.path.join(DATA_DIR, "training_data.db")
        # Initialize models
        self.vectorizer = HashingVectorizer(
    n_features=2**18,
    ngram_range=(1,3),
    lowercase=True,
    norm='l2', 
    alternate_sign=False,
    stop_words='english',
    dtype=np.float32,
    analyzer='char_wb',
    binary=True,
    
        )
        
          
        self.models = {
    "naive_bayes": MultinomialNB(alpha=0.01),  # More sensitive to rare features
    "random_forest": RandomForestClassifier(
        n_estimators=400,  # Increased from 200
        max_depth=50,       # Added depth constraint
        min_samples_leaf=2, # New parameter
        min_samples_split=5,  # Reduced from 2
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        warm_start=True
    ),
    "gradient_boosting": GradientBoostingClassifier(
        n_estimators=2000,  # Increased from 1500
        learning_rate=0.05,  # Adjusted from 0.02
        max_features='log2', # Added feature sampling
        max_depth=6,         # Slightly deeper trees
        subsample=0.7,       # More aggressive subsampling
        validation_fraction=0.15,
        n_iter_no_change=10  # Longer patience
    ),
    "sgd": SGDClassifier(
        loss='log',
        penalty='elasticnet',
        alpha=1e-6,          # Reduced regularization
        max_iter=2000,       # More iterations
        tol=1e-5,            # Tighter tolerance
        learning_rate='optimal',  # Better for large datasets
        eta0=0.01,           # Lower initial learning rate
        power_t=0.25,
        early_stopping=True,
        validation_fraction=0.15,
        n_iter_no_change=10,
        class_weight='balanced',
        average=64           # Larger averaging window
        )
        } 
        
        self.domain_limits = defaultdict(lambda: {
    'delay': WEB_CRAWLER_CONFIG['default_delay'],
    'last_request': 0
        }) 
        
        self.incremental_models = ["naive_bayes", "sgd"]  # Models that support partial_fit
        self.fallback_vectorizer = TfidfVectorizer(
          max_features=10000,
          ngram_range=(1, 3),
          stop_words='english',
          min_df=5,
          max_df=0.7
        ) 
        
        self.db_path = os.path.join(DATA_DIR, "training_data.db")
        
        self.genre_classifier = MultinomialNB(alpha=0.1)
    # Create necessary directories and initialize database schema
        self._initialize_filesystem()
        self._init_db()  
        
        
        if not hasattr(self.genre_classifier, 'classes_'):
          logger.info("Initializing fallback classifier with emergency data")
          self._load_emergency_cache()
          self._train_fallback_genre_classifier()
          self.fallback_trained = True
         
        if not self.fallback_trained:
          logger.error("Fallback failed. Retrying with base data...")
          self._train_fallback_genre_classifier() 
          self.fallback_trained = True
        self.model_config = {
    "naive_bayes": {
        "auto_tune": True,
        "alpha_range": [0.01, 0.1, 1.0]
    },
    "random_forest": {
        "tree_increment": 50,
        "max_depth_options": [None, 10, 20, 30]
    },
    "gradient_boosting": {
        "learning_rate_options": [0.01, 0.05, 0.1],
        "early_stopping_rounds": 10
        }
        }  
        
        
        #warm start
        for name, model in self.models.items():
          if hasattr(model, 'warm_start'):
            if name == "random_forest" and not hasattr(model, 'estimators_'):
              model.set_params(n_estimators=100, warm_start=True)
            elif name == "gradient_boosting" and not hasattr(model, 'estimators_'):
              model.set_params(n_estimators=1000, warm_start=True)
            elif name == "sgd":
                
                if not hasattr(model, 't_'):
                    model.set_params(warm_start=False)  # SGD requires partial_fit instead
 
        # Create necessary directories and files
        
        print('7')
        # Load data
        self.load_state()
        self.load_model_version()

        with self.db_lock:
          with self._get_cursor() as cursor:
            cursor.execute('''
    CREATE TABLE IF NOT EXISTS model_versions (
        id INTEGER PRIMARY KEY,
        timestamp DATETIME,
        model_data BLOB,
        vectorizer_data BLOB,
        metadata TEXT
    )''')
        with self.db_lock:
          with self._get_cursor() as cursor:
            try:
              cursor.execute("SELECT COUNT(*) FROM training_data")
              count = cursor.fetchone()[0]
              if count > 0:
                cursor.execute("SELECT DISTINCT label FROM training_data")
                self._unique_classes = {row[0] for row in cursor.fetchall()}  # Changed to _unique_classes
              else:
                self._unique_classes = set()  # Changed to _unique_classes
            finally: 
              cursor.close()
      
        print(3)
        # Book processing tracking
        self.processed_book_count = len(self.learning_progress.get('completed_books', []))
        self.total_books = len(self.book_index) if self.book_index else 0
        
        # Training status flags
        self.stop_training = False  # Flag to stop training threads
        self.training_in_progress = False
        self.training_stats = defaultdict(list)

        # Feature flags for advanced capabilities
        self.enable_sentiment_analysis = True
        self.sentiment_analyzer = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=10000)),
    ('classifier', SGDClassifier(loss='log'))
        ])

        self.sentiment_analyzer = None  # Will use text-based features instead
        
        self.network_status = True
        self.network_check_interval = 10  # seconds 
        self.offline_mode = WEB_CRAWLER_CONFIG['offline_mode'] 
        if self.offline_mode:
            logger.info("Initializing in OFFLINE MODE - Using cached data only")
            self._init_offline_resources()
            self._init_processing_queue()
        self._init_network_monitor() 
        
        self.enable_topic_modeling = False  # More advanced, enable as needed
        self._init_autosave()
        
        
        with self.db_lock:
          with self._get_cursor() as cursor: 
            
            cursor.execute("SELECT COUNT(*) FROM training_data")
            count = cursor.fetchone()[0]
            if count == 0:
              logger.info("Inserting warmup data")
              try:
                warmup_data = [(text, label, sha256(text.encode()).hexdigest()) 
                             for text, label in self.warmup_texts]
                cursor.executemany('''
                    INSERT OR IGNORE INTO training_data 
                    (text, label, processed, text_hash)
                    VALUES (?, ?, 0, ?)
                ''', warmup_data)
                cursor.connection.commit()
                inserted = cursor.execute("SELECT changes()").fetchone()[0]
                logger.info(f"Warmup inserted: {inserted} rows")
                if inserted < len(warmup_data):
                  logger.warning("Duplicate warmup entries detected")
                
              except Exception as e:
                logger.error(f"Warmup data insertion failed: {e}")
                self._load_emergency_cache()
            
                      
        
                      
        
                      
        if len(self._unique_classes) < 2:
              logger.info("Insufficient data - starting initial training")
              self.batch_size = 32  # Small batch for warmup
              self._train_batch()
              self.batch_size = 1024  # Reset to default 
        self.fallback_trained = False
        if not self.fallback_trained:
          logger.info("Initializing fallback (main) genre classifier")
          self._train_fallback_genre_classifier()
        if not self.fallback_trained:
          logger.error("Fallback genre classifier failed to initialize")
          raise RuntimeError("Critical failure in fallback initialization")
        if len(self._unique_classes) >= 2:
          logger.info("Starting initial training with available data")
          self._train_batch()
        else:
          logger.info("Insufficient classes, waiting for data...")
# Start processing threads with error handling
        try:
    # Start processing threads with error handling
          self.setup_web_crawler()
          self.update_seeds()
          self._init_processing_queue()
          self.start_learning()          # Start learning before web crawler
          self.start_web_crawler()
        except Exception as e:
          logger.critical(f"Failed to start threads: {str(e)}", exc_info=True)
          self._repair_database()
    # Attempt safe restart
          self._init_processing_queue()
          if not self.offline_mode:
            self.start_learning(continue_from_last=False)
        
        
        
        try:
        # Integrity check without outer lock
          with self._get_cursor() as cursor:
            cursor.execute("PRAGMA integrity_check")
            result = cursor.fetchone()
            if result[0] != "ok":
                logger.critical("Database corruption detected")
                self._repair_database()
        except sqlite3.OperationalError as e:
          logger.critical(f"Database integrity check failed: {str(e)}")
          self._repair_database()
                
        
        if self.offline_mode and not self._validate_offline_training_data():
          logger.critical("Offline mode with insufficient data - loading emergency set")
          self._load_emergency_cache()
          self.batch_size = 32  # Start with smaller batches
          self._train_batch()
          self.batch_size = 1024  # Reset to default after initial training
        #Seeds
        
        
        self.update_seeds() 
        
        
        if not hasattr(GodlikeAI, '_global_limiter'):
          GodlikeAI._global_limiter = AdaptiveRateLimiter(
        initial_rate=GUTENBERG_ROBOTS_DELAY,
        min_delay=0.5
          )
        self.rate_limiter = GodlikeAI._global_limiter
        
        logger.info("GodlikeAI initialized")
        
    _CLEAN_REGEX = [
    (re.compile(r'[^\w\s]'), ''),
    (re.compile(r'\d+'), ''),
    (re.compile(r'\s+'), ' ')
        ] 
        
    
    def _validate_connection(self, conn):
      try:
        with conn:
            conn.execute("BEGIN IMMEDIATE")
            conn.execute("SELECT 1").fetchone()
            conn.commit()
        return True
      except sqlite3.Error:
        return False
    def _get_connection(self):
      if not hasattr(self, 'db_pool'):
        
        logger.critical("Database connection pool not initialized... Initializing")
        

      """Get validated database connection with retry logic"""
      for _ in range(3):
        try: 
            
            conn = self.db_pool.get(timeout=5) 
            if self._validate_connection(conn):
                return conn
            else:
                conn.close()
                self.db_pool.put(self._create_new_connection())
        except queue.Empty:
            logger.warning("Connection pool empty, creating new connection")
            pass
      return self._create_new_connection()
    def check_network_status(self):
      """Check internet connectivity with timeout"""
      if self.offline_mode:
          logger.warning("Offline...")
          return False
      try:
        # Try connecting to a reliable service
          requests.get('http://connectivitycheck.gstatic.com/generate_204',
                    timeout=3)
          return True
      except Exception:
          return False
      return False
    def _init_processing_queue(self):
      """Initialize processing queues from database"""
      logger.info("Initializing offline processing queues")
    
    # Load books
      with self._get_cursor() as cursor:
        cursor.execute("""
            SELECT text, label 
            FROM training_data 
            WHERE label LIKE 'book:%' AND processed=0
        """)
        for text, label in cursor:
            self.processing_queue['books'].append((text, label))
    
    # Load web pages
      with self._get_cursor() as cursor:
        cursor.execute("""
            SELECT content, genre 
            FROM web_pages 
            WHERE processed=0
        """)
        for content, genre in cursor:
            self.processing_queue['web_pages'].append((content, genre))
    
      logger.info(f"Offline queues loaded: {len(self.processing_queue['books'])} books, "
                f"{len(self.processing_queue['web_pages'])} web pages")
      
    def _create_new_connection(self):
      """Create new connection with proper settings, fallback to in-memory"""
      db_path = os.path.join(DATA_DIR, "training_data.db")
      
      logger.debug(f"Creating new connection to: {db_path}")
      try:
        # Attempt file-based connection
        conn = sqlite3.connect(
            db_path,
            check_same_thread=False,
            timeout=30
        )
        conn.execute("SELECT 1").fetchone()  # Test connection
        return conn
      except sqlite3.Error as e:
        logger.error(f"File database failed, using in-memory: {str(e)}")
        # Fallback to in-memory
        return sqlite3.connect(":memory:", check_same_thread=False, timeout=30)
          

    def _return_connection(self, conn):
      """Safely return connection to pool"""
      try:
        if conn:
            self.db_pool.put(conn, timeout=5)
      except queue.Full:
        conn.close()
      except sqlite3.Error as e:
        logger.warning(f"Closing faulty connection: {str(e)}")
        conn.close()
                
    def _initialize_filesystem(self):
      """Initialize database structure with a direct connection"""
      
      try: 
        
        
        logger.info(f"initalizing filesystem...")
        # Use direct connection for schema setup
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Create tables if they don't exist 
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS system_state (
                id INTEGER PRIMARY KEY,
                user_profile TEXT,
                book_index TEXT,
                learning_progress TEXT,
                training_stats TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS training_data (
    id INTEGER PRIMARY KEY,
    text TEXT CHECK(LENGTH(text) >= 20),
    label TEXT CHECK(label GLOB '*[a-zA-Z]*'),
    processed INTEGER DEFAULT 0 CHECK(processed IN (0,1)),
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    features TEXT,
    text_hash TEXT,
    UNIQUE(text_hash, label) -- Add composite unique constraint here
)
        ''')
        # Create other necessary tables (e.g., web_pages, model_versions, etc.)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS web_pages (
                id INTEGER PRIMARY KEY,
                url TEXT UNIQUE,
                domain TEXT,
                content TEXT CHECK(LENGTH(content) >= 20),
                genre TEXT,
                status_code INTEGER,
                content_length INTEGER,
                crawl_duration REAL,
                links_found INTEGER,
                processed INTEGER DEFAULT 0 CHECK(processed IN (0,1)),
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_versions (
                id INTEGER PRIMARY KEY,
                timestamp DATETIME,
                model_data BLOB,
                vectorizer_data BLOB,
                metadata TEXT
            )
        ''')
        # Commit changes

        conn.commit()
      except Exception as e:
        logger.critical(f"Filesystem initialization failed: {str(e)}")
        raise
      finally:
        conn.close()  # Ensure connection is closed after schema setup 
    def _init_network_monitor(self):
      """Initialize continuous network monitoring"""
      def monitor_loop():
        while True:
            prev_status = self.network_status
            self.network_status = self.check_network_status()
            if prev_status != self.network_status:
                logger.info(f"Network status changed to {'online' if self.network_status else 'offline'}")
            time.sleep(self.network_check_interval)
    
      Thread(target=monitor_loop, daemon=True).start()
    def _initialize_fresh_models(self):
      """Create new model instances with current parameters"""
      return {
        "naive_bayes": MultinomialNB(alpha=0.1),
        "random_forest": RandomForestClassifier(
            class_weight='balanced_subsample',
            n_estimators=100,
            random_state=42,
            n_jobs=-1,
            warm_start=True
        ),
        "gradient_boosting": GradientBoostingClassifier(
            n_estimators=1000,
            learning_rate=0.05,
            max_depth=5,
            subsample=0.8,
            validation_fraction=0.1,
            n_iter_no_change=5,
            warm_start=True
        ),
        "sgd": SGDClassifier(
            loss='log',
            penalty='elasticnet',
            alpha=1e-6,
            max_iter=2000,
            tol=1e-5,
            learning_rate='optimal',
            eta0=0.01,
            power_t=0.25,
            early_stopping=True,
            validation_fraction=0.15,
            n_iter_no_change=10,
            class_weight='balanced',
            average=64
        )
      }
    @retry(stop=stop_after_attempt(3), retry=retry_if_exception_type((requests.exceptions.Timeout, requests.exceptions.ConnectionError)))
    def process_webpage(self, url, content=None): 
      """Process generic webpage content"""
      if self.offline_mode:
        logger.info("Offline mode - processing cached content")
        return self._process_cached_web_content()  # New method added
      response = None
      start_time = time.time()

      try:
        parsed = urlparse(url)
        domain = parsed.netloc
        self.web_crawl_stats['current_domain'] = domain

        if content is None:
            # Respect crawl delay
            elapsed = time.time() - self.domain_limits[domain]['last_request']
            delay = self.domain_limits[domain]['delay']
            if elapsed < delay:
                time.sleep(delay - elapsed)

            # 🔁 This is the network call that may raise retryable errors
            with requests.Session() as session:
                response = self.ethical_request(url)  # this must use session internally if needed
                raw_content = response.text
        else:
            try:
              raw_content = content.decode('utf-8', errors='replace')
            except AttributeError:
              raw_content = str(content)

        # Extract and clean HTML
        soup = BeautifulSoup(raw_content, 'html.parser')
        for script in soup(["script", "style", "noscript"]):
            script.decompose()
        main_content = self._extract_main_content(soup)

        text = self._extract_main_content(soup)
        if not text:
            return False

        paragraphs = self.extract_paragraphs(main_content)
        if not paragraphs:
            logger.warning(f"No content extracted from {url}")
            return False

        title = soup.title.string if soup.title else url
        genre = self.determine_genre(title, paragraphs) if title else "unknown"
 
        # Save to DB
        with self.db_lock:
            with self._get_cursor() as cursor:
                cursor.execute('''
                    INSERT INTO web_pages 
                    (url, domain, content, genre, status_code, 
                     content_length, crawl_duration, links_found, processed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0)
                ''', (
                    url, domain, '\n'.join(paragraphs), genre,
                    response.status_code if (content is None and response is not None) else 200,
                    len(raw_content),
                    time.time() - start_time,
                    len(soup.find_all('a')) if content is None else 0
                ))
        self.web_crawl_stats['processed_pages'] += 1
        return True

      except Exception as e:
        logger.error(f"Error processing {url}: {str(e)}")
        parsed_url = urlparse(url)
        if parsed_url.netloc not in self.blacklisted_domains:
          self._add_to_recovery_queue('website', {
            'url': url,
            'domain': parsed_url.netloc,
            'error': str(e),
            'retry_after': time.time() + 300  # Retry after 5 minutes
          })
        return False

      finally:
        if response is not None:
            response.close()
        if 'soup' in locals():
            try:
                soup.decompose()
            except:
                pass
        gc.collect()
    def _process_cached_web_content(self):
      """Process cached web content with memory management"""
      processed_count = 0
      try:
        while self.offline_cache['web_cache']:
            # Batch processing for efficiency
            batch = [self.offline_cache['web_cache'].popleft() 
                   for _ in range(min(100, len(self.offline_cache['web_cache'])))]
            
            # Parallel processing
            with ThreadPoolExecutor(max_workers=4) as executor:
                results = list(executor.map(
                    lambda item: self._process_single_cached_item(*item), 
                    batch
                ))
            
            processed_count += sum(results)
            
            # Memory protection
            if psutil.virtual_memory().percent > 85:
                logger.warning("High memory usage - clearing cache")
                self.offline_cache['web_cache'].clear()
                break
                
        logger.info(f"Processed {processed_count} cached pages offline")
        return processed_count > 0
      except Exception as e:
        logger.error(f"Offline processing failed: {str(e)}")
        return False

    def _process_single_cached_item(self, content, genre):
      """Validate and process individual cached items"""
      clean_content = self.validate_and_clean_text(content)
      if not clean_content:
        return False
        
      # Add to training system
      self.train(clean_content, genre)
      return True
    def _extract_main_content(self, soup):
      """Extract main readable content using advanced scoring heuristics."""
      if not soup:
        return ""

    

    # Define scoring weights
      WEIGHTS = {
        'min_words': 100,
        'word_weight': 0.2,
        'structure_weight': 0.8,
        'noise_penalty': 0.6,
        'header_bonus': 0.4,
        'text_length_weight': 0.3,
        'semantic_bonus': 2.0,
      }

    # Candidate tags to evaluate
      candidate_tags = ['article', 'main', 'div[role="main"]', 'section']
      NOISE_TAGS = ['nav', 'footer', 'aside', 'header', 'script', 'style']
       
    # Remove noise elements first
      for tag in NOISE_TAGS:
        for element in soup.select(tag):
            element.decompose()
    # Helper: Check for visible content
      def is_visible(text):
        return bool(text and text.strip())

    # Collect candidates
      candidates = []

      for elem in soup.find_all(candidate_tags):
        text = elem.get_text(separator=' ', strip=True)
        if not is_visible(text):
            continue

        words = re.findall(r'\w+', text)
        if len(words) < WEIGHTS['min_words']:
            continue

        # Scoring
        score = 0
        score += len(words) * WEIGHTS['word_weight']
        score += len(elem.find_all(['p', 'li', 'pre', 'code'])) * WEIGHTS['structure_weight']
        score -= len(elem.find_all(['nav', 'aside', 'header', 'footer', 'form'])) * WEIGHTS['noise_penalty']
        score += len(elem.find_all(['h1', 'h2', 'h3', 'h4', 'h5'])) * WEIGHTS['header_bonus']
        score += (len(text) / 1000.0) * WEIGHTS['text_length_weight']
        if elem.find_parents(['main', 'article']):
            score += WEIGHTS['semantic_bonus']

        candidates.append((elem, score))

    # Pick the highest scoring candidate
      if candidates:
        best_elem = max(candidates, key=lambda x: x[1])[0]
        for tag in ['header', 'footer', 'nav', 'aside']:
            for junk in best_elem.find_all(tag):
                junk.decompose()
        return best_elem.get_text(separator=' ', strip=True)

    # Fallback to known semantic selectors
      fallback_selectors = [
        'article', 'main', '[role="main"]', 
        '.post-content', '.article-body', '#content-main'
      ]
      for selector in fallback_selectors:
        elem = soup.select_one(selector)
        if elem and is_visible(elem.get_text()):
            return elem.get_text(separator=' ', strip=True)

    # Final fallback to <body> text
      body = soup.body
      return body.get_text(separator=' ', strip=True) if body else ""

      

            
    @retry(retry=retry_if_exception_type((requests.exceptions.HTTPError, requests.exceptions.ConnectionError)),
      stop=stop_after_attempt(5),
      wait=wait_exponential(multiplier=1, min=2, max=60))
    def throttled_request(self, url):
      """Make a rate-limited request with adaptive backoff"""
      with self.request_lock:
        self.rate_limiter.wait()
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            if response.status_code == 200:
                self.rate_limiter.success()
            else:
                self.rate_limiter.failure()
            return response
        except Exception as e:
            self.rate_limiter.failure()
            raise
    def manage_memory(self):
      """Optimize memory usage based on current workload"""
      current_mem = self.vm.percent()
    
    # Adjust batch sizes dynamically
      if current_mem > 98:
        self.batch_size = max(64, self.batch_size // 2)
        self.book_processing_batch = max(1, self.book_processing_batch // 2)
        logger.warning(f"Memory pressure: Reduced batch sizes to {self.batch_size}/{self.book_processing_batch}")
    
    # Clear caches when memory is low
      if current_mem > 99:
        self.context_window.clear()
        gc.collect()
        logger.warning("Critical memory pressure: Cleared context window and forced GC")
    
    # Adjust vectorizer memory usage
      if hasattr(self.vectorizer, 'partial_fit'):
        self.vectorizer.max_mem = GodlikeAI.USER_CONFIG_SCHEMA['memory_limit_mb']['default'] * 1024 * 1024 #ERROR GODLIKE AI HAS NO USER CONFIG SCHEMA MEMBER
    def close(self):
      """Safely close all database connections"""
      if hasattr(self, 'db_pool'):
        while True:
            try:
                conn = self.db_pool.get_nowait()
                conn.close()
            except queue.Empty:
                break

    def __del__(self):
      try:
        self.close()
      except Exception as e:
        logger.error(f"Cleanup error: {str(e)}")
      
      
    @contextmanager
    def _get_cursor(self):
        """Thread-safe cursor acquisition with connection pooling"""
        conn = None
        # Acquire the database lock for the entire context
        with self.db_lock:
            try:
                # Get a connection from the pool
                conn = self.db_pool.get(timeout=30) # Use the pool
                conn.execute("PRAGMA busy_timeout=60000") # Ensure busy timeout is set for this connection
                cursor = conn.cursor()
                
                yield cursor
                
                # If no exception occurred, commit the transaction
                conn.commit()
                
            except sqlite3.Error as e:
                if conn:
                    conn.rollback() # Rollback on error
                raise # Re-raise the exception
            finally:
                if conn:
                    # Always return the connection to the pool
                    try:
                        self.db_pool.put(conn, timeout=5)
                    except queue.Full:
                        # If the pool is full (shouldn't happen with a reasonable size), close the connection
                        conn.close()

        
    
    def collect_more_data(self, min_samples=1000):
      """Collect data from multiple sources until minimum samples reached"""
    # Initialize data sources
      self.update_seeds()
      self.start_commoncrawl_processing()
    
      while self._get_training_data_count() < min_samples:
        # Process books
        self.update_book_index()
        if self.book_index:
            self.process_book(random.choice(range(len(self.book_index))))
        
        # Process web pages
        if WEB_CRAWLER_CONFIG['seed_urls']:
            url = random.choice(WEB_CRAWLER_CONFIG['seed_urls'])
            self.process_webpage(url)
        
        # Process CommonCrawl data
        self._process_cc_batch()
        
        time.sleep(1)
        
    def setup_web_crawler(self):
      """Configure web crawler with proper etiquette"""
      self.user_agent = WEB_CRAWLER_CONFIG['user_agent']
      self.max_workers = WEB_CRAWLER_CONFIG['commoncrawl']['parallel_requests']
      self.robot_parsers = {}
      self.crawl_delays = {} 
      self.domain_queues = defaultdict(queue.PriorityQueue)  # Changed from defaultdict
      self.cc_index = []
      self.cc_warc_queue = queue.Queue()
      self.cc_processing_lock = Lock()
      self.headers = {
        "User-Agent": self.user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive"
      }
      self.domain_queues = defaultdict(lambda: PriorityQueue(maxsize=10000))
      self.domain_workers = defaultdict(int)
      
    def get_robots_rules(self, base_url):
      domain = urlparse(base_url).netloc
      with self._get_cursor() as cursor:
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS robots_cache (
                domain TEXT PRIMARY KEY,
                robots_txt TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )''')
        cursor.execute("SELECT robots_txt FROM robots_cache WHERE domain = ?", (domain,))
        row = cursor.fetchone()
        if row:
            robots_txt = row[0]
            rp = RobotFileParser()
            rp.parse(robots_txt.splitlines())
            return rp

    # Fetch fresh copy if not in cache
      rp = RobotFileParser()
      try:
        robots_url = urljoin(base_url, "/robots.txt")
        response = requests.get(robots_url, timeout=10)
        response.raise_for_status()
        robots_txt = response.text
        rp.parse(robots_txt.splitlines())
        with self._get_cursor() as cursor:
            cursor.execute('''
                INSERT OR REPLACE INTO robots_cache 
                (domain, robots_txt) 
                VALUES (?, ?)''', (domain, robots_txt))
      except requests.HTTPError as e:
        if e.response.status_code == 404:
            rp.parse([])  # Empty rules for non-existent robots.txt
        else:
            raise
      return rp
        
    def _add_to_recovery_queue(self, item_type, item):
      """Add failed processing items to recovery queue with type metadata"""
      if not hasattr(self, '_recovery_queue'):
        self._recovery_queue = deque(maxlen=100)
      self._recovery_queue.append({
        'type': item_type,
        'item': item,
        'timestamp': time.time(),
        'retries': 0
      })
    def _process_recovery_queue(self):
      """Process items from recovery queue with backoff strategy"""
      while self._recovery_queue:
        item = self._recovery_queue.popleft()
        item['retries'] += 1
        
        if item['retries'] > 5:
            logger.warning(f"Permanently failed {item['type']}: {item['item']}")
            if item['type'] == 'website':
                self.blacklisted_domains.add(item['item']['domain'])
            continue

        try:
            if item['type'] == 'book':
                success = self.process_book(item['item']['index'])
            elif item['type'] == 'website':
                success = self.process_webpage(item['item']['url'])
                
            if not success:
                item['retry_after'] = time.time() + (60 * item['retries'])
                self._recovery_queue.append(item)
        except Exception as e:
            logger.error(f"Recovery processing error: {str(e)}")
            item['retry_after'] = time.time() + (60 * item['retries'])
            self._recovery_queue.append(item)
            
    def check_for_leakage(self):
        """Check for data leakage between sets"""
        with self._get_cursor() as cursor:
            cursor.execute("""
                SELECT COUNT(*) FROM training_data
                WHERE text IN (SELECT text FROM validation_data)
                OR text IN (SELECT text FROM test_data)
            """)
            return cursor.fetchone()[0] > 0

    def _get_validation_count(self):
        """Get number of validation samples"""
        with self._get_cursor() as cursor:
            cursor.execute("SELECT COUNT(*) FROM validation_data")
            return cursor.fetchone()[0]

    def test_set_contaminated(self):
        """Check if test set contains training data"""
        with self._get_cursor() as cursor:
            cursor.execute("""
                SELECT COUNT(*) FROM test_data
                WHERE text IN (SELECT text FROM training_data)
            """)
            return cursor.fetchone()[0] > 0

    def update_seeds(self):
      """Update seed URLs from a wide range of modern and educational sources"""
      if WEB_CRAWLER_CONFIG['offline_mode']:
        logger.info("Offline mode - seed update skipped")
        return
      try:
        headers = {"User-Agent": self.user_agent}
        
        # Wikipedia: Math, Programming, Physics
        topics = [
    "Classical_mechanics",
    "Electromagnetism",
    "Optics",
    "Acoustics",
    "Fluid_dynamics",
    "Plasma_physics",
    "Particle_physics",
    "Nuclear_physics",
    "Statistical_mechanics",
    "Solid_state_physics",
    "Chaos_theory",
    "Entropy",
    "Enthalpy",
    "Heat_transfer",
    "Thermodynamic_cycles",
    "Carnot_engine",
    "Laws_of_thermodynamics",
    "Gibbs_free_energy",
    "Thermal_expansion",
    "Inorganic_chemistry",
    "Physical_chemistry",
    "Analytical_chemistry",
    "Electrochemistry",
    "Medicinal_chemistry",
    "Quantum_chemistry",
    "Chemical_bond",
    "Periodic_table",
    "Reaction_kinetics",
    "Spectroscopy",
    "Organic_chemistry",
    "Green_chemistry",
    "Supramolecular_chemistry",
    "Cosmology",
    "Big_Bang",
    "Dark_matter",
    "Dark_energy",
    "Stellar_evolution",
    "Black_hole",
    "Exoplanet",
    "Space_time",
    "Gravitational_waves",
    "Astrobiology",
    "Helioseismology",
    "Cosmic_microwave_background",
    "Oceanography",
    "Seismology",
    "Volcanology",
    "Paleontology",
    "Hydrology",
    "Glaciology",
    "Geomorphology",
    "Atmospheric_science",
    "Geochemistry",
    "Geophysics",
    "Molecular_biology",
    "Cell_biology",
    "Microbiology",
    "Immunology",
    "Developmental_biology",
    "Endocrinology",
    "Botany",
    "Zoology",
    "Taxonomy",
    "Genetics",
    "Biochemistry",
    "Evolutionary_biology",
    "Ecology",
    "Bioinformatics",
    "Neuroanatomy",
    "Cognitive_science",
    "Neuroplasticity",
    "Synapse",
    "Brain_mapping",
    "Neurochemistry",
    "Neurophysiology",
    "Sensory_processing",
    "Behavioral_psychology",
    "Developmental_psychology",
    "Social_psychology",
    "Abnormal_psychology",
    "Personality_psychology",
    "Cognitive_bias",
    "Intelligence",
    "Neuropsychology",
    "Evolutionary_psychology",
    "Positive_psychology",
    "Psychometrics",
    "Mathematical_logic",
    "Algebra",
    "Group_theory",
    "Ring_theory",
    "Graph_theory",
    "Category_theory",
    "Optimization",
    "Mathematical_modeling",
    "Calculus",
    "Number_theory",
    "Probability_theory",
    "Statistics",
    "Topology",
    "Set_theory",
    "Differential_equations",
    "Game_theory",
    "Software_architecture",
    "Operating_system_kernel",
    "Distributed_computing",
    "High_performance_computing",
    "Information_theory",
    "Human-computer_interaction",
    "Internet_of_things",
    "Edge_computing",
    "Software_testing",
    "Systems_programming",
    "Functional_programming",
    "Machine_learning",
    "Artificial_intelligence",
    "Computer_vision",
    "Natural_language_processing",
    "Cryptography",
    "Compiler_design",
    "Data_structures",
    "Algorithms",
    "Cybersecurity",
    "Mechatronics",
    "Control_systems",
    "Industrial_engineering",
    "Structural_engineering",
    "Petroleum_engineering",
    "Environmental_engineering",
    "Electrical_engineering",
    "Mechanical_engineering",
    "Civil_engineering",
    "Aerospace_engineering",
    "Materials_science",
    "Robotics",
    "Medical_genetics",
    "Pathology",
    "Radiology",
    "Psychiatry",
    "Immunotherapy",
    "Biomedical_engineering",
    "Clinical_trials",
    "Health_informatics",
    "Epidemiology",
    "Anatomy",
    "Pharmacology",
    "Surgery",
    "Nutrition",
    "Nursing",
    "Public_health",
    "Epistemology",
    "Metaphysics",
    "Phenomenology",
    "Philosophy_of_science",
    "Political_philosophy",
    "Utilitarianism",
    "Deontology",
    "Ethics",
    "Logic",
    "Existentialism",
    "Philosophy_of_mind",
    "Aesthetics",
    "Archaeology",
    "Semiotics",
    "Comparative_religion",
    "Mythology",
    "Social_contract",
    "Feminist_theory",
    "Critical_race_theory",
    "Queer_theory",
    "Marxism",
    "Linguistics",
    "Anthropology",
    "Cultural_studies",
    "Sociology",
    "Historiography",
    "Postcolonial_theory",
    "Media_studies",
    "Gender_studies",
    "Economics",
    "Political_science",
    "Urban_survival",
    "Food_preservation",
    "Tactical_medicine",
    "Improvised_weapons",
    "Foraging",
    "Camouflage",
    "Tracking",
    "Trapping",
    "Cold_weather_survival",
    "Desert_survival",
    "Wilderness_first_aid",
    "Shelter_construction",
    "Navigation",
    "Weapon_maintenance",
    "Conflict_resolution",
    "Emotional_intelligence",
    "Organizational_behavior",
    "Decision_making",
    "Self-regulation",
    "Motivational_psychology",
    "Financial_literacy",
    "Time_management",
    "Negotiation",
    "Mindfulness",
    "Leadership",
    "Public_speaking",
    "Goal_setting",
    "Critical_thinking",
    "Creative_problem_solving",
    "Stress_management",
    "Geothermal_energy",
    "Solar_power",
    "Wind_power",
    "Nuclear_power",
    "Carbon_capture",
    "Ecological_footprint",
    "Environmental_policy",
    "Biofuels",
    "Sustainable_agriculture",
    "Climate_modeling",
    "Waste_management",
    "Renewable_energy_policy",
    "Music_composition",
    "Music_performance",
    "Film_studies",
    "Photography",
    "Graphic_design",
    "Architecture",
    "Fashion_design",
    "Interior_design",
    "Mixology",
    "Gastronomy",
    "Kinesiology",
    "Theater_arts",
    "Sculpture",
    "Painting",
    "Calligraphy",
    "Culinary_arts",
    "Tattoo_art",
    "Animation",
    "Birth",
    "Childhood",
    "Adolescence",
    "Parenting",
    "Old_age",
    "Death",
    "Grief",
    "Love",
    "Sexuality",
    "Gender_identity",
    "Friendship",
    "Ritual",
    "Dreams",
    "Emotion",
    "Language",
    "Storytelling",
    "Morality",
    "Law",
    "War",
    "Peace",
    "Art",
    "Religion",
    "Technology",
    "Agriculture",
    "Trade",
    "Commerce",
    "Housing",
    "Transportation",
    "Education",
    "Leisure",
    "Sports",
    "Games",
    "Music",
    "Dance",
    "Festivals",
    "Belief_systems",
    "Authority",
    "Crime",
    "Punishment",
    "Justice",
    "Innovation",
    "Curiosity",
    "Exploration",
    "Courage",
    "Fear",
    "Sleep",
    "Nutrition_science",
    "Microeconomics",
    "Macroeconomics",
    "Addiction",
    "Digital_culture",
    "Virtual_reality",
    "Cyberpsychology",
    "Space_colonization",
    "Transhumanism"
        ]


        for topic in topics:
            response = requests.get(f"https://en.wikipedia.org/wiki/{topic}", headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.select('.mw-parser-output a[href^="/wiki/"]'):
                url = urljoin("https://en.wikipedia.org/", link['href'])
                if url not in WEB_CRAWLER_CONFIG['seed_urls']:
                    WEB_CRAWLER_CONFIG['seed_urls'].append(url)

        # Common Crawl
        cc_index = self.fetch_commoncrawl_index()
        for entry in cc_index[:100]:
            if entry['url'] not in WEB_CRAWLER_CONFIG['seed_urls']:
                WEB_CRAWLER_CONFIG['seed_urls'].append(entry['url'])

        # GitHub Trending (Python, JavaScript, C++)
        langs = [
    "python", "javascript", "cpp", "java", "c", "csharp", "go", "ruby", "rust", "php",
    "typescript", "kotlin", "swift", "scala", "shell", "perl", "dart", "elixir", "haskell",
    "lua", "r", "julia", "objective-c", "groovy", "powershell", "assembly", "matlab", "fortran"
        ]
        for lang in langs:
            gh_response = requests.get(f"https://github.com/trending/{lang}", headers=headers)
            gh_soup = BeautifulSoup(gh_response.text, 'html.parser')
            for link in gh_soup.select('h2 > a'):
                url = urljoin("https://github.com", link.get('href'))
                if url not in WEB_CRAWLER_CONFIG['seed_urls']:
                    WEB_CRAWLER_CONFIG['seed_urls'].append(url)

        # StackOverflow Hot Questions
        so_urls_to_add = []

# Loop through first 50 pages of hot questions
        for page in range(1, 50):  # Adjust this number to go deeper
          so_response = requests.get(f"https://stackoverflow.com/questions?sort=hot&page={page}", headers=headers)
          so_soup = BeautifulSoup(so_response.text, 'html.parser')
          for link in so_soup.select('a.question-hyperlink'):
            url = urljoin("https://stackoverflow.com", link.get('href'))
            if url not in WEB_CRAWLER_CONFIG['seed_urls'] and url not in so_urls_to_add:
              so_urls_to_add.append(url)

        WEB_CRAWLER_CONFIG['seed_urls'].extend(so_urls_to_add)


        # Hacker News
        hn_response = requests.get("https://news.ycombinator.com/", headers=headers)
        hn_soup = BeautifulSoup(hn_response.text, 'html.parser')
        for link in hn_soup.select('a.storylink'):
            url = link.get('href')
            if url and url not in WEB_CRAWLER_CONFIG['seed_urls']:
                WEB_CRAWLER_CONFIG['seed_urls'].append(url)
        

        
        



        # DevDocs.io main docs
        devdocs = [
            "https://devdocs.io/python~3.11/",
            "https://devdocs.io/javascript/",
            "https://devdocs.io/cpp/",
            "https://devdocs.io/html/",
            "https://devdocs.io/css/",
            "https://devdocs.io/linux/",
            "https://devdocs.io/git/"
        ]
        for doc_url in devdocs:
            if doc_url not in WEB_CRAWLER_CONFIG['seed_urls']:
                WEB_CRAWLER_CONFIG['seed_urls'].append(doc_url)
        
        

        def extract_urls_from_sitemap(xml_text):
    # Extracts all <loc> URLs from XML using regex
          return re.findall(r"<loc>(.*?)</loc>", xml_text)

        def crawl_google_news():
          index_url = "https://www.google.com/news/sitemapindex.xml"
          try:
            index_resp = requests.get(index_url, headers=headers, timeout=10)
            sitemap_urls = extract_urls_from_sitemap(index_resp.text)

            for sitemap_url in sitemap_urls:
                try:
                    sm_resp = requests.get(sitemap_url, headers=headers, timeout=10)
                    urls = extract_urls_from_sitemap(sm_resp.text)

                    for url in urls:
                        if self.is_url_alive(url) and url not in WEB_CRAWLER_CONFIG['seed_urls']:
                            WEB_CRAWLER_CONFIG['seed_urls'].append(url)

                except Exception as e:
                    print(f"[!] Failed to fetch sitemap: {sitemap_url} — {e}")

          except Exception as e:
            print(f"[!] Failed to fetch index sitemap — {e}")
        

      except Exception as e:
        print(f"Seed update error: {e}")



    def is_url_alive(self,url):
      try:
        r = requests.head(url, timeout=5, allow_redirects=True)
        return r.status_code == 200
      except:
        return False 
        
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def ethical_request(self, url, method="GET", params=None, data=None):
      """Make web requests with proper etiquette"""
      if WEB_CRAWLER_CONFIG['offline_mode'] or not self.check_network_status():
        logger.warning(f"Offline mode - skipping request to {url}")
        raise requests.exceptions.ConnectionError("Offline mode enabled")
      domain = urlparse(url).netloc
      parsed = urlparse(url) 
      if time.time() < self.domain_timeouts.get(domain, 0):
        logger.warning(f"Domain {domain} in timeout, skipping")
        raise requests.exceptions.RequestException(f"Domain {domain} temporarily blocked")
      if parsed.netloc in self.blacklisted_domains:
        raise ValueError(f"Domain {parsed.netloc} is blacklisted")
    # Check robots.txt
      rp = self.get_robots_rules(url)
      if rp and not rp.can_fetch(self.user_agent, url):
        logger.warning(f"URL disallowed by robots.txt: {url}")
        return None
    
    # Respect crawl delay
      delay = self.crawl_delays.get(domain, GUTENBERG_ROBOTS_DELAY)
      self.rate_limiter.wait()
      try:
        response = requests.request(
            method=method,
            url=url,
            params=params,
            data=data,
            headers=self.headers,
            timeout=30
        )
        if response.status_code == 429:
          retry_after = int(response.headers.get('Retry-After', 300))  # Default to 5 minutes
          logger.warning(f"Rate limited on {domain}, retry after {retry_after} seconds")
          self.domain_timeouts[domain] = time.time() + retry_after
          self.rate_limiter.failure(retry_after=retry_after)
          raise requests.exceptions.HTTPError(f"Rate limited on {domain}, retry after {retry_after} seconds")
        #add more if need
        
      except Exception as e:
        self.rate_limiter.failure()
        raise
        
        # Check if we're being blocked
        
    def fetch_commoncrawl_index(self):
      """Fetch latest Common Crawl index info"""
      index_list_url = WEB_CRAWLER_CONFIG['commoncrawl']['index_url']
      logger.info(f"Fetching Common Crawl index list from: {index_list_url}")
      try:
        # Use ethical_request to handle potential retries/rate limits if necessary
        # Although this is usually a fast, single request.
        response = requests.get(index_list_url, timeout=20, headers=self.headers)
        response.raise_for_status()
        index_data = response.json()
        logger.info(f"Successfully fetched {len(index_data)} crawl indexes.")
        return index_data
      except requests.exceptions.RequestException as e:
        logger.error(f"Failed to fetch CC index list from {index_list_url}: {str(e)}")
        return []
      except json.JSONDecodeError as e:
        logger.error(f"Failed to parse CC index JSON from {index_list_url}: {str(e)}")
        return []
      except Exception as e:
        logger.error(f"An unexpected error occurred fetching CC index: {str(e)}")
        return [] 
    def _fetch_warc_paths(self, crawl_id):
        """Fetch the list of WARC file paths for a specific crawl ID."""
        # Construct the URL for the warc.paths.gz file
        # Example path: crawl-data/CC-MAIN-2023-50/warc.paths.gz
        # The base URL might vary slightly, adjust if needed based on CC documentation
        warc_paths_url = f"https://data.commoncrawl.org/crawl-data/{crawl_id}/warc.paths.gz"
        logger.info(f"Fetching WARC paths for crawl {crawl_id} from {warc_paths_url}")

        warc_paths = []
        try:
            # Use requests with streaming to handle potentially large files
            response = requests.get(warc_paths_url, stream=True, timeout=60, headers=self.headers)
            response.raise_for_status()

            # Decompress and read line by line
            # Use io.BufferedReader for efficient reading of the binary stream
            with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz_file:
                 # Decode lines using utf-8, ignoring errors
                 for line in io.TextIOWrapper(gz_file, encoding='utf-8', errors='ignore'):
                     path = line.strip()
                     if path: # Ensure the line is not empty
                         warc_paths.append(path)

            logger.info(f"Successfully fetched {len(warc_paths)} WARC paths for crawl {crawl_id}.")
            return warc_paths

        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to fetch WARC paths file from {warc_paths_url}: {str(e)}")
            return []
        except gzip.BadGzipFile:
             logger.error(f"Failed to decompress WARC paths file from {warc_paths_url}. It might be corrupted or not gzipped.")
             return []
        except Exception as e:
            logger.error(f"An unexpected error occurred fetching WARC paths for {crawl_id}: {str(e)}")
            return []

    def _process_cc_batch(self):
      """Process a batch of CommonCrawl records"""
      if not self.cc_warc_queue.empty():
        try:
            warc_path = self.cc_warc_queue.get_nowait()
            self.process_cc_warc(warc_path)
        except queue.Empty:
            pass
    def process_cc_warc(self, warc_path):
      try:
        # Check existing progress
        with self._get_cursor() as cursor:
            cursor.execute("SELECT pages_processed FROM cc_processed WHERE warc_path=?", (warc_path,))
            row = cursor.fetchone()
            initial_pages = row['pages_processed'] if row else 0

        s3_url = f"{WEB_CRAWLER_CONFIG['commoncrawl']['warc_base']}{warc_path}"
        processed_count = initial_pages

        with requests.get(s3_url, stream=True) as response:
            def update_progress(progress):
                with self.lock:
                    self.web_crawl_stats['warc_progress'][warc_path] = round(progress * 100, 1)
                    self.web_crawl_stats["processed_count"] = processed_count
            progress_stream = ProgressStream(response, update_progress)
            
            for i, record in enumerate(ArchiveIterator(progress_stream)):
                if i < initial_pages:
                    continue  # Skip already processed records

                try:
                    if record.rec_type != 'response':
                        continue
                    
                    content_type = record.http_headers.get_header('Content-Type', '')
                    if 'text/plain' not in content_type:
                        continue

                    raw_content = record.content_stream().read()
                    try:
                        raw_text = raw_content.decode('utf-8', errors='replace')
                    except UnicodeDecodeError:
                        continue

                    if self.smart_lang_detect(raw_text[:500]) != 'en':
                        continue

                    url = record.rec_headers.get('WARC-Target-URI', '')
                    if not url or self.filter_low_quality_text_or_url(url):
                        continue

                    if self.process_webpage(url, content=raw_content):
                        processed_count += 1

                    # Update progress periodically
                    if processed_count % 100 == 0:
                        with self._get_cursor() as cursor:
                            cursor.execute('''
                                INSERT OR REPLACE INTO cc_processed 
                                (warc_path, pages_processed)
                                VALUES (?, ?)
                            ''', (warc_path, processed_count))
                            cursor.connection.commit()

                except Exception as e:
                    logger.error(f"WARC record processing failed: {str(e)}")

        # Final update
        with self._get_cursor() as cursor:
            cursor.execute('''
                INSERT OR REPLACE INTO cc_processed 
                (warc_path, pages_processed)
                VALUES (?, ?)
            ''', (warc_path, processed_count))
            cursor.connection.commit()

      except Exception as e:
        logger.error(f"WARC processing failed: {str(e)}")
      finally:
        # Cleanup after 60 seconds whether success or fail
        def delayed_cleanup():
            time.sleep(60)
            with self.lock:
                if warc_path in self.web_crawl_stats['warc_progress']:
                    del self.web_crawl_stats['warc_progress'][warc_path]
                    
        Thread(target=delayed_cleanup, daemon=True).start()
        
        if 'response' in locals():
            response.close()
        gc.collect()
        
    
    def _init_db(self):
      """Initialize SQLite database"""
      
      logger.info("Initializing database connection pool")
    
    # Create database directory if needed
      try:
        os.makedirs(DATA_DIR, exist_ok=True)
        logger.info(f"Database directory: {DATA_DIR}")
      except Exception as e:
        logger.critical(f"Could not create data directory: {str(e)}")
        raise

    # Initialize connection pool
      connection_attempts = 0
      while self.db_pool.qsize() < 5 and connection_attempts < 10:
        try: 
            conn = self._create_new_connection()
            if self._validate_connection(conn):
              try:
                self.db_pool.put_nowait(conn)
                logger.debug(f"Added connection to pool (total: {self.db_pool.qsize()})")
              except queue.Full:
                break
            else:
              conn.close()
        except Exception as e:
            logger.error(f"Connection initialization failed (attempt {connection_attempts+1}): {str(e)}")
            connection_attempts += 1
        time.sleep(0.1)

      if self.db_pool.qsize() == 0:
        logger.critical("Failed to initialize any database connections")
        raise RuntimeError("Database connection pool initialization failed")

      logger.info(f"Database pool initialized with {self.db_pool.qsize()} connections") 

      self.thread_local = local()     
      try:
          def DB_REINIT():
            with self._get_cursor() as cursor:
                cursor.execute('''
        CREATE TABLE IF NOT EXISTS cc_processed (
            warc_path TEXT PRIMARY KEY,
            processed_date DATETIME DEFAULT CURRENT_TIMESTAMP,
            pages_processed INTEGER
        )
    ''')
                cursor.execute('''
        CREATE TABLE IF NOT EXISTS model_versions (
            id INTEGER PRIMARY KEY,
            timestamp DATETIME,
            model_data BLOB,
            vectorizer_data BLOB,
            metadata TEXT,
            version INTEGER DEFAULT 1
        )
    ''')
                cursor.execute('''
CREATE TABLE IF NOT EXISTS system_state (
    id INTEGER PRIMARY KEY,
    user_profile TEXT,
    book_index TEXT,
    learning_progress TEXT,
    training_stats TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
)''')
                cursor.execute('''
    CREATE TABLE IF NOT EXISTS validation_data (
        id INTEGER PRIMARY KEY,
        text TEXT UNIQUE,
        label TEXT,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )''')
        
        
                cursor.execute('''
    CREATE TABLE IF NOT EXISTS test_data (
        id INTEGER PRIMARY KEY,
        text TEXT UNIQUE,
        label TEXT,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    )''') 
                cursor.execute('''CREATE TABLE IF NOT EXISTS training_data (
    id INTEGER PRIMARY KEY,
    text TEXT CHECK(LENGTH(text) >= 20),
    label TEXT CHECK(label GLOB '*[a-zA-Z]*'),
    processed INTEGER DEFAULT 0 CHECK(processed IN (0,1)),
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    features TEXT,
    text_hash TEXT,
    UNIQUE(text_hash, label) -- Add composite unique constraint here
)''')
# Add composite index for faster queries:
                cursor.execute('''
    CREATE TABLE IF NOT EXISTS cc_processed (
        warc_path TEXT PRIMARY KEY,
        processed_date DATETIME DEFAULT CURRENT_TIMESTAMP,
        pages_processed INTEGER
    )
''')
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='training_data'")
                cursor.execute('''CREATE TABLE IF NOT EXISTS model_versions
    (id INTEGER PRIMARY KEY,
    timestamp DATETIME,
    vectorizer_path TEXT,
    model_paths TEXT, 
    metadata TEXT)''')  
                  
                cursor.execute('''
    CREATE TABLE IF NOT EXISTS web_pages (
        id INTEGER PRIMARY KEY,
        url TEXT UNIQUE,
        domain TEXT,
        content TEXT CHECK(LENGTH(content) >= 20),
        genre TEXT,
        status_code INTEGER,
        content_length INTEGER,
        crawl_duration REAL,
        links_found INTEGER,
        processed INTEGER DEFAULT 0 CHECK(processed IN (0,1)),
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    )
''')
                cursor.execute('''
    CREATE TABLE IF NOT EXISTS robots_cache (
        domain TEXT PRIMARY KEY,
        rules BLOB,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    )
''')
                cursor.execute('''CREATE TABLE IF NOT EXISTS validation_data
                 (id INTEGER PRIMARY KEY, text TEXT, label TEXT)''')
                cursor.execute('''CREATE TABLE IF NOT EXISTS test_data
                 (id INTEGER PRIMARY KEY, text TEXT, label TEXT)''')  # NEW
                cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_training_data_text_hash 
            ON training_data (text_hash)
        ''')
                
                cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_web_pages_genre 
            ON web_pages (genre)
        ''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_compound 
    ON training_data (processed, label, timestamp)
                ''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_web_pages_compound 
    ON web_pages (processed, genre, domain)
                ''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_label_processed 
    ON training_data (label, processed)
''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_web_pages_domain_processed 
    ON web_pages (domain, processed)
''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_label_text 
    ON training_data (label, text)
''')               
                
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_web_pages_url 
    ON web_pages (url)
''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_web_domain 
    ON web_pages (domain)
                  ''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_processed
    ON training_data (processed)
                  ''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_web_processed
    ON web_pages (processed)
                  ''')
              # Create validation and test tables
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_processed_label 
    ON training_data (processed, label)
              ''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_processed 
    ON training_data (processed)
''')
                
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_processed 
               ON training_data (processed)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_label 
               ON training_data (label)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_val_text 
               ON validation_data (text)''')  # <-- Add this
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_test 
               ON test_data (label)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_training_processed 
                  ON training_data (processed)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_validation_label 
                  ON validation_data (label)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_training_text 
    ON training_data (text)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_validation_label_time 
    ON validation_data (label, timestamp)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_validation_label 
               ON validation_data (label)''')
                cursor.execute('''CREATE INDEX IF NOT EXISTS idx_validation_text 
               ON validation_data (text)''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_val_text_label 
    ON validation_data (text, label)''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_label_processed 
    ON training_data (label, processed) 
        ''') 
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_processed_label 
    ON training_data (processed, label)
''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_training_data_label_ts 
    ON training_data (label, timestamp)
''')
                cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_web_pages_processed_genre
    ON web_pages (processed, genre)
''')
                
                cursor.connection.commit()
                logger.info("Starting database initialization...")
                self._initialize_filesystem()
                logger.info('Database tables created succesfully') #this prints, but nothing happnes after 
          try:
            DB_REINIT()
          except Exception as e:
            logger.error(f"Database initialization error: {e}")
            self._repair_database()
            DB_REINIT()
      except (sqlite3.OperationalError, sqlite3.IntegrityError) as e: 
                logger.error(f"Database initialization error: {e}")
                self._repair_database()
                DB_REINIT() 
      try:
        with self._get_cursor() as cursor:
            cursor.execute("PRAGMA table_info(training_data)")
            if len(cursor.fetchall()) < 5:
                raise ValueError("Invalid table structure")
      except Exception as e:
        logger.critical(f"Database schema validation failed: {str(e)}")
        self._repair_database()
      # Get all training data
      with self.db_lock:
          
            with self._get_cursor() as cursor:  # Auto-closes cursor
              cursor.execute("SELECT COUNT(*) FROM training_data")
              count = cursor.fetchone()[0]
              if count == 0:
                logger.warning("No data available for fallback training")
                return
                
      with self.db_lock:
          
            with self._get_cursor() as cursor:
                cursor.execute("SELECT text, label FROM training_data")
                data = cursor.fetchall()
    
      if not data:
          logger.warning("No data available to train fallback genre classifier")
          return
    
      texts = [row[0] for row in data]
      labels = [row[1].split('::')[0] for row in data]  # Extract genre from combined label
    
      try:
          X = self.fallback_vectorizer.fit_transform(texts)
          self.genre_classifier.fit(X, labels)
          logger.info("Fallback genre classifier trained successfully")
        
      except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
          logger.error(f"Failed to train fallback classifier: {str(e)}")
          
    
     
    def validate_training_data(self):
      """Comprehensive data validation with integrity checks"""
      checks = {
        'text_length': "LENGTH(text) >= 20",
        'valid_labels': "label GLOB '*[a-zA-Z]*'",
        'no_html': "text NOT LIKE '%<%'",
        'no_urls': "text NOT LIKE '%http%'"
      }
    
      with self._get_cursor() as cursor:
        # Run data quality checks
        for check_name, condition in checks.items():
            cursor.execute(f"""
                SELECT COUNT(*) 
                FROM training_data 
                WHERE NOT ({condition})
            """)
            invalid_count = cursor.fetchone()[0]
            if invalid_count > 0:
                logger.warning(f"Found {invalid_count} invalid records for {check_name}")
                
        # Clean invalid data
        cursor.execute("""
            DELETE FROM training_data
            WHERE NOT (
                LENGTH(text) >= 20 AND
                label GLOB '*[a-zA-Z]*' AND
                text NOT LIKE '%<%' AND 
                text NOT LIKE '%http%'
            )
        """)
        
        # Optimize database
        cursor.execute("VACUUM")
        cursor.execute("ANALYZE")
    
      return True
     
    def _train_fallback_genre_classifier(self): 
      """Train fallback classifier with data validation"""  
      print('train fallback...')
      
      if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
        # Initialize with genre descriptions first
        genre_texts = list(self.genre_descriptions.values())
        self.fallback_vectorizer.fit(genre_texts) 
      
      if True:  
        with self._get_cursor() as cursor:   
            try:  
                cursor.execute("SELECT COUNT(*) FROM training_data")         
                count_row = cursor.fetchone()
                count = count_row[0] if count_row else 0
            except sqlite3.OperationalError as e:
                logger.error(f"Error checking training data count: {str(e)}")
                # Create table and insert warmup data 
                with self._get_cursor() as cursor: 
                  self._initialize_filesystem()
                  self._init_db()
                  warmup_data = [(text, label) for text, label in self.warmup_texts]
                  cursor.executemany('''
                    INSERT OR IGNORE INTO training_data 
                    (text, label, processed)
                    VALUES (?, ?, 0)
                ''', warmup_data)
                  cursor.connection.commit()
                # Re-execute count query after setup
                  cursor.execute("SELECT COUNT(*) FROM training_data")
                  count_row = cursor.fetchone()
                  count = count_row[0] if count_row else 0  
                  self.warmup_first = False 
                  
                  logger.info("succesfully recreated training_tables and inserted warmup data")
            except Exception as e:
                logger.error(f"Unexpected error during fallback training setup: {str(e)}")
                count = 0
                
            if count < 100:
                logger.warning("Insufficient data for fallback training")
                self.fallback_trained = False
                return
            
            # Proceed to fetch training data and train classifier
            cursor.execute("SELECT text, label FROM training_data")
            data = cursor.fetchall()

      texts = [row[0] for row in data]
      labels = [row[1].split('::')[0] for row in data]

      if len(set(labels)) < 2:
        logger.warning("Insufficient class diversity for fallback")
        return
    
      try:
        X = self.fallback_vectorizer.transform(texts)
        self.genre_classifier.fit(X, labels)
        self.fallback_trained = True 
        
        logger.info("Fallback genre classifier trained successfully")
      except Exception as e:
        logger.error(f"Failed to train fallback classifier: {str(e)}")
        self.fallback_trained = False
     
    
 
    
    @staticmethod
    def clean_text(text):
      text = text.lower().strip() 
      for pattern, repl in GodlikeAI._CLEAN_REGEX: 
        text = pattern.sub(repl, text)
      return text
    #IMPROVE THE LANGUAGE MODEL
    LANGUAGE_MODELS = {
    'en': ['the', 'and', 'ing', 'ion', 'ent', 'her', 'tha', 'nth', 'int', 'ere'],
    'fr': ['ent', 'les', 'des', 'que', 'est', 'dans', 'une', 'par', 'sur', 'ion'],
    'de': ['der', 'die', 'und', 'ein', 'ich', 'den', 'von', 'mit', 'nicht', 'ung'],
    'es': ['que', 'ent', 'los', 'del', 'est', 'con', 'por', 'una', 'las', 'ció'],
    'it': ['che', 'del', 'lla', 'ell', 'ere', 'sta', 'per', 'ent', 'ati', 'non'],
    'pt': ['que', 'com', 'ent', 'uma', 'por', 'est', 'ão ', 'nte', 'ara', 'sta'],
    'nl': ['van', 'het', 'een', 'der', 'ten', 'nde', 'gen', 'ver', 'sch', 'aan'],
    'sv': ['och', 'att', 'den', 'som', 'för', 'med', 'det', 'har', 'inte', 'era'],
    'no': ['det', 'som', 'har', 'med', 'den', 'ikke', 'for', 'til', 'han', 'var'],
    'da': ['der', 'det', 'men', 'for', 'har', 'med', 'sig', 'fra', 'den', 'var'],
    'fi': ['sta', 'tti', 'ssa', 'een', 'lla', 'sta', 'ksi', 'den', 'een', 'lle'],
    'pl': ['nie', 'sie', 'owa', 'ych', 'owa', 'ego', 'owi', 'ści', 'owe', 'one'],
    'cs': ['sta', 'ost', 'ní ', 'níc', 'ova', 'jem', 'ých', 'ten', 'pod', 'sta'],
    'sk': ['sta', 'ova', 'pre', 'nie', 'len', 'tia', 'ého', 'jem', 'ých', 'sta'],
    'sl': ['sta', 'nje', 'tem', 'ste', 'pri', 'sem', 'jen', 'res', 'sko', 'lah'],
    'ru': ['про', 'ост', 'ени', 'тов', 'ств', 'ого', 'ать', 'ска', 'его', 'оль'],
    'uk': ['про', 'ого', 'ені', 'аль', 'ован', 'сти', 'ють', 'ість', 'ння', 'ого'],
    'zh': ['的中国', '是一', '人民', '社会主', '主义', '发展', '经济', '文化', '国家', '政府'],
    'ja': ['です', 'する', 'こと', 'いる', 'なる', 'この', 'から', 'して', 'それ', 'よう'],
    'ko': ['습니다', '하다', '하고', '에서', '하는', '이다', '하는', '으로', '라고', '하면'],
    'ar': ['التي', 'والتي', 'الذي', 'الله', 'ذلك', 'عليه', 'قال', 'منها', 'كانت', 'عند'],
    'he': ['שלי', 'שלום', 'ישראל', 'אחד', 'כמו', 'אנשים', 'ילד', 'בית', 'חיים', 'שלום'],
    'hi': ['के ल', 'में ', 'ही ह', 'से ह', 'यह ', 'जो ह', 'था ', 'कर ', 'का ह', 'ने क'],
    'th': ['ที่ ', 'เป็น', 'ใน ', 'ไม่ ', 'ว่า ', 'มี ', 'และ', 'ได้ ', 'จะ ', 'ก็ '],
    }
    
    def _repair_database(self):
      logger.critical("Attempting database repair...")
      try:
        # Close existing connections and clean up
        self.close()
        if hasattr(self, 'db_pool'):
            self.db_pool = None

        db_path = os.path.join(DATA_DIR, "training_data.db")
        
        # iOS-compatible file handling
        if os.path.exists(db_path):
            try:
                # Attempt to delete corrupted database
                os.remove(db_path)
                logger.info("Removed corrupted database file")
            except Exception as e:
                logger.error(f"File removal failed: {str(e)}")
                # For iOS: Create unique filename instead of overwriting
                new_name = f"corrupted_{int(time.time())}.db"
                os.replace(db_path, os.path.join(DATA_DIR, new_name))
                logger.info(f"Renamed corrupted file to {new_name}")

        # Reinitialize with clean database
        self._initialize_filesystem()
        
        # Reload essential data
        self._load_emergency_cache()
        logger.info("Database rebuilt with emergency data")
        
      except Exception as e:
        logger.critical(f"Database repair failed: {str(e)}")
        # Final fallback - in-memory database
        self.db_path = ":memory:"
        self._initialize_filesystem()
        self._load_emergency_cache()
        
    
    @staticmethod
    def get_trigrams(text):
      text = GodlikeAI.clean_text(text)
      trigrams = [text[i:i+3] for i in range(len(text) - 2)]
      return Counter(trigrams)
    @staticmethod
    def compare_profiles(input_profile, lang_profile):
      score = 0.0
      for i, trigram in enumerate(lang_profile):
        weight = 1.0 / ((i + 1) ** 2)  # Stronger positional weighting
        score += 1 / (1 + input_profile.get(trigram, 0)) * weight # more matches = higher score
      return score
    @staticmethod
    def smart_lang_detect(text):
      input_trigrams = GodlikeAI.get_trigrams(text)
      lang_scores = {}
    
      for lang, profile in GodlikeAI.LANGUAGE_MODELS.items():
        lang_scores[lang] = GodlikeAI.compare_profiles(input_trigrams, profile)
    
      if not lang_scores:
        return (None, {})
    
    # Softmax for probabilities
      max_score = max(lang_scores.values())
      exp_scores = {lang: math.exp(score - max_score) for lang, score in lang_scores.items()}
      total = sum(exp_scores.values())
      probabilities = {lang: exp_score/total for lang, exp_score in exp_scores.items()}
    
      return max(lang_scores, key=lang_scores.get) if lang_scores else 'en', probabilities
    
    def _calculate_semantic_relevance(self, text):
      """Calculate semantic relevance score for context weighting"""
      if not self.context_window:
        return 1.0  # Maximum relevance if no existing context
      if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
        self._train_fallback_genre_classifier()
    # Encode current text: 
      try:
        current_vec = self.fallback_vectorizer.transform([text])
        context_vecs = self.fallback_vectorizer.transform([item['text'] for item in self.context_window])
        similarities = cosine_similarity(current_vec, context_vecs)
        return np.mean(similarities) if similarities else 0.5 
      except ValueError:
        return 0.5
    def _extract_text_features(self, text):
      """Extract linguistic features for enhanced analysis"""
      features = {}
    
    # Basic stats
      features['length'] = len(text)
      features['word_count'] = len(text.split())
      features['unique_words'] = len(set(text.split()))
    
    # Readability metrics
      sentences = re.split(r'[.!?]+', text)
      features['sentence_count'] = len(sentences)
      features['avg_sentence_length'] = features['word_count']/len(sentences) if sentences else 0
    
    # Sentiment indicators
      features['exclamation_count'] = text.count('!')
      features['question_count'] = text.count('?')
    
    # Part-of-speech patterns (approximate)
      features['adjective_count'] = len(re.findall(r'\b(amazing|good|great|excellent|poor|bad|terrible)\b', text, re.I))
      features['modal_verb_count'] = len(re.findall(r'\b(can|should|would|could|must)\b', text, re.I))
    
      return features
    def _get_thread_connection(self):
      """Get or create a connection for the current thread"""
      if not hasattr(self.thread_local, 'conn'):
        self.thread_local.conn = self._create_new_connection() #causees journal WAL error disk io error?
      return self.thread_local.conn
    def _init_offline_resources(self):
        """Load essential cached resources for offline operation"""
        logger.info("Initializing offline resources from database")
        self.offline_cache = {
            'book_index': [],
            'web_cache': deque(maxlen=5000),
            'model_checkpoints': [],
            'last_verified': time.time()
        }

        # Load cached books
        with self._get_cursor() as cursor:
            cursor.execute("""
                SELECT text, label 
                FROM training_data 
                WHERE label LIKE 'book:%'
                ORDER BY RANDOM()
                LIMIT 1000
            """)
            book_count = 0
            for text, label in cursor:
                if self.validate_and_clean_text(text):
                    self.offline_cache['book_index'].append((text, label))
                    book_count += 1

        # Load cached web pages
        with self._get_cursor() as cursor:
            cursor.execute("""
                SELECT content, genre 
                FROM web_pages 
                WHERE LENGTH(content) >= 100
                ORDER BY RANDOM()
                LIMIT 10000
            """)
            web_count = 0
            for content, genre in cursor:
                if self.validate_and_clean_text(content):
                    self.offline_cache['web_cache'].append((content, genre))
                    web_count += 1

        logger.info(f"Loaded offline cache: {book_count} books, {web_count} web pages")

        # Validate minimum dataset requirements
        if book_count < 100 or web_count < 1000:
            logger.warning("Insufficient offline data - loading emergency dataset")
            self._load_emergency_cache() 
            
    def process_in_chunks(self, text, label, chunk_size=5000):
      """Generator to process text in manageable chunks"""
      if not isinstance(text, str) or not text.strip():
        return  # Skip invalid text
      words = text.split()
      for i in range(0, len(words), chunk_size):
        yield ' '.join(words[i:i+chunk_size]), label
        
    def update_context(self, text):
      """Enhanced memory management with attention weighting"""
      with self.lock:
        now = time.time()
        
        # Calculate attention score using multiple factors
        length_weight = math.log(len(text) + 1)
        recency_weight = 1.0 / (1.0 + len(self.context_window))
        semantic_weight = self._calculate_semantic_relevance(text)
        
        attention_score = (length_weight * 0.3 + 
                          recency_weight * 0.2 + 
                          semantic_weight * 0.5)
        
        # Apply decay to existing memories
        for item in self.context_window:
            age = now - item['timestamp']
            item['weight'] *= math.exp(-self.context_decay * age)
        
        # Add new memory with combined attention score
        new_memory = {
            'text': text,
            'timestamp': now,
            'weight': attention_score,
            'access_count': 0
        }
        self.context_window = deque(
                sorted(self.context_window, key=lambda x: -x['weight']),  
                maxlen=self.max_context_length
            )
        # Manage memory capacity
        if len(self.context_window) >= self.max_context_length:
            # Remove lowest weighted item
            self.context_window.remove(min(self.context_window, key=lambda x: x['weight']))
            
        self.context_window.append(new_memory)
        
        # Keep sorted by composite weight 
        
        sorted_context = sorted(self.context_window, key=lambda x: -x['weight'])
        self.context_window = deque(sorted_context, maxlen=self.max_context_length)

    def expand_keywords(self, keywords):
      expanded = set(keywords)
      for word in keywords:
        if word in self.genre_descriptions:
            expanded.update(self.genre_descriptions[word])
      return expanded

    def determine_genre(self,title, paragraphs=None):
      if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
        self._train_fallback_genre_classifier()
        if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
          return "unknown" 
      if not hasattr(self.genre_classifier, 'classes_'):
        self._train_fallback_genre_classifier()
      if self.genre_model is None and not self.fallback_trained:
        logger.info("Training fallback genre classifier")
        self._train_fallback_genre_classifier()
      if self.genre_model is None and not self.fallback_trained:
        logger.warning("Genre classifiers not initialized")
        return "unknown"
      content = title.lower()
      if paragraphs:
        valid_paras = [p.strip().lower() for p in paragraphs if p.strip()]
        if valid_paras:
            sampled_paras = random.sample(valid_paras, min(3, len(valid_paras)))
            content += ' ' + ' '.join(sampled_paras)
    
      if not content.strip():
        return "web_content" if "http" in title.lower() else "non-fiction" if len(title.split()) > 3 else "unknown"
      if self.smart_lang_detect(content) != 'en':
        raise ValueError("Non-English content detected")  # Added proper error handling
    # Try ML-based (TF-IDF + cosine similarity) if model available
      try:
        if self.genre_model is not None and hasattr(self.fallback_vectorizer, 'vocabulary_'):
            texts = list(self.genre_descriptions.values())
            labels = list(self.genre_descriptions.keys())
            vec = self.fallback_vectorizer.transform([content])
            genre_vecs = self.fallback_vectorizer.transform(texts)
            sims = cosine_similarity(vec, genre_vecs)[0]
            best = labels[np.argmax(sims)]
            if best in self.genre_descriptions:
                return best
      except Exception as e:
        logger.warning(f"TF-IDF genre detection failed: {e}")

    # Fallback: Use contextual clues and conceptual patterns from title + paragraphs
      words = set(re.findall(r'\b\w+\b', content))
      title = str(title) if title else ""
      title_words = set(re.findall(r'\b\w+\b', title.lower()))
      expanded = {genre: self.expand_keywords(keywords) for genre, keywords in self.genre_descriptions.items()} #undefined variable expand keywords error
      scores = defaultdict(int)

    # Increase score based on keyword matches or thematic alignment, prioritizing the title
      for genre, kwset in expanded.items():
        for kw in kwset:
            if kw in words:
                scores[genre] += 2 if kw in title_words else 1

      if scores:
        max_score = max(scores.values())
        top = [g for g, s in scores.items() if s == max_score]
        return top[0]

    # Heuristics for non-fiction (more expanded patterns)
      if re.search(r'\b(18|19|20|21)\d{2}\b', content) or re.search(r'\b(?:[A-Z][a-z]+(?: [A-Z][a-z]+)+)\b', content):
        return 'non-fiction'

      if len(words) > 5:
        return 'fiction'

      return 'unknown' 
      
    

# Utility function to apply filters to text
    def filter_low_quality_text_or_url(self, text, patterns=None):
      """
    Filter text based on patterns indicating low-quality content.
    
    Args:
        text (str): The text to filter
        patterns (list): Regular expression patterns to match
        specialized_patterns (dict): Dictionary of specialized pattern groups
        
    Returns:
        bool: True if the text should be excluded, False otherwise
      """
      if patterns is None:
        patterns = excluded_patterns
        
    # Check main exclusion patterns
      for pattern in patterns:
        if re.search(pattern, text):
            return True
    
    # Check specialized patterns if provided
      if patterns:
        for category, pattern_list in patterns.items():
            matches = 0
            for pattern in pattern_list:
                if re.search(pattern, text):
                    matches += 1
            
            # If multiple patterns from the same category match, likely low quality
            if matches >= 3:  # Threshold can be adjusted
                return True
                
      return False
    def evaluate_test_set(self):
      """Evaluate model performance on test set"""
      if not self.fitted:
        logger.warning("Model not trained yet")
        return 0.0
    
      with self.db_lock:
        
            with self._get_cursor() as cursor:
              cursor.execute("SELECT text, label FROM test_data")
              test_data = cursor.fetchall()
            
      test_texts = [row[0] for row in test_data]
      test_labels = [row[1] for row in test_data]
       
      try:
         X_test = self.vectorizer.transform(test_texts) #if test data avilable
      except ValueError as e:
         logger.error(f"Vectorization error: {str(e)}")
         return 0.0
    
      
    
      try:
        X_test = self.vectorizer.transform(test_texts)
        model = self.models[self.active_model]
        preds = model.predict(X_test)
        accuracy = accuracy_score(test_labels, preds)
        logger.info(f"Test Set Accuracy ({self.active_model}): {accuracy:.4f}")
        return accuracy
      except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
        logger.error(f"Test evaluation failed: {str(e)}")
        return 0.0

    
    
    def _autosave_loop(self):
      """Background autosave task."""
      logger.info("Autosave system started")
      while True:
        try:
            time.sleep(self.autosave_interval)
            with self.lock:
                self.save_state()
                self.save_model_version()
            logger.debug("Autosave completed")
            self.last_save_time = time.time()
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Autosave failed: {str(e)}")
            time.sleep(60)
                
    def validate_and_clean_text(self, text):
      """Validate and clean text data before processing"""
      if text is None:
        return None
        
      # Check if we have actual text content
      if not isinstance(text, str):
        try:
            text = str(text)
        except:
            return None
    
      # Remove unwanted characters and normalize
      text = re.sub(r'[\r\n\t]+', ' ', text)  # Replace newlines and tabs with spaces
      text = re.sub(r'\s{2,}', ' ', text)     # Replace multiple spaces with single space
      text = text.strip()                     # Remove leading/trailing whitespace
    
      
        
      text = re.sub(r'[^\x20-\x7E]', ' ', text)
      # Check meaningful contenmap
      if len(set(text)) < 20:  # If text has fewer than 20 unique characters
         return None
      if len(re.findall(r'\w{3,}', text)) < 3:
        return None
      return text.strip()
      # Check for nonsensical or corrupt text
      
      try:
        text = text.encode('utf-8', 'ignore').decode('utf-8')
      except UnicodeDecodeError:
        return None
      # Return validated and cleaned text
      return text 
     
    def _init_autosave(self):
        """Safe ation of autosave system"""
        if self.autosave_thread is None:
            self.autosave_thread = Thread(
                target=self._autosave_loop,
                daemon=True,
                name="AutosaveThread"
            )
            self.autosave_thread.start()
            
    def run_in_foreground(self):
          """Keep processing active for iPad background execution."""
          logger.info("Starting foreground persistence mode")
          self.foreground_event.clear()
        
          while not self.foreground_event.is_set():
            try:
                # Maintain learning activity
                if not self.training_in_progress:
                    self.start_learning()
                
                # Process system tasks
                self._maintain_connection()
                
                # iPad-friendly sleep
                time.sleep(0.5)
                
            except KeyboardInterrupt:
              self.stop_learning()
              self.foreground_event.set()
            except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
              logger.error(f"Foreground loop error: {str(e)}")
              time.sleep(5)

    def _maintain_connection(self):
          """Keep network connections alive."""
          if random.random() < 0.1:  # 10% chance per loop
            try:
                requests.get(GUTENBERG_BASE_URL, timeout=5)
            except:
                pass
        
        
        
    
    def _handle_corrupted_memory(self):
      logger.error("Memory file corrupted. Reinitializing...")
      self.models = self._initialize_fresh_models()
      self.fitted = False
    
    
  
            
    def _create_default_profile(self):
        """Create and save default user profile."""
        default_profile = {
    "preferences": {
        "batch_size": GodlikeAI.USER_CONFIG_SCHEMA['batch_size']['default'],
        "active_model": GodlikeAI.USER_CONFIG_SCHEMA['active_model']['default'],
        "memory_limit_mb": GodlikeAI.USER_CONFIG_SCHEMA['memory_limit_mb']['default'],
        "learning_rate": GodlikeAI.USER_CONFIG_SCHEMA['learning_rate']['default']
    }
        }
        try:
          with self.db_lock:
            with self._get_cursor() as cursor:
                cursor.execute('''
                    INSERT INTO system_state 
                    (user_profile, book_index, learning_progress, training_stats)
                    VALUES (?, ?, ?, ?)
                ''', (
                    json.dumps(default_profile),
                    json.dumps([]),
                    json.dumps(self._create_default_progress()),
                    json.dumps(self._create_default_stats())
                ))
                cursor.connection.commit()
        except sqlite3.Error as e:
          logger.error(f"Error saving default profile: {str(e)}")
        return default_profile
 
    
    
    
    def save_state(self):
      """Save complete system state to database"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                INSERT INTO system_state 
                (user_profile, book_index, learning_progress, training_stats)
                VALUES (?, ?, ?, ?)
            ''', (
                json.dumps(self.user_profile),
                json.dumps(self.book_index),
                json.dumps(self.learning_progress),
                json.dumps(self.training_stats)
            ))
            # Keep last 5 states
            cursor.execute('''
                DELETE FROM system_state 
                WHERE id NOT IN (
                    SELECT id FROM system_state 
                    ORDER BY timestamp DESC 
                    LIMIT 5
                )
            ''')
            cursor.connection.commit()
      logger.info("System state saved")

    def load_state(self):
      """Load complete system state from database"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                SELECT user_profile, book_index, learning_progress, training_stats
                FROM system_state 
                ORDER BY timestamp DESC LIMIT 1
            ''')
            row = cursor.fetchone()
    
      if row:
        self.user_profile = json.loads(row[0])
        self.book_index = json.loads(row[1])
        self.learning_progress = json.loads(row[2]) 
        self.training_stats = json.loads(row[3])
        logger.info("Loaded system state from database")
      else:
        logger.info("Initializing new system state")
        self.user_profile = self._create_default_profile()
        self.book_index = []
        self.learning_progress = self._create_default_progress()
        self.training_stats = self._create_default_stats()
        self.save_state()
    
    #add more to version if needed
    def save_model_version(self):
      """Save current model version to database"""
      model_data = pickle.dumps(self.models)
      vectorizer_data = pickle.dumps(self.vectorizer)
    
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                INSERT INTO model_versions 
                (timestamp, model_data, vectorizer_data, metadata)
                VALUES (?, ?, ?, ?)
            ''', (
                datetime.now().isoformat(),
                model_data,
                vectorizer_data,
                json.dumps({
                    'sklearn_version': sklearn.__version__,
                    'model_version': str(self.model_version)
                })
            ))
            # Keep last 3 versions
            cursor.execute('''
                DELETE FROM model_versions 
                WHERE id NOT IN (
                    SELECT id FROM model_versions 
                    ORDER BY timestamp DESC 
                    LIMIT 3
                )
            ''')
            cursor.connection.commit()

    def load_model_version(self):
      """Load latest model version from database"""
      row = None
      try:
        with self.db_lock:
            with self._get_cursor() as cursor:
                cursor.execute('''
                    SELECT model_data, vectorizer_data, metadata 
                    FROM model_versions 
                    ORDER BY timestamp DESC LIMIT 1
                ''')
                row = cursor.fetchone()

        if row and len(row[0]) > 100:  # Basic corruption check
            self.models = pickle.loads(row[0])
            self.vectorizer = pickle.loads(row[1])
            self.fitted = True
            logger.info("Loaded existing model version")
        else:
            logger.info("No valid model found, initializing fresh")
            self._initialize_fresh_models()
            self.fitted = False
            
      except Exception as e:
        logger.error(f"Model loading failed: {str(e)}")
        self._initialize_fresh_models()
        self.fitted = False
        self._train_fallback_genre_classifier()  # Ensure basic functionality

      if not row:
            logger.info("No model versions found, initializing fresh")
            self._initialize_fresh_models()
            self.fitted = False
            return

      try:
            model_data = pickle.loads(row[0])
            vectorizer_data = pickle.loads(row[1])
            metadata = json.loads(row[2])
            
            
            if version.parse(str(metadata.get('model_version', '1.0'))) != version.parse(str(self.model_version)):
              logger.warning("Model version mismatch, reinitializing")
              self.model_version = metadata.get('model_version', 1)  # Add this line
              self._initialize_fresh_models()
              self.fitted = False
              return
      except Exception as e:
        if WEB_CRAWLER_CONFIG['offline_mode']:
            logger.critical("Offline mode - using emergency models")
            self._initialize_emergency_models()
            self.fitted = True
        else:
            raise

      if row:
        
        try:
            models = str(pickle.loads(row[0]))
            vectorizer = str(pickle.loads(row[1]))
            metadata = json.loads(row[2])
            
            if metadata.get('sklearn_version') != sklearn.__version__:
                raise ValueError("Sklearn version mismatch")
                
            self.models = models
            self.vectorizer = vectorizer 
            if metadata.get('model_version', 1) != self.model_version:
              logger.warning("Model version mismatch, reinitializing")
              self._initialize_fresh_models()
              self.fitted = False
            else:
              self.models = models
              self.vectorizer = vectorizer
              self.fitted = True
            if len(row[0]) < 100 or len(row[1]) < 100:
                raise ValueError("Invalid model data length")
                
            
            self.fitted = True
            
            logger.info("Loaded model version from database")
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            self._initialize_fresh_models()
      else:
        logger.info("No model versions found, initializing fresh")
        self._initialize_fresh_models()
        self.fitted = False


    
            
    def _create_default_progress(self):
      return {
        "completed_books": [],
        "partially_processed_books": {
            # Structure: {book_id: {"paragraph_index": int, "retries": int}}
        },
        "processing_queue": [],
        "last_update": datetime.now().isoformat()
      }

    


    
            
    def _create_default_stats(self):
        """Create default training statistics structure."""
        return {
            "accuracy_history": [],
            "training_batches": 0,
            "total_samples_processed": 0,
            "model_evaluations": {}
        }
    
    
    def validate_text(self, text):
        """Comprehensive text validation with linguistic checks"""
        if not isinstance(text, str) or len(text) < 20:
            return False
            
        # Check meaningful content
        char_ratio = len(set(text)) / len(text) if text else 0
        if char_ratio < 0.4:  # Too repetitive
            return False
            
        # Check for valid word structures
        word_check = re.findall(r'\b[a-zA-Z]{3,}\b', text)
        if len(word_check) < 3:
            return False
            
        # Sentiment coherence check
        
        #if False:
          #if self.sentiment_analyzer: #self. sentiment analyzer is not callable
            #sentiment = self.sentiment_analyzer(text[:512])[0]
            #if sentiment['score'] < 0.4:  # Neutral/confusing text
                #return False
                
        return True
    def _initialize_emergency_models(self):
      """Fallback models for offline availability"""
      self.models = {
        "naive_bayes": MultinomialNB(alpha=0.1),
        "sgd": SGDClassifier(loss='log')
      }
      logger.warning("Using emergency fallback models")
    def _validate_offline_data(self):
      """ Ensure minimum viable dataset for offline operation"""
      checks = [
        ('training_data', 500),
        ('web_pages', 1000),
        ('model_versions', 1)
      ]
    
      with self._get_cursor() as cursor:
        for table, min_rows in checks:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            if count < min_rows:
                logger.error(f"Insufficient {table} for offline: {count}/{min_rows}")
                return False
      return True
    def get_class_balance(self):
      """Check if any class has <50% of maximum class size"""
      with self._get_cursor() as cursor:
        cursor.execute('''
            SELECT label, COUNT(*) as count 
            FROM training_data 
            GROUP BY label
        ''')
        counts = [row[1] for row in cursor.fetchall()]
        if not counts:
            return True
        max_count = max(counts)
        return all(c >= (max_count * 0.5) for c in counts)
        
    def train(self, input_text, label):
      
      try:
          with self.db_lock:
            input_text = self.validate_and_clean_text(input_text)
            if not input_text or not isinstance(label, str):
              return False
            if not isinstance(input_text, str) or not isinstance(label, str):
              return False
            cleaned_text = self.validate_and_clean_text(input_text)
            if not cleaned_text or len(cleaned_text) < 20:
              return False
            if not self.validate_text(input_text):
              return False
            if not self.validate_label(label):
              return False 
            if not isinstance(input_text, str) or len(input_text) < 20:
              return False
            if not re.search(r'\w{3,}', input_text):
              return False
            if not self.validate_label(label):
              logger.error(f"Invalid label format: {label}")
              return False 
            text_hash = sha256(input_text.encode()).hexdigest()
            with self._get_cursor() as cursor:
                with self._get_cursor() as cursor:
                  cursor.execute('''
            SELECT id FROM training_data 
            WHERE text_hash = ? AND label = ?
            LIMIT 1
        ''', (text_hash, label)) 
                    
            with self.db_lock:
              if not input_text or not input_text.strip():
                  return False
            with self._get_cursor() as cursor:
              cursor.execute("SELECT text, label FROM training_data WHERE processed=0 LIMIT ?", (self.batch_size,))
              batch_rows = cursor.fetchall()
            if len(batch_rows) < 100:
              logger.warning(f"Insufficient batch size {len(batch_rows)}, skipping")
              return False
              
            features = self._extract_text_features(cleaned_text)
            text_hash = sha256(cleaned_text.encode()).hexdigest() 

# Update INSERT to include hash
            cursor.execute('''
    INSERT OR IGNORE INTO training_data 
    (text, label, processed, text_hash) 
    VALUES (?, ?, 0, ?)
''', (cleaned_text, label, text_hash))
    # Store features in database
            with self._get_cursor() as cursor:
              cursor.execute('''
            INSERT OR IGNORE INTO training_data 
            (text, label, processed, features) 
            VALUES (?, ?, 0, ?)
        ''', (cleaned_text, label, json.dumps(features)))
            if self.enable_sentiment_analysis:
              texts = [row[0] for row in batch_rows]
              sentiment_labels = [1 if any(word in text.lower() for word in ['good', 'great', 'excellent']) 
                        else 0 for text in texts]
              self.sentiment_analyzer.named_steps['classifier'].partial_fit(  # Access classifier directly
                self.vectorizer.transform(texts), 
                sentiment_labels,
                classes=[0, 1]
              )
            # Determine data split (10% test, 10% validation, 80% train)
            rand = random.random()
            if rand < 0.1:  # Test data
                with self._get_cursor() as cursor:
                      cursor.execute('INSERT INTO test_data (text, label) VALUES (?,?)',
                                 (input_text, label))
                return True
            elif rand < 0.2:  # Validation data
                
                with self._get_cursor() as cursor:
                      cursor.execute('INSERT INTO validation_data (text, label) VALUES (?,?)',
                                 (input_text, label))
                      val_size = max(10, int(len(batch_rows) * 0.1))  # Ensure minimum validation samples
                      if len(batch_rows) - val_size < 50:
                        logger.warning("Insufficient training samples after split")
                        return False
                return True
            else:  # Training data
                
                with self._get_cursor() as cursor:
                      try:
                        with self._get_cursor() as cursor:
                          cursor.execute('''
            INSERT OR IGNORE INTO training_data 
            (text, label, processed) 
            VALUES (?, ?, 0)
        ''', (cleaned_text, label))
                      except sqlite3.IntegrityError as e:
                          logger.debug(f"Duplicate entry skipped: {cleaned_text[:50]}...")
                          return True
                      except sqlite3.Error as e:
                        logger.error(f"Database error: {str(e)}")
                        return False
                
                # Check unprocessed count
                cursor.execute('SELECT COUNT(*) FROM training_data WHERE processed = 0')
                unprocessed_count = cursor.fetchone()[0]
                if unprocessed_count >= self.batch_size:
                    self._train_batch()
                return True
      except sqlite3.Error as e:
        logger.error(f"Database error in train(): {str(e)}")
        return False
        
    def save_memory(self):
      """Save current state to database"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                UPDATE system_state SET
                user_profile = ?,
                book_index = ?,
                learning_progress = ?,
                training_stats = ?
                ORDER BY timestamp DESC LIMIT 1
            ''', (
                json.dumps(self.user_profile),
                json.dumps(self.book_index),
                json.dumps(self.learning_progress),
                json.dumps(self.training_stats)
            ))
            cursor.connection.commit()

    def save_training_stats(self):
      """Save training statistics to database"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                INSERT INTO training_stats (stats_data)
                VALUES (?)
            ''', (json.dumps(self.training_stats),))
            cursor.connection.commit()

    def save_book_index(self):
      """Save book index to database"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                UPDATE system_state SET
                book_index = ?
                ORDER BY timestamp DESC LIMIT 1
            ''', (json.dumps(self.book_index),))
            cursor.connection.commit()

    def save_learning_progress(self):
      """Save learning progress to database"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute('''
                UPDATE system_state SET
                learning_progress = ?
                ORDER BY timestamp DESC LIMIT 1
            ''', (json.dumps(self.learning_progress),))
            cursor.connection.commit()
        
    def train_models(self, save_checkpoints=True):
      """Train models with improved progress tracking and checkpointing"""
      X_train, X_test, y_train, y_test = [], [], [], []
      if self.training_in_progress:
        logger.warning("Training already in progress")
        return False
    
      try:
        self.training_in_progress = True
        self.stop_training = False
        
        # Load training data
        with self.db_lock:
            with self._get_cursor() as cursor:
              cursor.execute('''
    SELECT text, label 
    FROM training_data 
    WHERE processed=1 
    AND id NOT IN (SELECT id FROM validation_data)
            ''')
              data = cursor.fetchall()
              cursor.close()
        
        if not data:
            logger.warning("No processed data available for training")
            return False
        
        texts = [row[0] for row in data]
        labels = [row[1] for row in data]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Transform data once
        logger.info("Vectorizing training data...")
        X_train_vec = self.vectorizer.transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)
        
        results = {}
        
        if X_train.shape[0] < 10:  # <-- Add this check
          logger.warning("Insufficient training samples after split")
          return False
        
        # Train each model with progress tracking
        for model_name, model in self.models.items():
            if self.stop_training:
                logger.info("Training stopped by user")
                break
            
            logger.info(f"Training model: {model_name}")
            
            # Training with progress feedback
            if model_name == "random_forest":
                if hasattr(model, 'estimators_') and len(model.estimators_) > 0:
                  current_estimators = len(model.estimators_) if hasattr(model, 'estimators_') else 0
                  new_estimators = 50  # Number of new trees to add
    
    # Update model parameters
                  model.set_params(n_estimators=current_estimators + new_estimators)
                  model.fit(X_train_vec, y_train)
                else:
        # Add trees incrementally
                  new_trees = 10
                  model.n_estimators += new_trees
                  model.fit(X_train, y_train)
                # For models that don't implement partial_fit
                n_estimators = model.n_estimators
                model.n_estimators = 1  # Start with one tree
                
                # Create initial model
                model.fit(X_train_vec, y_train)
                
                # Add trees incrementally
                for i in tqdm(range(1, n_estimators), desc=f"Training {model_name}"):
                    if self.stop_training:
                        break
                    
                    # Add more trees
                    model.n_estimators = i + 1
                    model.fit(X_train_vec, y_train)
                    
                    # Periodic checkpoint
                    if save_checkpoints and i % 10 == 0:
                        self.save_memory()
            else:
                # For models supporting partial_fit like MultinomialNB
                # Get unique classes
                unique_classes = list(set(y_train))
                
                # Initial fit
                model.partial_fit(X_train_vec[:100], y_train[:100], classes=unique_classes)
                
                # Continue training in batches
                batch_size = self.batch_size
                num_batches = (len(X_train_vec) - 100) // batch_size + 1
                
                for i in tqdm(range(num_batches), desc=f"Training {model_name}"):
                    if self.stop_training:
                        break
                        
                    start_idx = 100 + i * batch_size
                    end_idx = min(start_idx + batch_size, len(X_train_vec))
                    
                    if start_idx >= len(X_train_vec):
                        break
                        
                    model.partial_fit(
                        X_train_vec[start_idx:end_idx], 
                        y_train[start_idx:end_idx]
                    )
                    
                    # Periodic checkpoint
                    if save_checkpoints and i % 10 == 0:
                        self.save_memory()
            
            # Evaluate model
            y_pred = model.predict(X_test_vec)
            accuracy = accuracy_score(y_test, y_pred)
            
            logger.info(f"{model_name} accuracy: {accuracy:.4f}")
            
            results[model_name] = {
                "accuracy": accuracy,
                "test_size": len(y_test),
                "trained_at": datetime.now().isoformat()
            }
        
        # Update training stats
        self.training_stats.update(results)
        self.save_training_stats()
        self.save_memory()
        
        logger.info("Model training completed successfully")
        self.fitted = True
        return True
    
      except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
        logger.error(f"Training error: {str(e)}")
        return False
      finally:
        self.training_in_progress = False
        gc.collect()  # Force garbage collection
    
    def _optimize_hyperparameters(self, X_train, y_train):
      """Optimize hyperparameters for all models"""
      if len(np.unique(y_train)) < 2:
        return

    # Naive Bayes optimization
      if "naive_bayes" in self.models:
        param_grid = {'alpha': loguniform(1e-2, 100)}
        search = RandomizedSearchCV(
            MultinomialNB(), param_grid, n_iter=10, cv=3, n_jobs=-1
        )
        search.fit(X_train, y_train)
        self.models["naive_bayes"] = search.best_estimator_

    # Random Forest optimization
      if "random_forest" in self.models:
        param_dist = {
            'n_estimators': [100, 200, 300],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10]
        }
        search = RandomizedSearchCV(
            RandomForestClassifier(n_jobs=-1),
            param_dist, n_iter=10, cv=3, n_jobs=-1
        )
        search.fit(X_train, y_train)
        self.models["random_forest"] = search.best_estimator_

    # Gradient Boosting optimization  
      if "gradient_boosting" in self.models:
        param_dist = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.05, 0.1],
            'max_depth': [3, 5, 7]
        }
        search = RandomizedSearchCV(
            GradientBoostingClassifier(),
            param_dist, n_iter=10, cv=3, n_jobs=-1
        )
        search.fit(X_train, y_train)
        self.models["gradient_boosting"] = search.best_estimator_

    # SGD optimization
      if "sgd" in self.models:
        param_dist = {
            'alpha': loguniform(1e-6, 1e-3),
            'penalty': ['l2', 'elasticnet'],
            'l1_ratio': [0.15, 0.3, 0.5]
        }
        search = RandomizedSearchCV(
            SGDClassifier(loss='log'),
            param_dist, n_iter=10, cv=3, n_jobs=-1
        )
        search.fit(X_train, y_train)
        self.models["sgd"] = search.best_estimator_
    
    def balance_classes(self):
      """Balance class distribution using SMOTE-inspired approach"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            # Get class distribution
            cursor.execute('''
                SELECT label, COUNT(*) as count 
                FROM training_data 
                WHERE processed=0 
                GROUP BY label
            ''')
            class_dist = {row[0]: row[1] for row in cursor.fetchall()}
            
            if not class_dist:
                return False
                
            max_samples = max(class_dist.values())
            balanced_batch = []
            
            # Generate synthetic samples for minority classes
            for label, count in class_dist.items():
                if count < max_samples:
                    cursor.execute('''
                        SELECT text 
                        FROM training_data 
                        WHERE label=? 
                        ORDER BY RANDOM() 
                        LIMIT ?
                    ''', (label, max_samples - count))
                    
                    samples = [row[0] for row in cursor.fetchall()]
                    for sample in samples:
                        # Simple text augmentation
                        words = sample.split()
                        if len(words) > 5:
                            # Shuffle words to create synthetic sample
                            random.shuffle(words)
                            balanced_batch.append((
                                ' '.join(words),
                                label
                            ))
            
            # Insert synthetic samples
            if balanced_batch:
                cursor.executemany('''
                    INSERT INTO training_data (text, label, processed)
                    VALUES (?, ?, 0)
                ''', balanced_batch)
                cursor.connection.commit()
                logger.info(f"Added {len(balanced_batch)} synthetic samples")
                
            return True
    def _validate_offline_training_data(self):
      """Ensure minimum data quality for offline training"""
      with self._get_cursor() as cursor:
        cursor.execute("""
            SELECT COUNT(DISTINCT label) 
            FROM training_data 
            WHERE processed=0
        """)
        unique_classes = cursor.fetchone()[0]
        return unique_classes >= 3 and self._get_training_data_count() >= 1000
    def _load_emergency_cache(self):
      """Load pre-packaged training data for offline use"""
      #emergency_data_raw= [ this is format ofimported list
        #("scientific research methodology", "science"),
       # ("philosophical concepts overview", "philosophy"),
        #("advanced mathematics principles", "science"),
       # ("literary analysis techniques", "literature")
      #]
      emergency_data = [
            (text, label) for text, label in emergency_data_raw  # Defined elsewhere
            if self.validate_and_clean_text(text) and self.validate_label(label)
      ]
      with self._get_cursor() as cursor:
        cursor.executemany("""
        INSERT OR IGNORE INTO training_data
        (text, label, processed, text_hash)
        VALUES (?, ?, 0, ?)
    """, [(text, label, sha256(text.encode()).hexdigest()) for text, label in emergency_data_raw
          if self.validate_and_clean_text(text) and self.validate_label(label)])

      logger.info("Loaded emergency training cache")
    def _train_batch(self, batch_to_train=None):
      if self.offline_mode and not self._validate_offline_training_data():
        logger.warning("Offline mode - Insufficient training data, using emergency cache")
        self._load_emergency_cache()  # New method added
      batch_rows = []
      X_train, X_val, y_train, y_val = [], [], [], []
      if len(self._unique_classes) < 2:
        logger.error("Insufficient classes for training")
        return False
      if not self.is_ready_to_train():
        logger.warning("System not ready for training")
        return False
      min_batch_size = 100
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=1")
            processed_count = cursor.fetchone()[0]
        if processed_count == 0:
          min_batch_size = 1  # Allow any batch size for initial training

      if len(batch_rows) < min_batch_size:
        logger.warning(f"Insufficient batch size {len(batch_rows)}, skipping")
        return False
      if (len(batch_rows) < min_batch_size) and self.warmup_first is not True:
          logger.warning(f"Batch too small: {len(batch_rows)} < {min_batch_size}")
          self.balance_classes()  # Try to augment data
          return False
      if len(set(row[2] for row in batch_rows)) < 2:
        logger.warning("Insufficient class diversity")
        return False
      if not self.is_vectorizer_ready():
        self.vectorizer.fit([d[1] for d in batch_rows])
      if not hasattr(self.vectorizer, 'vocabulary_'):
        logger.error("Vectorizer not initialized - performing emergency fit")
        sample_texts = [row[0] for row in batch_rows[:100]]
        self.vectorizer.fit(sample_texts)
        if not hasattr(self.vectorizer, 'vocabulary_'):
          raise RuntimeError("Failed to initialize vectorizer") 
      if not all([
        self.batch_size >= GodlikeAI.USER_CONFIG_SCHEMA['batch_size']['min'],
        self.active_model in self.models,
        self.max_memory_items > 0
        ]):
          logger.error("Invalid configuration detected, resetting defaults")
          self.modify_profile('batch_size', GodlikeAI.USER_CONFIG_SCHEMA['batch_size']['default'])
          self.modify_profile('active_model', GodlikeAI.USER_CONFIG_SCHEMA['active_model']['default'])
      with self.training_lock:
        """Train on unprocessed data batches"""
        with self.db_lock:
            with self._get_cursor() as cursor:
              cursor.execute('''
                SELECT content, genre 
                FROM web_pages 
                WHERE processed=0 
                LIMIT ?
            ''', (self.batch_size//2,))
              web_data = cursor.fetchall()
            
              if web_data:
                texts = [row[0] for row in web_data]
                labels = [row[1] for row in web_data]
                self._process_text_batch(texts, labels)
                
                # Mark web pages as processed
                cursor.execute('''
                    UPDATE web_pages 
                    SET processed=1 
                    WHERE rowid IN (
                        SELECT rowid 
                        FROM web_pages 
                        WHERE processed=0 
                        LIMIT ?
                    )
                ''', (self.batch_size//2,))
                
        with self.db_lock:
              with self._get_cursor() as cursor:
                conn = self._get_connection()
                try:
                  cursor = conn.cursor()
                  cursor.execute("BEGIN EXCLUSIVE")
                  
        # Transaction logic
                  conn.commit()
                except Exception as e:
                  conn.rollback()
                  raise
                finally:
                  cursor.close()
                  self._return_connection(conn)
            # Get batch with IDs
                if self.active_model == "sgd":
    # Get stratified sample
                  cursor.execute('''
        WITH ranked_data AS (
            SELECT *, 
                   ROW_NUMBER() OVER (PARTITION BY label ORDER BY RANDOM()) AS rn
            FROM training_data 
            WHERE processed = 0
        )
        SELECT text, label
        FROM ranked_data
        WHERE rn <= ?
        ORDER BY RANDOM()
    ''', (self.batch_size // len(self._unique_classes),))
                else:
                  with self.db_lock:
                    with self._get_cursor() as cursor:
            # Use window functions for consistent batch selection
                      cursor.execute('''
                SELECT text, label FROM (
                    SELECT *, ROW_NUMBER() OVER (ORDER BY RANDOM()) as rn
                    FROM training_data 
                    WHERE processed = 0
                ) WHERE rn <= ?
            ''', (self.batch_size,))
                batch_rows = cursor.fetchall()
                batch_rows = cursor.fetchmany(self.batch_size)
                cursor.execute('COMMIT')
                if len(batch_rows) < 100:  # Handle small batches differently
                  val_size = max(1, int(len(batch_rows) * 0.1))
                  if len(batch_rows) - val_size < 2:
                    logger.warning("Skipping validation for tiny batch")
                    X_train, y_train = texts, labels
                    X_val, y_val = [], []
                  else:
                    X_train, X_val, y_train, y_val = train_test_split(
            texts, labels, 
            test_size=val_size, 
            stratify=labels,
            random_state=42
                    )
                else:
                  X_train, X_val, y_train, y_val = train_test_split(
        texts, labels, 
        test_size=0.2, 
        stratify=labels,
        random_state=42
                  )
        
        
                  
        
        cursor.execute("""
    SELECT text, label 
    FROM training_data 
    WHERE timestamp < datetime('now', '-1 day')
    ORDER BY RANDOM()
    LIMIT ?
""", (self.batch_size,))

        with self.conn:
          cursor = self.conn.cursor()
          cursor.execute('BEGIN TRANSACTION')  
        
          random.shuffle(batch_rows)
          if not batch_rows:
            logger.warning("Empty training batch")
            return False 
          with self._get_cursor() as cursor:
            cursor.execute('''
            SELECT COUNT(*) FROM training_data
            WHERE text IN (SELECT text FROM validation_data)
               OR text IN (SELECT text FROM test_data)
        ''')
            if cursor.fetchone()[0] > 0:
              raise ValueError("Data leakage detected between sets!")
          val_size = max(10, int(len(batch_rows) * 0.1))  # Ensure minimum samples
          if len(batch_rows) - val_size < 20:  # Don't split tiny batches
            val_size = 0
          if len(batch_rows) >= 100:  # Only split when sufficient data
              val_size = min(max(5, int(len(batch_rows) * 0.1)), len(batch_rows) - 10)
              if val_size < 5:
                val_size = 0
              test_size = min(max(2, int(len(batch_rows) * 0.05)), len(batch_rows) - val_size - 5)
          else:
            val_size = 0
            test_size = 0
        
          stratify = [row[2] for row in batch_rows] if len(set(row[2] for row in batch_rows)) > 1 else None
        
          train_rows, val_rows = train_test_split(
          batch_rows,
          test_size=0.1,
          stratify=[row[2] for row in batch_rows],  # Use labels
          random_state=42
          ) 
        
          train_texts = [row[1] for row in train_rows]
          val_texts = [row[1] for row in val_rows]
          train_labels = [row[2] for row in train_rows]
          val_labels = [row[2] for row in val_rows]
        
          if not train_texts and not val_texts:
            logger.error("No training data available")
            return False
          min_samples = 2  # Minimum samples per class

          unique_labels, counts = np.unique(y_train, return_counts=True)
          if any(c < min_samples for c in counts):
            logger.warning("Insufficient samples per class after split")
            self.balance_classes()
            return False 
          if self.check_for_leakage():
            logger.critical("Data leakage detected! Stopping training")
            self._repair_database()
            raise RuntimeError("Data contamination")
    # Process validation data
          val_ids = [row[0] for row in val_rows]
          val_texts = [row[1] for row in val_rows]
          val_labels = [row[2] for row in val_rows]

          with self.db_lock:
          
            with self._get_cursor() as cursor:
            # Remove validation data from training set
              cursor.execute("BEGIN TRANSACTION")
              try:
                # Split and process data atomically
                train_ids, val_ids = [], []
                train_texts, train_labels = [], []
                val_texts, val_labels = [], []
                    
                    # Proper stratified split 
                train_idx, val_idx = train_test_split(
    range(len(batch_rows)),
    test_size=0.1,
    stratify=[row[2] for row in batch_rows],
    random_state=42
                )
                unique_labels = list(set(row[2] for row in batch_rows))
                if len(unique_labels) < 2:
                    logger.error("Insufficient class diversity")
                    cursor.execute("ROLLBACK")
                    return False
                if True:
                  if val_ids:
                      
                      cursor.execute('BEGIN TRANSACTION')
                      try:
                          if val_ids:
                            with self._get_cursor() as cursor:
                              placeholders = ','.join(['?']*len(val_ids))
                              cursor.execute(f"""
        DELETE FROM training_data 
        WHERE id IN ({placeholders})
    """, val_ids)
                          cursor.connection.commit()
                      except:
                          self.conn.rollback()
                          raise
                      
                # Add to validation set
                  cursor.executemany(
                  'INSERT INTO validation_data (text, label) VALUES (?,?)',
                  list(zip(val_texts, val_labels))
                  )
            # Mark training data as processed
                train_ids = [row[0] for row in train_rows]
                if train_ids:
                  cursor.execute('''
    UPDATE training_data 
    SET processed = 1 
    WHERE id IN ({seq})
    '''.format(seq=','.join(['?']*len(train_ids))), 
    train_ids)
                  cursor.connection.commit()
              except:
                  self.conn.rollback()
                  raise
     
          
          with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
            executor.submit(self._train_single_model, name, X_train, y_train, X_val, y_val): name 
            for name in self.models
            }
        
            for future in as_completed(futures):
              name = futures[future]
              try:
                metrics = future.result()
                self.training_stats["model_evaluations"][name].append(metrics)
              except Exception as e:
                logger.error(f"Error training {name}: {str(e)}")
          
          if not hasattr(self.vectorizer, 'vocabulary_'):
                logger.error("Vectorizer not properly initialized")
                return False 
                
          
          if not self.get_class_balance():
            logger.warning("Class imbalance detected - applying balancing")
            self.balance_classes()
          if not hasattr(self.vectorizer, 'vocabulary_'):
            with self._get_cursor() as cursor:
              cursor.execute("SELECT text FROM training_data LIMIT 1000")
              sample_texts = [row[0] for row in cursor.fetchall()]
              self.vectorizer.fit(sample_texts)
              
    
    # Proceed with training
          train_texts = [row[1] for row in train_rows]
          train_labels = [row[2] for row in train_rows]
    
          if not train_texts:
            logger.warning("No training data after split")
            return False
          if not train_labels:  # Add check for empty labels
             logger.error("No training labels available")
             return False
          y_train = train_labels  # Ensure y_train is assigned

          
          X_train = self.vectorizer.transform(train_texts)
          
          if X_train.shape[0] == 0: 
            raise ValueError("Empty training batch")
          if X_train.getnnz() == 0:
            raise ValueError("All-zero feature matrix")
          
          text_features = [json.loads(row[2]) for row in batch_rows if row[2]]
          feature_matrix = np.array([
    [f.get('word_count', 0), 
     f.get('avg_sentence_length', 0),
     f.get('adjective_count', 0)]
    for f in text_features
          ])
    
    # Combine with text vectors
          
    # Check class balance
          unique_labels, counts = np.unique(labels, return_counts=True)
          if np.min(counts) < 2:
            self.balance_classes()
            raise ValueError("Insufficient class examples")
         
          if X_train.shape[0] < 10 or len(np.unique(y_train)) < 2:
            logger.error("Insufficient training data or class diversity")
            with self._get_cursor() as cursor:
              cursor.execute('''
            DELETE FROM training_data 
            WHERE id IN (
                SELECT id 
                FROM training_data 
                WHERE processed=0 
                ORDER BY RANDOM() 
                LIMIT %d
            )
        ''' % (self.batch_size,))
             
            return False
         
          try:
        # Split batch data into training and validation sets (10% for validation)
        
            train_texts = [row[1] for row in train_rows]
            train_labels = [row[2] for row in train_rows]
        
            with self.db_lock:
            
              with self._get_cursor() as cursor:
                cursor.executemany('INSERT INTO validation_data (text, label) VALUES (?,?)',
                  list(zip(val_texts, val_labels)))
                cursor.connection.commit()
# Add to validation set
        
        
     
              

        # Split into texts and labels FIRST
            X_train_only = self.vectorizer.transform(train_texts)
            self._optimize_hyperparameters(X_train_only, y_train)#wrong
            train_texts = [row[1] for row in train_rows]
            val_texts = [row[1] for row in val_rows]
    
    # Transform all data first
            X_full = self.vectorizer.transform(train_texts + val_texts)
            X_val = X_full[len(train_texts):]
            X_train = X_full[:len(train_texts)]
            X_val = X_full[len(train_texts):]
            y_train = train_labels
            y_val = val_labels                           # Add this
     

            with self.db_lock:
            
              with self._get_cursor() as cursor:
                cursor.execute("SELECT DISTINCT label FROM training_data")
                self._unique_classes = set(row[0] for row in cursor.fetchall())
                cursor.close()
  
        
        # Optimize hyperparameters for models if we have enough DATA_DIR
        # With this (use only current training split):
            if self.training_stats["training_batches"] % 5 == 0:  # Every 5 batches
              if X_train.shape[0] > 500:  # Only when sufficient data
                self._optimize_hyperparameters(X_train, y_train)
         
         
            with self.db_lock:
              with self._get_cursor() as cursor:
                cursor.execute("SELECT DISTINCT label FROM training_data")
                all_classes = [row[0] for row in cursor.fetchall()]
                unique_classes = np.unique(all_classes)
             #Add this after finishing operations:
                cursor.close()
             
            
            
            unique_labels = np.unique(y_train)
            if len(unique_labels) < 2:
              logger.error("Insufficient class variety for training")
              return False  
             
            with self.db_lock:
              with self._get_cursor() as cursor:
                cursor.execute("SELECT DISTINCT label FROM training_data")
                all_classes = sorted([row[0] for row in cursor.fetchall()])
        
            if not all_classes:
              logger.warning("No classes found for SGD training")
              return
        # Train models with optimal parameters
        
            model_metrics = {}
            for model_name, model in self.models.items():
              start_time = time.time()
              if model_name == "naive_bayes":
                with self.lock:
                  classes = np.array(sorted(self._unique_classes))
                if self.fitted:  # Incremental learning
                  model.partial_fit(X_train, y_train, classes=unique_classes)
                else:
                  model.partial_fit(X_train, y_train, classes=unique_classes)
                  self.fitted = True
            # Lock vectorizer after initial fit
            
              elif model_name == "random_forest":
            # Random Forest requires full retraining
                start_time = time.time()
    
            # Get all processed data for retraining
                with self.db_lock:
                  with self._get_cursor() as cursor: 
                    cursor.execute("SELECT text, label FROM training_data WHERE processed=1")
                    full_train = cursor.fetchall()
                    full_texts = [row[0] for row in full_train]
                    full_labels = [row[1] for row in full_train]
                    cursor.close()
                batch_size = 1000
                for i in range(0, len(full_texts), batch_size):
                  chunk_texts = full_texts[i:i+batch_size]
                  X_chunk = self.vectorizer.transform(chunk_texts)
                  model.partial_fit(X_chunk, full_labels[i:i+batch_size])
    
                # Retrain periodically or when significant new data arrives
                if not hasattr(self, 'last_rf_retrain'):
                  self.last_rf_retrain = 0
                if (self.training_stats["training_batches"] - self.last_rf_retrain) >= 5:
                  self.last_rf_retrain = self.training_stats["training_batches"]
                  logger.info("Retraining Random Forest with full dataset")
                  model.fit(X_full, full_labels)
                  self.fitted = True
              elif name == "sgd":
    # Get current model state
                current_classes = model.classes_.tolist() if hasattr(model, 'classes_') else []
    
    # Identify new classes
                new_classes = [cls for cls in all_classes if cls not in current_classes]
    
                if new_classes:
        # Combine old and new classes
                  updated_classes = sorted(current_classes + new_classes)
        
        # Create new model with expanded classes
                  new_model = SGDClassifier(**model.get_params())
                  if current_classes:
            # Warm start with existing weights
                    new_model.coef_ = np.vstack([model.coef_, np.zeros((len(new_classes), model.coef_.shape[1]))])
                    new_model.intercept_ = np.concatenate([model.intercept_, np.zeros(len(new_classes))])
            
                  new_model.partial_fit(X_train, y_train, classes=updated_classes)
                  self.models['sgd'] = new_model
                else:
                  model.partial_fit(X_train, y_train, classes=all_classes)
                  
              elif model_name == "gradient_boosting":
    # Get all processed data for proper GB training
                with self.db_lock:
                  with self._get_cursor() as cursor:
                    cursor.execute("SELECT text, label FROM training_data WHERE processed=1")
                    full_data = cursor.fetchall()
                    full_texts = [row[0] for row in full_data]
                    full_labels = [row[1] for row in full_data]
    
                X_full = self.vectorizer.transform(full_texts)
    
    # Incremental training with warm start
                if self.fitted:
        # Increase tree count by 50 each batch
                  current_estimators = model.n_estimators
                  model.n_estimators = current_estimators + 50
                  model.fit(X_full, full_labels)
              
                else:
                  model.fit(X_full, full_labels)
                  self.fitted = True
              
        
    ########
    
            training_time = time.time() - start_time
    ######## 
          
          
# Early stopping mechanism
            best_val_loss = float('inf')
            patience = 3
            no_improvement = 0

            for epoch in range(100):
              model.partial_fit(X_train, y_train, classes=np.unique(y_train))
              try:
                val_loss = log_loss(y_val, model.predict_proba(X_val))
              except Exception as e:
                logger.error(f"Error calculating validation loss: {str(e)}")
                val_loss = float('inf')
    
              if val_loss < best_val_loss:
                best_val_loss = val_loss
                no_improvement = 0
        # Save best model
                self._save_checkpoint()
              else:
                no_improvement +=1
                if no_improvement >= patience:
                  break
          
            try:
               X_train = self.vectorizer.transform(train_texts)
            except ValueError as e:
              logger.error(f"Vectorization failed: {str(e)}")
              logger.info("Sample text: %s", train_texts[0][:100] if train_texts else "No texts")
              return False
            if self.genre_model is None and len(train_texts) > 100:
              self._train_fallback_genre_classifier() 
            if len(val_texts) > 0:
              X_val = self.vectorizer.transform(val_texts)
              val_preds = model.predict(X_val)
              val_acc = accuracy_score(val_labels, val_preds)
              logger.info(f"Validation accuracy: {val_acc:.2f}")
        
            with self.db_lock:
            
              with self._get_cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=1")
                count = cursor.fetchone()[0]
                cursor.close()
        
            model_metrics[model_name] = {
                "training_time": training_time,
                "samples": count,
                "parameters": str(model.get_params())
            }
          
            self._evaluate_models()
        
    
        # Update training stats
            self.training_stats["training_batches"] += 1
            self.training_stats["total_samples_processed"] += len(train_texts)
        
     
            with self.db_lock:
            
              with self._get_cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM validation_data")
                val_count = cursor.fetchone()[0]
                cursor.close()
            if val_count > 0:
              self._evaluate_models()
        
        # Save everything
            cursor.executemany('INSERT INTO validation_data (text, label) VALUES (?,?)',
              zip(val_texts, val_labels))
            self.save_memory()
        
            self.save_training_stats()
          
            with self.db_lock:
              with self._get_cursor() as cursor:
                cursor.execute('''
            SELECT label, COUNT(*) as count 
            FROM training_data 
            GROUP BY label
                ''')
                class_dist = cursor.fetchall()
                minority_class = min(class_dist, key=lambda x: x[1])
                if minority_class[1] < 100:
                  logger.warning(f"Class imbalance detected! {minority_class[0]} has only {minority_class[1]} samples")
        
            del X_train, X_val, train_texts, val_texts
            gc.collect()
            if 'cursor' in locals():
              cursor.close()
            return True
        
          except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error during batch training: {str(e)}")
            return False
        cursor.connection.commit()
    @property
    def unique_classes(self):
      if not hasattr(self, '_unique_classes'):
        with self.db_lock:
            with self._get_cursor() as cursor:
              cursor.execute("SELECT DISTINCT label FROM training_data")
              self._unique_classes = {row[0] for row in cursor.fetchall()}
              cursor.close()
      return self._unique_classes   
    def _evaluate_models(self):
      """Evaluate all models on validation data"""
      try:
        with self.db_lock:
          with self._get_cursor() as cursor:
            cursor.execute("SELECT text, label FROM validation_data")
            val_data = cursor.fetchall()
            val_texts = [row[0] for row in val_data]
            val_labels = [row[1] for row in val_data]
        
        X_val = self.vectorizer.transform(val_texts)
        
        for name, model in self.models.items():
            try:
                pred = model.predict(X_val)
                accuracy = accuracy_score(val_labels, pred)
                
                # Store detailed metrics
                metrics = {
                    "accuracy": accuracy,
                    "timestamp": datetime.now().isoformat(),
                    "model_params": model.get_params()
                }
                
                if hasattr(model, "predict_proba"):
                    proba = model.predict_proba(X_val)
                    metrics["log_loss"] = log_loss(val_labels, proba)
                    
                self.training_stats["model_evaluations"].setdefault(name, []).append(metrics)
                
            except Exception as e:
                logger.error(f"Error evaluating {name}: {str(e)}") 
                
      except Exception as e:
          #do something
          pass
    def select_best_model(self):
      """Select best model based on weighted evaluation metrics"""
      best_model = None
      best_score = -np.inf
    
      for name in self.models:
        metrics = self.training_stats["model_evaluations"].get(name, [])
        if not metrics:
            continue
            
        # Calculate weighted average of last 3 accuracies
        weights = [0.5, 0.3, 0.2]
        recent = [m["accuracy"] for m in metrics[-3:]]
        weighted_acc = sum(w*a for w,a in zip(weights[:len(recent)], recent))
        
        # Penalize slower models
        speed_penalty = np.mean([m.get("training_time",0) for m in metrics[-3:]]) / 60
        
        final_score = weighted_acc - (speed_penalty * 0.01)
        
        if final_score > best_score:
            best_score = final_score
            best_model = name
    
      if best_model:
        self.active_model = best_model
        logger.info(f"Selected new best model: {best_model} (score: {best_score:.4f})")
        return True
    
      return False
    
    def is_vectorizer_ready(self):
      return hasattr(self.vectorizer, 'vocabulary_') or \
           (hasattr(self.vectorizer, 'fit') and self.fitted)
           
    def _process_text_batch(self, texts, labels):
      """Process batch of text data"""
      with self.db_lock:
        with self._get_cursor() as cursor:
            cursor.executemany('''
                INSERT INTO training_data (text, label, processed)
                VALUES (?, ?, 0)
            ''', zip(texts, labels))
            cursor.connection.commit()
    def predict(self, input_text):
        """Predict based on input using the active model."""
        with self.model_load_lock:
          if not self.fitted: 
            return "AI is still learning. Feed it more data."
            
          try:
            with self.lock:
              X = self.vectorizer.transform([input_text])
              model = self.models[self.active_model]
              pred = model.predict(X)[0]
            
            # Get prediction probabilities if available
              if hasattr(model, "predict_proba"):
                    proba = model.predict_proba(X)[0]
                    max_proba = max(proba)
                    confidence = f"Confidence: {max_proba:.2f}"
                    self.last_confidence = max_proba  # Store for status printer
              else:
                    confidence = "Confidence score not available"
                    self.last_confidence = 0.0
                    
              return f"Prediction: {pred} ({confidence})"
            
          except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error during prediction: {str(e)}")
            self.last_confidence = 0.0
            return f"Prediction error: {str(e)}"
    def get_config_schema(self):
      return GodlikeAI.USER_CONFIG_SCHEMA

    def modify_profile(self, key, value):
      """Unified configuration handler with schema validation"""
      schema = GodlikeAI.USER_CONFIG_SCHEMA.get(key)
      if not schema:
        logger.error(f"Invalid config key: {key}")
        return False

    # Type validation and conversion
      try:
        if schema['type'] == int:
            value = int(value)
            if 'min' in schema and value < schema['min']:
                raise ValueError(f"Minimum {schema['min']}")
            if 'max' in schema and value > schema['max']:
                raise ValueError(f"Maximum {schema['max']}")
        elif schema['type'] == float:
            value = float(value)
        elif schema['type'] == str and 'options' in schema:
          if value not in schema['options']:
            raise ValueError(f"Valid options: {', '.join(schema['options'])}") 

        # Apply changes to runtime parameters
        if key == 'batch_size':
            self.batch_size = value
        elif key == 'active_model':
            if value in self.models:
                self.active_model = value
        elif key == 'memory_limit_mb':
            self.max_memory_items = int((value * 1024 * 1024) / 1000)  # Approx per-item memory
        elif key == 'learning_rate':
            if hasattr(self.models['sgd'], 'eta0'):
                self.models['sgd'].eta0 = value

        # Update user profile
        self.user_profile['preferences'][key] = value
        self.save_state()
        logger.info(f"Updated config: {key} = {value}")
        return True
      except (ValueError, TypeError) as e:
        logger.error(f"Invalid value {value} for {key}: {str(e)}")
        return False

    def _fetch_book_catalog(self):
        """Fetch the catalog of top books from Project Gutenberg."""
        try:
            logger.info("Fetching Gutenberg catalog...")
            response = self.throttled_request(GUTENBERG_CATALOG_URL)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            book_links = []
            
            # Find book links in the top 100 section
            for h2 in soup.find_all('h2'):
                if "Top 100" in h2.text:
                    section = h2.find_next('ol')
                    if section:
                        for link in section.find_all('a'):
                            if 'ebooks' in link.get('href', ''):
                                book_url = urljoin(GUTENBERG_BASE_URL, link['href'])
                                book_links.append({
                                    "title": link.text.strip(),
                                    "url": book_url,
                                    "id": book_url.split('/')[-1],
                                    "processed": False
                                })
            
            # If we found top 100, expand to more books by browsing alphabetically
            if book_links and len(book_links) < MAX_BOOKS_TO_COLLECT:
                letter_links = []
                for link in soup.find_all('a'):
                    href = link.get('href', '')
                    if '/browse/authors/' in href and len(href.split('/')[-1]) == 1:
                        letter_links.append(urljoin(GUTENBERG_BASE_URL, href))
                
                # Get a random sample of letters to explore
                random.shuffle(letter_links)
                for letter_url in letter_links[:5]:  # Limit to 5 letters for efficiency
                    time.sleep(GUTENBERG_ROBOTS_DELAY)
                    try:
                        response = self.throttled_request(letter_url)
                        response.raise_for_status()
                        author_soup = BeautifulSoup(response.text, 'html.parser')
                        
                        for author_link in author_soup.find_all('a'):
                            if '/ebooks/author' in author_link.get('href', ''):
                                time.sleep(GUTENBERG_ROBOTS_DELAY)
                                author_page = urljoin(GUTENBERG_BASE_URL, author_link['href'])
                                
                                try:
                                    author_response = self.throttled_request(author_page)
                                    author_response.raise_for_status()
                                    book_soup = BeautifulSoup(author_response.text, 'html.parser')
                                    
                                    for book_link in book_soup.find_all('a'):
                                        if '/ebooks/' in book_link.get('href', '') and book_link.get('href', '').split('/')[-1].isdigit():
                                            book_url = urljoin(GUTENBERG_BASE_URL, book_link['href'])
                                            book_id = book_url.split('/')[-1]
                                            
                                            # Check if we already have this book
                                            if not any(b['id'] == book_id for b in book_links):
                                                book_links.append({
                                                    "title": book_link.text.strip(),
                                                    "url": book_url,
                                                    "id": book_id,
                                                    "author": author_link.text.strip(),
                                                    "processed": False
                                                })
                                            
                                            # Stop if we've collected enough books
                                            if len(book_links) >= MAX_BOOKS_TO_COLLECT:
                                                break
                                                
                                except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
                                    logger.error(f"Error processing author page {author_page}: {str(e)}")
                                    continue
                                    
                            if len(book_links) >= MAX_BOOKS_TO_COLLECT:
                                break
                                
                    except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
                        logger.error(f"Error processing letter page {letter_url}: {str(e)}")
                        continue
                        
                    if len(book_links) >= MAX_BOOKS_TO_COLLECT:
                        break
            
            logger.info(f"Found {len(book_links)} books in the catalog")
            return book_links
            
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error fetching book catalog: {str(e)}")
            return [] 
    def _train_single_model(self, name, X_train, y_train, X_val, y_val):
      """Train a single model with validation"""
      model = self.models[name]
      start_time = time.time()
    
    # Handle incremental vs batch models
      if name in self.incremental_models:
        model.partial_fit(X_train, y_train, classes=np.unique(y_train))
      else:
        if hasattr(model, 'warm_start') and model.warm_start:
            model.n_estimators += 50
            model.fit(X_train, y_train)
        else:
            model.fit(X_train, y_train)

    # Calculate validation metrics
      val_pred = model.predict(X_val)
      val_acc = accuracy_score(y_val, val_pred)
      
      return {
        "timestamp": datetime.now().isoformat(),
        "accuracy": val_acc,
        "training_time": time.time() - start_time
      } 
    
    def _save_checkpoint(self):
        """Save current model state"""
        self.save_memory()
        logger.info("Training checkpoint saved")
        
    def update_book_index(self):
        """Update the book index by fetching the latest catalog.""" 
        if WEB_CRAWLER_CONFIG['offline_mode']:
          logger.info("Offline mode - using cached book index")
          return False
        new_books = self._fetch_book_catalog()
        
        if not new_books:
            logger.warning("No books found to update index")
            return False
            
        if not self.book_index:
            self.book_index = new_books
        else:
            # Update existing index with new books
            existing_ids = {book['id'] for book in self.book_index}
            for book in new_books:
                if book['id'] not in existing_ids:
                    self.book_index.append(book)
        
        self.total_books = len(self.book_index)
        self.save_book_index()
        return True 
    def configure_model(self, model_name, config): #INTEGRATE THIS INOT THE UI
      """Configure specific model parameters"""
      if model_name not in self.models:
        raise ValueError(f"Invalid model name: {model_name}")
    
      model = self.models[model_name]
      current_params = model.get_params()
    
    # Handle model-specific configurations
      if model_name == "random_forest":
        if 'n_estimators' in config:
            new_estimators = config['n_estimators']
            model.set_params(n_estimators=new_estimators, warm_start=True)
            
      elif model_name == "gradient_boosting":
        if 'learning_rate' in config:
            model.set_params(learning_rate=config['learning_rate'])
      
      logger.info(f"Updated {model_name} configuration")
      self.save_model_version()
     
    def _get_book_text_url(self, book_url):
        """Extract the text file URL from a book page."""
        try:
            response = self.throttled_request(book_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for text file formats (plain text preferred)
            formats = {
                'text/plain': None,
                'text/plain; charset=utf-8': None,
                'text/plain; charset=us-ascii': None
            }
            
            for link in soup.find_all('a'):
                for format_type in formats.keys():
                    if link.get('type') == format_type:
                        formats[format_type] = urljoin(GUTENBERG_BASE_URL, link.get('href'))
                        break
            
            # Get the first available format, prioritizing UTF-8
            for format_type in ['text/plain; charset=utf-8', 'text/plain', 'text/plain; charset=us-ascii']:
                if formats[format_type]:
                    return formats[format_type]
            
            return None
            
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error getting text URL from {book_url}: {str(e)}")
            return None

    @retry(retry=retry_if_exception_type((requests.exceptions.HTTPError, requests.exceptions.ConnectionError)),
      stop=stop_after_attempt(5),
      wait=wait_exponential(multiplier=1, min=2, max=60))
    def fetch_text_from_url(self, url):
      try:
        with self.request_lock:
            elapsed = time.time() - self.last_request_time
            if elapsed < GUTENBERG_ROBOTS_DELAY:
                time.sleep(GUTENBERG_ROBOTS_DELAY - elapsed)
                
            response = self.throttled_request(url)
            response.encoding = 'utf-8'
            response.raise_for_status()
            self.last_request_time = time.time()
            return response.text
      except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
        logger.error(f"Request failed: {str(e)}")
        raise

      

    
      
        
    # Find start and end markers for the actual content
    def extract_paragraphs(self, text):
    # Improved start/end markers
      if not text:
        return [] 
      start_markers = [
          r"\*\*\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK",
          r"\*+ ?START OF (THIS|THE) PROJECT GUTENBERG",
          r"\[Illustration:"
      ]
    
      end_markers = [
          r"\*\*\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK",
          r"\*+ ?END OF (THIS|THE) PROJECT GUTENBERG",
          r"End of (the )?Project Gutenberg",
          r"\*\*\*END OF THE PROJECT GUTENBERG"
      ]

    # Use regex search with flags
      start_pos = 0
      for pattern in start_markers:
        match = re.search(pattern, text, flags=re.IGNORECASE)
        if match:
            start_pos = match.end()
            break

      end_pos = len(text)
      for pattern in end_markers:
        match = re.search(pattern, text, flags=re.IGNORECASE)
        if match:
          end_pos = match.start()
          break
    
    # Extract content between markers
      content = text[start_pos:end_pos].strip()
    

    # First, separate by chapter indicators
      chapter_pattern = re.compile(r'\n\s*(?:CHAPTER|Chapter|[IVXLC]+)\s+[IVXLCDM\d]+.*?\n', re.MULTILINE)
      chapters = chapter_pattern.split(content)
    
      paragraphs = []
      for chapter in chapters:
        # Split into paragraphs, preserving dialogue
        chapter_paragraphs = re.split(r'\n\s*\n', chapter)
        
        for p in chapter_paragraphs:
            # Clean up paragraph
            p = p.strip()
            if not p:  # Skip empty paragraphs
              continue
            p = re.sub(r'\s+', ' ', p)  # Normalize whitespace
            
            # Filter out likely headers, page numbers, etc.
            if len(p) < 15:  # Too short
                continue
                
            if re.match(r'^[A-Z\s]+$', p):  # All caps - likely a header
                continue
                
            if re.match(r'^\d+$', p):  # Just a number - likely a page number
                continue
            
            # Add semantic paragraph filtering - discard meaningless paragraphs
            word_count = len(p.split())
            if word_count < 5:  # Too few words to be meaningful
                continue
                
            # Split very long paragraphs into smaller units for better learning
            if word_count > 200:
                sentences = re.split(r'(?<=[.!?])\s+', p)
                sentence_groups = [sentences[i:i+5] for i in range(0, len(sentences), 5)]
                for group in sentence_groups:
                    if group:
                        paragraphs.append(' '.join(group))
            else:
                paragraphs.append(p)
                
      return paragraphs
      
    
    def retry_book(self, book_index):
      max_retries = 3
      for attempt in range(max_retries):
        try:
            self.process_book(book_index)  
            return True
        except Exception as e:
            logger.warning(f"Attempt {attempt+1} failed: {str(e)}")
            time.sleep(2 ** attempt)
      return False
    
    @retry(stop=stop_after_attempt(3), wait=wait_exponential())
    def process_book(self, book_index):
      """Process a single book from the index."""
      try: 
        
        if self.vm.used() > MAX_MEMORY_MB * 1024 * 1024:
            gc.collect()
            self.context_window.clear()
        with self._get_cursor() as cursor:
          cursor.execute("SAVEPOINT book_processing")
          soup = None
          if book_index >= len(self.book_index):
            logger.warning(f"Book index {book_index} out of range")
            return False
            
          book = self.book_index[book_index]
        
        
        # Skip if already processed
          if book.get('processed', False):
            logger.info(f"Book {book['id']} already processed, skipping")
            return True
            
          with self.lock:
            self.web_crawl_stats['active_books'].append({
                'title': book['title'],
                'progress': 0.0
            })
            
          try:
            logger.info(f"Processing book: {book['title']} (ID: {book['id']})")
            
            # Check if we have a text URL, if not get it
            if 'text_url' not in book:
                book['text_url'] = self._get_book_text_url(book['url'])
                if not book['text_url']:
                    logger.warning(f"Could not find text URL for book {book['title']}")
                    book['processed'] = False
                    book['error'] = "No text URL found"
                    self.book_index[book_index] = book
                    self.save_book_index()
                    return False
                    
            # Fetch the text
            text = self.fetch_text_from_url(book['text_url'])
            if not text:
                logger.warning(f"Could not fetch text for book {book['title']}")
                book['processed'] = False
                book['error'] = "Text fetch failed"
                self.book_index[book_index] = book
                self.save_book_index()
                return False
                
            # Process in chunks with explicit cleanup
            chunk_size = 1000000  # 1MB chunks
            paragraphs = []
            with requests.get(book['text_url'], stream=True) as response:
              for chunk in response.iter_content(chunk_size, decode_unicode=True):
                paragraphs += self.extract_paragraphs(chunk)
                del chunk
                gc.collect()
            del text
            gc.collect()
            try:
              genre_label = self.determine_genre(book['title'], paragraphs)
            except Exception as e:
              logger.error(f"Genre classification failed: {str(e)}")
              genre_label = "unknown"
    # Emergency fallback
              if len(paragraphs) > 0:
                genre_label = "fiction" if any(' the ' in p.lower() for p in paragraphs[:3]) else "non-fiction"
                
            combined_label = f"{genre_label}::book:{book['id']}"
            for chunk in self.process_in_chunks(text, combined_label):
              self._train_batch()
 
              
 
            if not paragraphs:
              logger.warning(f"No paragraphs extracted from book {book['title']}")
            # Extract paragraphs
    
            if not paragraphs:
                logger.warning(f"No paragraphs extracted from book {book['title']}")
                book['processed'] = False
                book['error'] = "No paragraphs extracted"
                self.book_index[book_index] = book
                self.save_book_index()
                return False
            if not isinstance(paragraphs, list) or len(paragraphs) == 0:
              logger.error("Invalid paragraphs format")
              return False
                
            # Generate labels
            book_label = f"book:{book['id']}"
            
            genre_label = self.determine_genre(book['title'], paragraphs)
            

            book_processed = False
            paragraphs_processed = 0
            total_paragraphs = len(paragraphs)
            if total_paragraphs == 0:
              logger.error("No paragraphs available for training")
              return False
            # Check if we have partial processing from before
            partially_processed = self.learning_progress.setdefault(
            'partially_processed_books', {})  # Ensure key exists
            if book['id'] in partially_processed:
              status = partially_processed[book['id']]
              paragraphs_processed = status["paragraph_index"]
              retries = status.get("retries", 0)
            else:
              paragraphs_processed = 0
              retries = 0
            
            # Process paragraphs
            for i, para in enumerate(paragraphs[paragraphs_processed:], paragraphs_processed):
                # Check if we should stop training
                if self.stop_training:
                    # Save progress for resuming later

# Save progress for resuming later
                    if 'partially_processed_books' not in self.learning_progress:
                        self.learning_progress['partially_processed_books'] = {} 
                        
                    batch_size = 100
                    for i in range(0, len(paragraphs), batch_size):
                      batch = paragraphs[i:i+batch_size]
                      with self.db_lock:
                         with self._get_cursor() as cursor:
                           cursor.executemany('''
                        INSERT INTO training_data (text, label, processed)
                        VALUES (?, ?, 0)
                    ''', [(p, combined_label) for p in batch])
                           cursor.connection.commit()
            
            # Update progress
                    self.learning_progress['partially_processed_books'][book['id']] = i + len(batch)
                    self.save_learning_progress()
                    logger.info(f"Training stopped. Progress saved at paragraph {i}/{total_paragraphs}")
                    return False
                
                # Add paragraph to training data
                combined_label = f"{genre_label}::{book_label}"
                self.train(para, combined_label)
                
                # Update progress periodically
                if i % 50 == 0:
                    paragraphs_processed = i
                    if 'partially_processed_books' not in self.learning_progress:
                        self.learning_progress['partially_processed_books'] = {}
                    self.learning_progress['partially_processed_books'][book['id']] = i
                    self.save_learning_progress()
                    logger.info(f"Processed {i}/{total_paragraphs} paragraphs from {book['title']}")
            
            # Mark book as processed
            book['processed'] = True
            book['processing_date'] = datetime.now().isoformat()
            book['paragraphs_count'] = total_paragraphs
            
            # Update and save book index
            self.book_index[book_index] = book
            self.save_book_index()
            
            # Update learning progress
            if 'completed_books' not in self.learning_progress:
                self.learning_progress['completed_books'] = []
            self.learning_progress['completed_books'].append(book['id'])
            with self.lock:
              if 'completed_books' not in self.learning_progress:
                self.learning_progress['completed_books'] = []
              self.learning_progress['completed_books'].append(book['id'])
            # Remove from partially processed if it exists
            if 'partially_processed_books' in self.learning_progress and book['id'] in self.learning_progress['partially_processed_books']:
                del self.learning_progress['partially_processed_books'][book['id']]
            
            self.learning_progress['last_processed_index'] = book_index
            self.save_learning_progress()
            
            # Update processed book count
            self.processed_book_count = len(self.learning_progress.get('completed_books', []))
            if i % 50 == 0:
              # Calculate progress
              progress = i / total_paragraphs
              with self.lock:
                for b in self.web_crawl_stats['active_books']:
                    if b['title'] == book['title']:
                        b['progress'] = progress
                        break
                        
            logger.info(f"Book processed successfully: {book['title']}")
            del paragraphs
            gc.collect()
            return True
            
          except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error processing book {book['title']}: {str(e)}", 
                 exc_info=True, 
                 extra={'book_id': book['id'], 'url': book['url']})
            book['error'] = str(e)
            self.book_index[book_index] = book
            self.save_book_index()
            self._add_to_recovery_queue('book', {
    'index': book_index,
    'url': book['url'],
    'last_attempt': datetime.now().isoformat()
            })
            return False
          finally:
            with self.lock:
              self.web_crawl_stats['active_books'] = [
                b for b in self.web_crawl_stats['active_books']
                if b['title'] != book['title']
              ]
            if 'response' in locals():
              response.close()
            if 'soup' in locals():
             try:
                soup.decompose()
             except AttributeError:
                pass

    # Explicitly release large objects
            for var in ['text', 'paragraphs', 'soup']:
              if var in locals():
                del locals()[var]
            
            
             
        # Explicitly clear large objects
            if text:
                del text
            if paragraphs:
                del paragraphs 
                
            if soup:
                del soup
            gc.collect()  
            gc.freeze()
            cursor.execute("RELEASE SAVEPOINT book_processing")
      except MemoryError:
        gc.collect()
        self.context_window.clear()
        logger.warning("Memory pressure detected - cleared context")
      except:
         for var in ['text', 'paragraphs']:
           if var in locals():
             del locals()[var]
         gc.collect()
         with self._get_cursor() as cursor:
           cursor.execute("ROLLBACK TO SAVEPOINT book_processing")
    #SUPPORT FOR MORE INTERNET 
    def start_web_crawler(self):
      """Initialize web crawler with proper domain queue handling."""
      try:
        if WEB_CRAWLER_CONFIG['offline_mode']:
            logger.info("Offline mode - web crawler disabled")
            return

        # Seed domain queues with initial URLs
        for url in WEB_CRAWLER_CONFIG['seed_urls']:
            parsed = urlparse(url)
            domain = parsed.netloc
            # Add URL to domain's priority queue with initial priority 0
            self.domain_queues[domain].put((0, url))

        # Start Common Crawl processing
        cc_thread = Thread(target=self.start_commoncrawl_processing, daemon=True)
        cc_thread.start()

        # Get unique domains that have queued URLs
        domains = [domain for domain, q in self.domain_queues.items() if not q.empty()]

        # Start domain crawlers with enhanced error handling
        with ThreadPoolExecutor(
            max_workers=WEB_CRAWLER_CONFIG['politeness']['max_parallel_domains'],
            thread_name_prefix="DomainCrawler"
        ) as executor:
            futures = {}
            for domain in domains:
                future = executor.submit(self._domain_crawler, domain)
                futures[future] = domain
                logger.debug(f"Started crawler for domain: {domain}")

            for future in concurrent.futures.as_completed(futures):
                domain = futures[future]
                try:
                    future.result()
                    logger.info(f"Domain crawler completed: {domain}")
                except Exception as e:
                    logger.error(f"Domain crawler failed for {domain}: {str(e)}")
                    self._add_to_recovery_queue('domain', {
                        'domain': domain,
                        'error': str(e),
                        'retry_after': time.time() + 300
                    })

        cc_thread.join()

      except Exception as e:
        logger.critical(f"Web crawler startup failed: {str(e)}", exc_info=True)
        self._add_to_recovery_queue('system', {'component': 'web_crawler'})
        # Enter degraded mode 
        logger.info("Temporarily Entering Offline Mode...")
        self.offline_mode = True
        self._init_offline_resources()

    def _domain_crawler(self, domain):
      """Process URLs from a specific domain's queue with politeness rules."""
      logger.info(f"Starting domain crawler for: {domain}")
      try:
        while True:
            try:
                # Get next URL with priority handling
                priority, url = self.domain_queues[domain].get(timeout=30)
                
                # Check domain timeout status
                if time.time() < self.domain_timeouts.get(domain, 0):
                    logger.debug(f"Domain {domain} in cooldown, skipping")
                    continue
                
                # Process the webpage
                if self.process_webpage(url):
                    self.domain_queues[domain].task_done()
                    self.rate_limiter.success()
                else:
                    self.rate_limiter.failure()
                
                # Respect crawl delay
                time.sleep(self.domain_limits[domain]['delay'])

            except queue.Empty:
                logger.debug(f"No more URLs in queue for {domain}")
                break
                
            except Exception as e:
                logger.error(f"Error processing URL in {domain} queue: {str(e)}")
                self.domain_retries[domain] = self.domain_retries.get(domain, 0) + 1
                
                if self.domain_retries[domain] > WEB_CRAWLER_CONFIG['politeness']['max_retries']:
                    logger.warning(f"Blacklisting domain {domain} after max retries")
                    self.blacklisted_domains.add(domain)
                    break

      finally:
        logger.info(f"Domain crawler exiting for: {domain}")
        # Cleanup resources
        if domain in self.domain_workers:
            del self.domain_workers[domain]
      
    def start_learning(self, continue_from_last=True):
        """Start the book processing and learning process."""
        try:
          if self.training_in_progress:
            logger.warning("Training is already in progress")
            return False
          with self.training_lock:
            if self.training_in_progress:
              logger.warning("Training is already in progress")
              return False
          self.stop_training = False
          self.training_in_progress = True
        
        # Update book index if needed
          if not self.book_index:
            logger.info("No book index found. Updating...")
            self.update_book_index()
            
        # Determine starting point
          start_index = 0
          if continue_from_last and 'last_processed_index' in self.learning_progress:
            start_index = self.learning_progress['last_processed_index'] + 1
            
        # Start training thread
          training_thread = Thread(
            target=self._learning_thread, 
            args=(start_index,)
          )
          training_thread.daemon = True
          training_thread.start()
          self.validate_training_data()
          self.balance_classes()
          logger.info(f"Learning started from book index {start_index}")
          logger.info(f"Web Page Learning Started: Processing {len(self.cc_index)} Common Crawl collections")
          return True 
        except Exception as e:
          logger.error(f"Learning process failed to start: {str(e)}")
          self._add_to_recovery_queue('system', {'component': 'learning'})

    def stop_learning(self):
      """Stop the learning process gracefully."""
      if not self.training_in_progress:
        logger.warning("No training in progress to stop")
        return False
            
      logger.info("Stopping learning process...")
      self.stop_training = True
      self.training_in_progress = False
      return True

    def _learning_thread(self, start_index):
      """Process books using parallel batches with resume support"""
      if random.random() < 0.2:  # 20% chance to process recoveries
        self._process_recovery_queue()
      with tqdm(total=self.total_books, desc="Processing Books") as pbar:
        queue = self.learning_progress.get("processing_queue", [])
        if not queue:
            queue = list(range(start_index, len(self.book_index)))
        for book_idx in queue:
                self.process_book(book_idx)
                pbar.update(1) 
                pbar.set_postfix({
                    'processed': self.processed_book_count,
                    'accuracy': self.training_stats.get('latest_accuracy', 0)
                })
        while queue and not self.stop_training:
          try:
            with self.lock:
              queue = self.learning_progress["processing_queue"]
              if not queue:
                queue = list(range(start_index, len(self.book_index)))
                
            batch_size = self.book_processing_batch
            if not queue:
              logger.info("Processing queue empty")
              return
            batch = queue[:batch_size]
            if not batch:
              return
    
            while queue and not self.stop_training:
              batch = queue[:batch_size]
              self.process_books_parallel(batch, max_workers=8)
              pbar.update(len(batch))
              
              with self.lock:
                # Update processing queue
                self.learning_progress["processing_queue"] = [
                    idx for idx in queue 
                    if idx not in batch
                ]
                self.save_learning_progress()
                queue = self.learning_progress["processing_queue"]
                if random.random() < 0.1:  # 10% chance to evaluate
                  test_acc = self.evaluate_test_set()
                logger.info(f"Final Test Accuracy: {test_acc:.4f}")
                self.training_stats["final_test_accuracy"] = test_acc
                self.save_training_stats()
              time.sleep(GUTENBERG_ROBOTS_DELAY * batch_size)
          
          except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error processing batch: {str(e)}", exc_info=True)
            self.save_learning_progress()
            self.save_memory()
          finally:
            self.training_in_progress = False 
            gc.collect()
     
    def start_commoncrawl_processing(self):
      """Start processing Common Crawl data in parallel."""
      if WEB_CRAWLER_CONFIG['offline_mode']:
        logger.info("Offline mode - CommonCrawl processing disabled")
        return
      if not self.network_status:
          logger.warning("Network offline - CommonCrawl processing skipped")
          return

      # 1. Fetch the list of available crawl indexes
      self.cc_index_info = self.fetch_commoncrawl_index() # Renamed variable for clarity
      if not self.cc_index_info:
          logger.error("Could not retrieve Common Crawl index information. Aborting CC processing.")
          return

      # 2. Select the crawl to process (e.g., the latest one)
      # The list is usually sorted newest first
      try:
          latest_crawl_info = self.cc_index_info[0]
          crawl_id = latest_crawl_info['id']
          logger.info(f"Selected Common Crawl index to process: {crawl_id} ({latest_crawl_info.get('name', 'N/A')})")
      except (IndexError, KeyError) as e:
            logger.error(f"Could not determine latest crawl ID from index info: {e}. Index Info: {self.cc_index_info}")
            return

      # 3. Fetch the list of WARC file paths for the selected crawl
      warc_file_paths = self._fetch_warc_paths(crawl_id)
      if not warc_file_paths:
          logger.error(f"Could not retrieve WARC file paths for crawl {crawl_id}. Aborting CC processing.")
          return

      # 4. Populate WARC queue (clearing any old paths first)
      logger.info(f"Populating WARC queue with {len(warc_file_paths)} paths...")
      # Clear existing queue if necessary (optional, depends on desired behavior)
      while not self.cc_warc_queue.empty():
          try:
              self.cc_warc_queue.get_nowait()
          except queue.Empty:
              break
      # Add new paths
      for path in warc_file_paths:
          self.cc_warc_queue.put(path)
      logger.info(f"WARC queue populated.")


      # 5. Start processing threads
      num_workers = WEB_CRAWLER_CONFIG['commoncrawl']['parallel_requests']
      logger.info(f"Starting {num_workers} Common Crawl worker threads...")
      active_futures = set()

      with ThreadPoolExecutor(max_workers=num_workers, thread_name_prefix="CC_Worker") as executor:
        while not self.cc_warc_queue.empty() or active_futures:
            # Submit new tasks if queue has items and workers are available
            while not self.cc_warc_queue.empty() and len(active_futures) < num_workers:
                try:
                    warc_path = self.cc_warc_queue.get_nowait()
                    logger.debug(f"Submitting WARC task: {warc_path}")
                    future = executor.submit(self.process_cc_warc, warc_path)
                    active_futures.add(future)
                except queue.Empty:
                    break # Queue is empty for now
                except Exception as e:
                    logger.error(f"Error submitting WARC task: {e}")

            # Process completed futures
            if not active_futures:
                time.sleep(1) # Avoid busy-waiting if queue is temporarily empty
                continue

            done, active_futures = concurrent.futures.wait(active_futures, timeout=1.0, return_when=concurrent.futures.FIRST_COMPLETED)

            for future in done:
                try:
                    future.result() # Check for exceptions
                    logger.debug("WARC task completed successfully.")
                except Exception as e:
                    # Log the error, but allow processing to continue with other files
                    logger.error(f"Common Crawl WARC processing task failed: {str(e)}", exc_info=True) # Add traceback

            # Optional: Add a small delay to prevent overwhelming the system
            time.sleep(0.1)

      logger.info("Common Crawl processing threads finished.")

    def process_books_parallel(self, book_indices, max_workers=4):
      """Process multiple books in parallel using thread pool."""
      stop_event = Event() 
      with self.training_lock:
        if not book_indices:
          return True

    # Create processing queue and event
        paragraph_queue = queue.Queue(maxsize=10000)  # Prevent unlimited memory usage
        stop_event = Event()
      
      
      
      chunk_size = 10  # Process books in chunks
      for i in range(0, len(book_indices), chunk_size):
        batch = book_indices[i:i+chunk_size]
        max_workers = min(max_workers, os.cpu_count() or 4)
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
          futures = []
          for idx in book_indices:
            futures.append(executor.submit(self._process_book_safe, idx))
        
          for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                logger.error("Book processing failed: %s", str(e))
                book_idx = future.book_idx  # Store idx when submitting future
                self.learning_progress['failed_books'].append(book_idx)
        gc.collect()  # Explicit GC after each chunk
        self.save_memory()  # Checkpoint progress
      
      
      def worker_function(book_idx):
        """Worker to process individual books and populate paragraph queue."""
        try:
            book_index = self.learning_progress["processing_queue"][book_idx]
            book = self.book_index[book_index]
            status = self.learning_progress["partially_processed_books"].get(
                book['id'], {"paragraph_index": 0, "retries": 0}
            )
            
            text_url = book.get('text_url') or self._get_book_text_url(book['url'])
            if not text_url:
                return False
                
            text = self.fetch_text_from_url(text_url)
            def paragraph_generator(text):
              buffer = []
              for para in self.extract_paragraphs(text):
                buffer.append(para)
              if len(buffer) >= 100:
                yield buffer
                buffer = []
              if buffer:
                yield buffer

            for chunk in paragraph_generator(text):
    # Process chunk
              del chunk
              gc.collect()
            
            # Add paragraphs to processing 
            paragraphs = self.extract_paragraphs(text)  # Add this line
            for para in paragraphs[status["paragraph_index"]:]: 
                if stop_event.is_set():
                    break
                genre = self.determine_genre(book['title'], paragraphs) 
                label = f"{genre}::book:{book['id']}"  # Add genre context

                paragraph_queue.put((para, label))
            
            # Mark book as processed if completed
            if not stop_event.is_set():
                with self.lock:
                    if book['id'] in self.learning_progress["partially_processed_books"]:
                        del self.learning_progress["partially_processed_books"][book['id']]
                    self.learning_progress["completed_books"].append(book['id'])
            
            return True
            
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error processing book {book_idx}: {str(e)}")
            return False
        finally:
          del text, paragraphs  
          gc.collect()

      
      def consumer_function():
        """Process paragraphs in batches for training."""
        batch = [] 
        nonlocal stop_event  # Added line
        while not stop_event.is_set() or not paragraph_queue.empty(): 
          try: 
            text, label = paragraph_queue.get(timeout=1)
            batch.append((text, label))
            if len(batch) >= self.batch_size:
                # Insert batch into database
                with self.db_lock:
                    
                        with self._get_cursor() as cursor:
                          cursor.executemany(
                            "INSERT INTO training_data (text, label, processed) VALUES (?, ?, 0)",
                            batch
                          )
                          cursor.connection.commit()
                          cursor.close()
                # Reset batch and check if training should trigger
                batch = []
                with self.db_lock:
                    
                        with self._get_cursor() as cursor:
                          cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed = 0")
                          count = cursor.fetchone()[0]
                          cursor.close()
                if count >= self.batch_size:
                    self._train_batch()
            paragraph_queue.task_done()
          except queue.Empty:
            stop_event = Event() 
            consumer_thread = Thread(target=consumer_function, daemon=True)
            consumer_thread.start()
            # Execute workers
            max_workers = min(max_workers, os.cpu_count() or 4)
            with concurrent.futures.ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="BookWorker"
            ) as executor:
              futures = [executor.submit(worker_function, idx) for idx in book_indices]
              concurrent.futures.wait(futures)

            # Signal consumer to stop
            stop_event.set()
            paragraph_queue.join()
            consumer_thread.join(timeout=30)
    # Process remaining batch
        if batch:
          with self.db_lock:
            
                with self._get_cursor() as cursor:
                  cursor.executemany(
                    "INSERT INTO training_data (text, label, processed) VALUES (?, ?, 0)",
                    batch
                  )
                  cursor.connection.commit()
                  cursor.close()
          with self.db_lock:
            
                with self._get_cursor() as cursor:
                  cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed = 0")
                  count = cursor.fetchone()[0]
                  cursor.close()
          if count >= self.batch_size:
            self._train_batch()
    def _process_book_safe(self, book_idx):
      """Wrapper with local database connection"""
      local_conn = sqlite3.connect(
        os.path.join(DATA_DIR, "training_data.db"),
        check_same_thread=False,
        timeout=30
      )
      try:
        with local_conn:
            cursor = local_conn.cursor()
            # Perform processing using local connection
            self.process_book(book_idx)
      finally:
        local_conn.close()
      with concurrent.futures.ThreadPoolExecutor(max_workers=WEB_CRAWLER_CONFIG['commoncrawl']['parallel_requests']) as executor:
        futures = []
        futures.append(executor.submit(self._process_book_wrapper, book_idx))
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                logger.error(f"Book processing failed: {str(e)}")
                
    def _process_book_wrapper(self, book_idx):
      """Wrapper with proper resource cleanup"""
      try:
        local_conn = sqlite3.connect(os.path.join(DATA_DIR, "training_data.db"))
        local_ai = GodlikeAI()
        
        local_ai.conn = local_conn
        local_ai.process_book(book_idx)
      finally:
        local_conn.close()
        del local_ai
        gc.collect()
         
    def get_learning_status(self):
        """Get the current learning status."""
        with self.training_lock:
          training_status = self.training_in_progress
        with self.db_lock:
          
            with self._get_cursor() as cursor:
              cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=1")
              train_size = cursor.fetchone()[0]
              cursor.close()
        
        status = {
            "training_in_progress": self.training_in_progress,
            "processed_books": self.processed_book_count,
            "total_books": self.total_books,
            "progress_percentage": 0 if not self.total_books else (self.processed_book_count / self.total_books) * 100,
            "training_data_size": train_size,
            "active_model": self.active_model,
            "fitted": self.fitted,
        }
        
        # Add accuracy if available
        if self.training_stats.get("accuracy_history"):
            status["latest_accuracy"] = self.training_stats["accuracy_history"][-1]["accuracy"] 
        else:
          status["latest_accuracy"] = 0.0
        return status
        

    def semantic_search(self, text):
    # Build term-document matrix
      doc_vectors = self.vectorizer.transform([item['text'] for item in self.context_window])
      query_vec = self.vectorizer.transform([text])

    # Calculate similarity with decay weights
      similarities = cosine_similarity(query_vec, doc_vectors)[0]
      weighted_similarities = [
        sim * (0.9 ** idx) * self.context_window[idx]['weight']
        for idx, sim in enumerate(similarities)
      ]

    # Retrieve most relevant context
      if weighted_similarities:
        max_idx = np.argmax(weighted_similarities)
        return self.context_window[max_idx]['text']
      return ""
       
    def analyze_text(self, text, analysis_type="response"):
      try:
        self.update_context(text)
        if not self.fitted:
          return "AI is still learning. Not enough data for analysis."
        if not text.strip():
          return "Please provide meaningful input for analysis"
        if not isinstance(text, str) or len(text.strip()) < 3:
          return "Invalid input: text must be at least 3 characters"
        
        text = text[:5000]  # Limit input size
        
    
        try:
          with self.db_lock:
          
              with self._get_cursor() as cursor:
                cursor.execute("SELECT id, text, label FROM training_data WHERE processed=1")
                all_data = cursor.fetchall()
                cursor.close()
        # Advanced response generation
          if analysis_type == "response":
            with self.db_lock:
              
                with self._get_cursor() as cursor:
                  cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=1")
                  count = cursor.fetchone()[0]
                  cursor.close()
            if count < 10:
              return "I'm still gathering knowledge, please interact with me to help me learn"
            
            # Vectorize input text
            X_input = self.vectorizer.transform([text])
            model = self.models[self.active_model]
            
            #
            context_similarity = self._calculate_semantic_relevance(text)
            attention_weight = 0.7 if context_similarity > 0.6 else 0.3
            # Get prediction with confidence
            pred = model.predict(X_input)[0]
            if self.context_window:
              context_preds = [self.models[self.active_model].predict(self.vectorizer.transform([ctx['text']])) for ctx in self.context_window]
            pred = max(set(context_preds + [pred]), key=(context_preds + [pred]).count)
            confidence = 0.0
            if hasattr(model, "predict_proba"):
                probabilities = model.predict_proba(X_input)[0]
                max_proba_index = probabilities.argmax()
                confidence = probabilities[max_proba_index]
            
            # Extract key concepts from input
            
            
            
            # Filter out common words and keep meaningful terms
            try:
              if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
                self._train_fallback_genre_classifier()
              temp_vectorizer = TfidfVectorizer(
                max_features=200,
                ngram_range=(1, 2),
                stop_words='english',
                vocabulary=self.fallback_vectorizer.vocabulary_
              )
              X = temp_vectorizer.transform([text])
              features = X.indices
              X = temp_vectorizer.transform([text])
              key_terms = [] 
              if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
                self._train_fallback_genre_classifier()
                if not hasattr(self.fallback_vectorizer, 'vocabulary_'):
                  key_terms = []
                  return "Analysis unavailable - insufficient training data" 
                  
            except ValueError as e:
              logger.warning(f"Vectorizer error: {str(e)}")
              key_terms = []
            
            # Find semantically similar content with clustering
            # Create term vectors for all training data
            if key_terms:
                # Find documents containing the key terms
                relevant_docs = []
                relevant_indices = []
                relevant_ids = []
                with self.db_lock:
                    
                        with self._get_cursor() as cursor:
                          cursor.execute("SELECT id, text, label FROM training_data WHERE processed=1")
                          cursor.execute("SELECT text, label FROM training_data WHERE processed=1")
                          all_data = cursor.fetchall()
                with self.db_lock:
                 
                   with self._get_cursor() as cursor:
                     cursor.execute("SELECT label FROM training_data WHERE processed=1")
                     all_labels = [row[0] for row in cursor.fetchall()]
                     cursor.close()
                for i in range(len(all_labels)):
                  current_label = all_labels[i]
    
                  window = 5
                  start = max(0, i - window)
                  end = min(len(all_labels), i + window)
    
                  for j in range(start, end):
                    if i == j:
                      continue
                    neighbor_label = all_labels[j]
                for i, doc in enumerate(all_data):
                    # Check if document contains any key terms
                    if any(term in doc.lower() for term in key_terms):
                        relevant_docs.append(doc)
                        relevant_indices.append(i)
                        cursor.execute("SELECT id, text, label FROM training_data WHERE processed=1")  # Add id to selection
                        relevant_ids = [row[0] for row in cursor.fetchall()]  # Get IDs properly
                # Use TF-IDF similarity for better matching
                if relevant_docs:
                    # Create mini-corpus with input text and relevant docs
                    mini_corpus = [text] + relevant_docs
                    mini_X = self.vectorizer.transform(mini_corpus)
                    
                    # Calculate cosine similarity between input and each doc
                    input_vector = mini_X[0]
                    similarities = []
                    
                    for i in range(1, len(mini_corpus)):
                        doc_vector = mini_X[i]
                        input_vector = self.fallback_vectorizer.transform([text])
                        doc_vectors = self.fallback_vectorizer.transform(relevant_docs)
                        similarities = cosine_similarity(input_vector, doc_vectors)[0]
                        similarity = cosine_similarity(
                            input_vector, 
                            doc_vector
                          )[0][0]
                        similarities.append((i-1, similarity))
                    
                    # Sort by similarity
                    similarities.sort(key=lambda x: x[1], reverse=True)
                    
                    # Get top N most similar documents
                    top_n = min(5, len(similarities))
                    most_relevant_indices = [similarities[i][0] for i in range(top_n)]
                    most_relevant_docs = [relevant_docs[i] for i in most_relevant_indices]
                    cursor.execute("SELECT id, label FROM training_data WHERE processed=1")
                    all_ids = [row[0] for row in cursor.fetchall()]
                    most_relevant_labels = []  # Initialize the list here

                    for i in range(top_n):
                      doc_idx = most_relevant_indices[i]
                      original_idx = relevant_indices[doc_idx]  # Use doc_idx to get the original index
                      actual_id = relevant_ids[original_idx] 
                      with self.db_lock:
                        
                          with self._get_cursor() as cursor:
                            cursor.execute("SELECT label FROM training_data WHERE id=?", (actual_id,))
                          label = cursor.fetchone()[0]
                      most_relevant_labels.append(label)

                    
                    # Get the most frequent label among relevant docs
                    label_counter = {}
                    for label in most_relevant_labels:
                        label_counter[label] = label_counter.get(label, 0) + 1
                    
                    common_label = max(label_counter.items(), key=lambda x: x[1])[0]
                    
                    # Generate more coherent response using the most relevant content
                    if most_relevant_docs:
                        # Extract best sentences from relevant docs
                        all_sentences = []
                        for doc in most_relevant_docs:
                            sentences = re.split(r'[.!?]', doc)
                            for s in sentences:
                                s = s.strip()
                                if s and len(s) > 20:
                                    # Check if sentence contains any key term
                                    if any(term in s.lower() for term in key_terms):
                                        all_sentences.append(s)
                        
                        # Remove duplicates
                        all_sentences = list(set(all_sentences))
                        
                        # Generate coherent response with good transitions
                        if all_sentences:
                            # Choose 2-3 sentences for the response
                            selected_sentences = random.sample(all_sentences, min(3, len(all_sentences)))
                            
                            response = []
                            
                            # Create an introduction
                            predicted_genre = pred.split('::')[0] if '::' in pred else pred

                            if "?" in text:
                              if key_terms:
                                intro = f"Regarding your question about {key_terms[0]}, here's what I found..."
                              else:
                                intro = f"In response to your question related to {predicted_genre}, here's what I know.."
                              response.append(intro)

                            else:
                              if key_terms:
                                intro = f"Analyzing the concept of {key_terms[0]}, "
                              else:
                                intro = f"Within the context of {predicted_genre}, "
                              response.append(intro)
                            transitions = [
    "Building on this, ",
    "Expanding further, ",
    "This connects to ",
    "Supporting this perspective, ",
    "Complementary research shows "
                            ]
                            # Add selected sentences with proper transitions
                            for i, sentence in enumerate(selected_sentences):
                              if i == 0:
                                response.append(sentence.capitalize() + ". ")
                              else:
                                transition = transitions[(i-1) % len(transitions)]
                                response.append(transition + sentence.lower() + ". ")
                                    
                                    
                            
                            # Add a conclusion or question
                            if random.random() < 0.05:  # 5% chance to add a question
                                response.append(random.choice([
                                    f"Would you like to know more about {key_terms[0] if key_terms else 'this'}?",
                                    "Does that answer your question?",
                                    "Is there a specific aspect you'd like me to elaborate on?",
                                    "How does this align with your understanding?"
                                ]))
                            confidence_str = f"\n\n[Confidence Score: {confidence:.2f}/1.00]"
        
                            if isinstance(response, str):
                              return response + confidence_str
                            if isinstance(response, list):
                              return json.dumps({'response': response, 'confidence': confidence_str})
                            else:
                              return json.dumps({'response': [response], 'confidence': confidence_str})
                            
            
            # Fallback response with improved variety
            return random.choice([
                "I'm still developing my understanding of this topic. Could you share more details?",
                f"I recognize elements related to {pred}, but I need more context to provide a detailed response.",
                "I'm processing this information and comparing it to my knowledge base. Could you elaborate further?",
                "That's an interesting point. I'm analyzing similar patterns I've observed before.",
                f"The concepts you've mentioned are familiar to me. Can you explain more about {key_terms[0] if key_terms else 'your thoughts'}?"
            ])
                
        # Other analysis types remain unchanged
          elif analysis_type == "sentiment":
            pass #add advanced sentiment analyzer
                
          elif analysis_type == "complexity":
            words = re.findall(r'\w+', text)
            sentences = re.split(r'[.!?]+', text)
            avg_words_per_sentence = len(words) / max(1, len(sentences))
            
            if avg_words_per_sentence > 20:
                return "Complex text"
            elif avg_words_per_sentence > 10:
                return "Moderate complexity"
            else:
                return "Simple text"
            
          else:
            return f"Analysis type '{analysis_type}' not supported"
          
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
          logger.error(f"Error during text analysis: {str(e)}")
          return f"Analysis error: {str(e)}"
          
      finally:
          del X_input, model, probabilities
          gc.collect()
    def validate_label(self, label):
      return re.match(r'^[a-z0-9_]+(::[a-z0-9_]+)*$', label) is not None
            
    def clear_memory(self, confirmation="no"):
        """Clear AI's memory and start fresh."""
        if confirmation.lower() != "yes":
            return "Memory clear aborted. Please confirm with 'yes'."
            
        try:
            # Reset training data
            with self.db_lock:
              
                with self._get_cursor() as cursor:
                  cursor.execute("DELETE FROM training_data")
                  cursor.execute("DELETE FROM validation_data")
                  cursor.execute("DELETE FROM test_data")
                  cursor.execute("UPDATE sqlite_sequence SET seq=0 WHERE name IN ('training_data', 'validation_data', 'test_data')")
                  cursor.connection.commit()
            self.batch_data = []
            
            
            # Reset models
            self.vectorizer = HashingVectorizer(
    n_features=2**18,
    ngram_range=(1,3),
    lowercase=True,
    norm='l2', 
    alternate_sign=False,
    stop_words='english',
    dtype=np.float32,
    binary=True,
    analyzer='char_wb',
    
            )
            self.models = {
    "naive_bayes": MultinomialNB(alpha=0.01),  # More sensitive to rare features
    "random_forest": RandomForestClassifier(
        class_weight='balanced_subsample',
        n_estimators=400,  # Increased from 200
        max_depth=50,       # Added depth constraint
        min_samples_leaf=2, # New parameter
        min_samples_split=5,  # Reduced from 2
        max_features='sqrt',
        random_state=42,
        n_jobs=-1,
        warm_start=True
    ),
    "gradient_boosting": GradientBoostingClassifier(
        n_estimators=2000,  # Increased from 1500
        learning_rate=0.05,  # Adjusted from 0.02
        max_features='log2', # Added feature sampling
        max_depth=6,         # Slightly deeper trees
        subsample=0.7,       # More aggressive subsampling
        validation_fraction=0.15,
        n_iter_no_change=10  # Longer patience
    ),
    "sgd": SGDClassifier(
        loss='log',
        penalty='elasticnet',
        alpha=1e-6,          # Reduced regularization
        max_iter=2000,       # More iterations
        tol=1e-5,            # Tighter tolerance
        learning_rate='optimal',  # Better for large datasets
        eta0=0.01,           # Lower initial learning rate
        power_t=0.25,
        early_stopping=True,
        validation_fraction=0.15,
        n_iter_no_change=10,
        class_weight='balanced',
        average=64           # Larger averaging window
    )
            }
            
            # Reset validation data
            self.validation_data = {"texts": [], "labels": []}
            
            # Reset training stats
            self.training_stats = self._create_default_stats()
            
            # Save empty state
            self.save_memory()
            
            self.save_training_stats()
            
            logger.info("Memory cleared successfully")
            return "Memory cleared successfully. AI is reset to initial state."
            
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error clearing memory: {str(e)}")
            return f"Error clearing memory: {str(e)}"
            
    def export_knowledge_graph(self, format="json"):
        """Export a knowledge graph based on learned data."""
        with self.db_lock:
          
            with self._get_cursor() as cursor:
              cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=1")
            count = cursor.fetchone()[0]
            if count == 0:
              return "Not enough data to generate knowledge graph"
            
        try:
            # Create a simplified knowledge graph
            graph = {"nodes": [], "edges": []}
            
            # Get unique labels
            with self.db_lock:
              
                with self._get_cursor() as cursor:
                  cursor.execute("SELECT DISTINCT label FROM training_data WHERE processed=1")
                  unique_labels = [row[0] for row in cursor.fetchall()]
            
            # Add nodes for each label
            with self.db_lock:
              
                with self._get_cursor() as cursor:
                  cursor.execute("SELECT label, COUNT(*) as count FROM training_data WHERE processed=1 GROUP BY label")
                  label_counts = {row[0]: row[1] for row in cursor.fetchall()}
                  for label in unique_labels:
                    count = label_counts.get(label, 0)
                    graph["nodes"].append({
                    "id": label,
                    "type": label.split(':')[0] if ':' in label else "concept",
                    "weight": count
                    })
                
            # Create connections between labels that often appear together
            label_pairs = {}
            with self.db_lock:
              
                with self._get_cursor() as cursor:
                  cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=1")
                  count = cursor.fetchone()[0]

            total_records = cursor.execute("SELECT COUNT(*) FROM training_data").fetchone()[0]
            for i in range(total_records):
                with self.db_lock:
                  
                    with self._get_cursor() as cursor:
                      cursor.execute("SELECT label FROM training_data WHERE processed=1")
                      all_labels = [row[0] for row in cursor.fetchall()]
                      cursor.close()
                
                window = 5  # Look at nearby items
                start = max(0, i - window)
                end = min(len(count), i + window)
                
                for j in range(start, end):
                    if i == j:
                        continue
                    cursor.execute("SELECT label FROM training_data WHERE processed=1")
                    all_labels = [row[0] for row in cursor.fetchall()]
                    neighbor_label = all_labels[j]
                    for i in range(len(all_labels)):    
                      current_label = all_labels[i]
                     
                    if current_label != neighbor_label:
                        pair_key = f"{current_label}|{neighbor_label}" if current_label < neighbor_label else f"{neighbor_label}|{current_label}"
                        label_pairs[pair_key] = label_pairs.get(pair_key, 0) + 1
            
            # Add edges for frequently co-occurring labels
            threshold = 3  # Minimum co-occurrences to create an edge
            for pair, count in label_pairs.items():
                if count >= threshold:
                    source, target = pair.split('|')
                    graph["edges"].append({
                        "source": source,
                        "target": target,
                        "weight": count
                    })
            
            # Export in requested format
            if format.lower() == "json":
                return json.dumps(graph, indent=2)
            else:
                return f"Export format '{format}' not supported"
                
        except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
            logger.error(f"Error exporting knowledge graph: {str(e)}")
            return f"Export error: {str(e)}"
    
    def _get_training_data_count(self):
      with self._get_cursor() as cursor:
        cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed=0")
        return cursor.fetchone()[0]
        
    def is_ready_to_train(self):
      """Check if the system is ready for training""" 
    # Verify database is properly initialized
      try:
        with self.db_lock:
            with self._get_cursor() as cursor:
              cursor.execute("SELECT COUNT(*) FROM training_data WHERE processed = 1")
              processed_count = cursor.fetchone()[0]
              cursor.close()
            
        if processed_count < 100:  # Minimum threshold
            logger.warning(f"Insufficient processed data: {processed_count} records (minimum 100)")
            return False
            
        # Check if data directory exists and is writable
        if not os.path.exists(DATA_DIR) or not os.access(DATA_DIR, os.W_OK):
            logger.error(f"Data directory {DATA_DIR} does not exist or is not writable")
            return False
            
        # Check for at least 2 unique classes
        if len(self._unique_classes) < 2:
            logger.warning(f"Insufficient classes for classification: {len(self._unique_classes)} (minimum 2)")
            return False
        checks = {
        'database_connection': lambda: self.db_pool.qsize() > 0,
        'min_training_samples': lambda: self._get_training_data_count() >= 1000,
        'class_balance': lambda: len(self._unique_classes) >= 3,
        'vectorizer_ready': lambda: hasattr(self.vectorizer, 'vocabulary_'),
        'data_leakage': self.check_for_leakage(),
        'representative_samples': self._get_training_data_count() > 10_000,
        'validation_set_size': self._get_validation_count() > 2000,
        'test_set_integrity': not self.test_set_contaminated() 
        }

        for check_name, check_fn in checks.items():
          if not check_fn():
            logger.error(f"Training blocked - failed check: {check_name}")
            return False
    
        return True
      except (sqlite3.OperationalError, sqlite3.IntegrityError) as e:
        logger.error(f"Readiness check failed: {str(e)}")
        return False
    def status_printer(self, ai):
    # ANSI escape codes
      RESET = "\033[0m"
      BOLD = "\033[1m"
      COLORS = {
        'blue': "\033[34m",
        'green': "\033[32m",
        'yellow': "\033[33m",
        'red': "\033[31m",
        'cyan': "\033[36m"
      }
      icon_map = {
        'book': {'processing': '📖', 'retrying': '🔄', 'failed': '❌', 'completed': '✅'},
        'website': {'processing': '🌐', 'retrying': '🔄', 'failed': '❌', 'completed': '✅'},
        'warc': {'processing': '📦', 'retrying': '🔄', 'failed': '❌', 'completed': '✅'}
      } 
    # Dynamic progress bar with terminal width adaptation
      def progress_bar(p, width=40):
        cols = os.get_terminal_size().columns
        bar_width = min(width, max(20, cols-50))
        filled = int(p * bar_width)
        return f"{COLORS['green']}{'█' * filled}{COLORS['red']}{'░' * (bar_width - filled)}{RESET}"
      
      last_height = 0
      while True:
        # Get fresh status data
        status = ai.get_learning_status()
        web = ai.web_crawl_stats
        network_status = "ONLINE" if ai.network_status else "OFFLINE"
        
        # Collect all active processes with progress
        active_processes = []
        # Books
        for bid, progress in web['book_progress'].items():
            book = next((b for b in ai.book_index if b['id'] == bid), None)
            active_processes.append({
                "type": "book",
                "title": f"{book['title'][:20]}..." if book else f"Book {bid}",
                "progress": progress
            })
        # Web Pages
        for domain, progress in web['website_progress'].items():
            active_processes.append({
                "type": "website",
                "title": f"{domain[:20]}...",
                "progress": progress
            })
        # WARC Files
        for warc, progress in web.get('warc_progress', {}).items():
            active_processes.append({
                "type": "warc",
                "title": f"{os.path.basename(warc)[:20]}...",
                "progress": progress,
                "processed_count": self.web_crawl_stats["processed_count"]
            }) #ERROR UNDEIFNED VARIABLE DATA

        # Calculate dynamic layout
        terminal_width = os.get_terminal_size().columns
        terminal_height = os.get_terminal_size().lines

        # Calculate available space
        header_lines = 4
        footer_lines = 4
        max_process_lines = terminal_height - header_lines - footer_lines - 1  # -1 for safety buffer
        display_processes = active_processes[:max_process_lines]

        # Clear previous output
        print(f"\033[{last_height}A\033[J", end="")
        
        # Print header
        print(f"{BOLD}{COLORS['blue']}┌{'─' * (terminal_width-2)}┐{RESET}")
        print(f"{BOLD}{COLORS['blue']}│ {'🤖 GodlikeAI - Real-Time Processing Dashboard':^{terminal_width-4}} │{RESET}")
        print(f"{BOLD}{COLORS['blue']}├{'─' * (terminal_width-2)}┤{RESET}")
        
        # Print active processes
        if display_processes:
            for proc in display_processes:
                icon = icon_map[proc['type']].get(proc.get('status', 'processing'), '❓')
                name = proc['title'].ljust(24)
                
                # Handle failed state
                if proc.get('progress', 0) < 0:
                    status = f"{COLORS['red']}FAILED{RESET}"
                    bar = ""
                else:
                    bar = progress_bar(proc.get('progress', 0))
                    status = f"{proc.get('progress', 0)*100:5.1f}%"
                
                # Truncate name if needed
                max_name_len = terminal_width - 45  # Adjust based on other elements
                trimmed_name = (proc['title'][:max_name_len-3] + '...') if len(proc['title']) > max_name_len else proc['title']
                
                line = f"{COLORS['blue']}│ {icon} {trimmed_name.ljust(max_name_len)} {bar} {status} {RESET}"
                print(line[:terminal_width-1])  # Ensure line doesn't overflow
        
        # Print footer
        print(f"{BOLD}{COLORS['blue']}├{'─' * (terminal_width-2)}┤{RESET}")
        model_status = f"{status['active_model']} | Confidence: {ai.last_confidence*100:.1f}%"
        mem_status = f"RAM: {psutil.virtual_memory().percent}% | Net: {network_status}"
        print(f"{COLORS['blue']}│ {model_status.ljust(terminal_width-4)} {RESET}")
        print(f"{COLORS['blue']}│ {mem_status.ljust(terminal_width-4)} {RESET}")
        print(f"{BOLD}{COLORS['blue']}└{'─' * (terminal_width-2)}┘{RESET}")
        
        last_height = header_lines + len(display_processes) + footer_lines
        time.sleep(1)




# AI
if __name__ == "__main__":
    print('Test@')
    ai = GodlikeAI()
    print(10)
    ai.logger.setLevel(logging.DEBUG)
    print("P")
    ai.validate_training_data()
    print("L")
    ai.balance_classes()
    print("Z")
    # Database integrity check
    with ai._get_cursor() as cursor:
        cursor.execute("PRAGMA quick_check")
        result = cursor.fetchone()
        if result[0] != 'ok':
            ai._repair_database()

    # Configuration changes
    #ai.modify_profile("batch_size", "256")
    #ai.modify_profile("active_model", "random_forest")
    print("checking if model is ready to train....")
    # Initial training check
    if ai.is_ready_to_train():
        ai.train_models(save_checkpoints=True)
        ai.evaluate_test_set()
    else:
        print("Collecting More Data...")
        ai.collect_more_data(min_samples=1000)

    # Start background processes in threads
    
    learning_thread = Thread(target=ai.start_learning, daemon=True)
    web_crawler_thread = Thread(target=ai.start_web_crawler, daemon=True)
    learning_thread.start()
    web_crawler_thread.start()

    # Start foreground processing in separate thread
    foreground_thread = Thread(target=ai.run_in_foreground, daemon=True)
    foreground_thread.start()

    # Interactive interface in main thread
    print("\n" + "=" * 60)
    print("GodlikeAI Conversational System Initializing...")
    print("=" * 60)
    
    # Start status printer in background
    status_thread = Thread(target=ai.status_printer, args=(ai,), daemon=True)
    status_thread.start()

    conversation_history = []
    try:
        while True:
            user_input = input("\nYou: ")
            conversation_history.append(f"User: {user_input}")
            
            if user_input.lower() == 'quit':
                print("\nStopping AI learning process...")
                ai.stop_learning()
                print("Goodbye!")
                break
                
            # Handle special commands
            elif user_input.lower().startswith('modify'):
                _, key, value = user_input.split(maxsplit=2)
                ai.modify_profile(key, value)
            elif user_input.lower() == 'clear memory':
                print(ai.clear_memory(confirmation="yes"))
            elif user_input.lower() == 'export':
                print(ai.export_knowledge_graph())
    
            # Generate AI response
            else:
                if ai.fitted:
                    response = ai.analyze_text(user_input, "response")
                    print(f"\nAI: {response}")
                    conversation_history.append(f"AI: {response}")
                else:
                    print("\nAI: I'm still learning. Please continue our conversation to help me improve.")
                    
    except KeyboardInterrupt: 
        print("\n\nSession ended by user. Finalizing operations...")
        ai.stop_learning()
        ai.save_memory()
        ai.save_learning_progress()
        ai.save_learning_progress()    
